{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import collections\n",
    "import hashlib\n",
    "import numbers\n",
    "\n",
    "from tensorflow.python.eager import context\n",
    "from tensorflow.python.framework import constant_op\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.framework import tensor_util\n",
    "from tensorflow.python.layers import base as base_layer\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import clip_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.ops import partitioned_variables\n",
    "from tensorflow.python.ops import random_ops\n",
    "from tensorflow.python.ops import tensor_array_ops\n",
    "from tensorflow.python.ops import variable_scope as vs\n",
    "from tensorflow.python.ops import variables as tf_variables\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "from tensorflow.python.util import nest\n",
    "from tensorflow.contrib.rnn.python.ops.core_rnn_cell import _Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# uplading mnist data \n",
    "\n",
    "old_v = tf.logging.get_verbosity()\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "train_data = mnist.train.images  # Returns np.array\n",
    "train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "eval_data = mnist.test.images  # Returns np.array\n",
    "eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)\n",
    "\n",
    "tf.logging.set_verbosity(old_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 1e-5\n",
    "training_steps = 5000\n",
    "batch_size = 128\n",
    "display_step = 200\n",
    "test_len=128\n",
    "grad_clip=200\n",
    "# Network Parameters\n",
    "num_input = 2 # MNIST data input (img shape: 28*28)\n",
    "timesteps = 28*28 # timesteps\n",
    "num_hidden = 128 # hidden layer num of features\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a tuple object for the cell, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "_KernelRNNStateTuple = collections.namedtuple(\"KernelRNNStateTuple\", (\"h\",\"h_hat\",\"Theta\", \"Gamma\",\"input_trace\",\"recurrent_trace\",\"input_sensitivity\",\"recurrent_sensitivity\",\"kernel_coeff\"))\n",
    "_KernelRNNOutputTuple = collections.namedtuple(\"KernelRNNOutputTuple\", (\"h\",\"h_hat\",\"Theta\",\"Gamma\", \"input_trace\",\"recurrent_trace\"))\n",
    "\n",
    "class KernelRNNStateTuple(_KernelRNNStateTuple):\n",
    "  \"\"\"Tuple used by kernel RNN Cells for `state_variables `.\n",
    "  Stores 9 elements: `(h, h_hat, Theta, Gamma, input_trace,recurrent_trace, input_sensitivity,recurrent_sensitivity, kernel_coeff`, in that order. \n",
    "  always is used for this type of cell\n",
    "  \"\"\"\n",
    "  __slots__ = ()\n",
    "\n",
    "  @property\n",
    "  def dtype(self):\n",
    "    (h, h_hat,Theta, Gamma, input_trace,recurrent_trace, input_sensitivity, recurrent_sensitivity, kernel_coeff ) = self\n",
    "    if h.dtype != h_hat.dtype:\n",
    "      raise TypeError(\"Inconsistent internal state: %s vs %s\" %\n",
    "                      (str(h.dtype), str(h_hat.dtype)))\n",
    "    return h_hat.dtype\n",
    "\n",
    "\n",
    "class KernelRNNOutputTuple(_KernelRNNOutputTuple):\n",
    "  \"\"\"Tuple used by kernel Cells for output state.\n",
    "  Stores 6 elements: `(h,h_hat, Theta, Gamma, input_trace, recurrent_trace)`, \n",
    "  Only used when `output_is_tuple=True`.\n",
    "  \"\"\"\n",
    "  __slots__ = ()\n",
    "\n",
    "  @property\n",
    "  def dtype(self):\n",
    "    (h, h_hat, Theta, Gamma, input_trace, recurrent_trace) = self\n",
    "    if h.dtype != h_hat.dtype:\n",
    "      raise TypeError(\"Inconsistent internal state: %s vs %s\" %\n",
    "                      (str(h.dtype), str(h_hat.dtype)))\n",
    "    return h_hat.dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "## expand dimensions for incoming recurrent and input activations for multiplication with current acivation \n",
    "def _tensor_expand_dim(x,y,output_size):\n",
    "    \"\"\"input - x : a 2D tensor with batch x n \n",
    "    y is a 2D with size batch x m\n",
    "    outputs is 3D tensor with size batch x n x n and batch x n x m \n",
    "    \"\"\" \n",
    "    shape_x=x.get_shape()\n",
    "    shape_y=y.get_shape()\n",
    "    #y=tf.cast(y,tf.float32)\n",
    "    # define a matrix for removing the diagonal in recurrent spikes \n",
    "    diag_zero= lambda:tf.subtract(tf.constant(1.0,shape=[shape_x[1],shape_x[1]]),\n",
    "                                                    tf.eye(output_size))\n",
    "    x_diag_fixer = tf.Variable(initial_value=diag_zero, dtype=tf.float32)\n",
    "    # expand x  \n",
    "    x_temp=tf.reshape(tf.tile(x,[1,output_size]),[-1,output_size,shape_x[1]])\n",
    "    # remove diagonal \n",
    "    x_expand=tf.multiply(x_temp,x_diag_fixer)\n",
    "    # expand y  \n",
    "    y_expand=tf.reshape(tf.tile(y,[1,output_size]),[-1,output_size,shape_y[1]])\n",
    "    return x_expand, y_expand\n",
    "\n",
    "def _create_pertubation(x,mean,std):\n",
    "    shape=x.get_shape()\n",
    "    logging.warn(\"%s: Please use float \", [shape[0].value,shape[1].value])\n",
    "    scope=vs.get_variable_scope()\n",
    "    with vs.variable_scope (scope) as perturbation_scope:\n",
    "        perturbation=tf.constant(1.0,shape=[shape[0].value,shape[1].value])\n",
    "    #perturbation=tf.constant(tf.random_normal(shape=[shape[0].value,shape[1].value], mean=mean,stddev=std))\n",
    "    \n",
    "    return perturbation\n",
    "\n",
    "def _gaussian_noise_perturbation(input_layer, std):\n",
    "    noise = tf.random_normal(shape=tf.shape(input_layer), mean=0.0, stddev=std, dtype=tf.float32) \n",
    "    return tf.multiply(input_layer,0) + noise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a Kernel RNN cell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelRNNCell(tf.contrib.rnn.RNNCell):\n",
    "    \"\"\"Kernel recurrent neural network Cell\n",
    "      Args:\n",
    "        num_units: int, The number of units in the cell.\n",
    "        activation: Nonlinearity to use.  Default: `Relu`.\n",
    "        eligibility_kernel: kernel funtion to use for elibility \n",
    "        reuse: (optional) Python boolean describing whether to reuse variables\n",
    "         in an existing scope.  If not `True`, and the existing scope already has\n",
    "         the given variables, an error is raised.\n",
    "        kernel_initializer: (optional) The initializer to use for the weight and\n",
    "        projection matrices.\n",
    "        bias_initializer: (optional) The initializer to use for the bias.\n",
    "    \"\"\"\n",
    "    def __init__(self,num_units,num_inputs,activation=None,reuse=None,eligibility_kernel=None,state_is_tuple=True,\n",
    "                output_is_tuple=False,noise_std=1.0,batch_KeRNL=True):\n",
    "        \n",
    "        super(KernelRNNCell,self).__init__(_reuse=reuse)\n",
    "        self._num_units = num_units\n",
    "        self._num_inputs= num_inputs\n",
    "        self._activation = activation or math_ops.tanh\n",
    "        self._eligibility_kernel = eligibility_kernel or math_ops.exp\n",
    "        self._noise_std=noise_std\n",
    "        self._linear = None\n",
    "        self._state_is_tuple=state_is_tuple\n",
    "        self._output_is_tuple= output_is_tuple\n",
    "        self._batch_KeRNL=batch_KeRNL\n",
    "        self._tensor_expand_dim=_tensor_expand_dim\n",
    "        self._gaussian_noise_perturbation=_gaussian_noise_perturbation\n",
    "    \n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return (KernelRNNStateTuple(self._num_units, \n",
    "                                    self._num_units, \n",
    "                                    self._num_units, \n",
    "                                    self._num_units, \n",
    "                                    np.array([self._num_units,self._num_inputs]), \n",
    "                                    np.array([self._num_units,self._num_units]),\n",
    "                                    np.array([self._num_units,self._num_inputs]),\n",
    "                                    np.array([self._num_units,self._num_units]),\n",
    "                                    self._num_units)\n",
    "                if self._state_is_tuple else self._num_units)\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return (KernelRNNOutputTuple(self._num_units, \n",
    "                                     self._num_units, \n",
    "                                     self._num_units, \n",
    "                                     self._num_units,\n",
    "                                     np.array([self._num_units,self._num_inputs]), \n",
    "                                     np.array([self._num_units,self._num_units]))\n",
    "                if self._output_is_tuple else self._num_units)\n",
    "\n",
    "    # call function routine \n",
    "    def call(self, inputs, state):\n",
    "        \"\"\"Kernel RNN cell (KernelRNN).\n",
    "        Args:\n",
    "          inputs: `2-D` tensor with shape `[batch_size x input_size]`.\n",
    "          state: An `KernelRNNStateTuple` of state tensors, shaped as following \n",
    "            h:                   [batch_size x self.state_size]`\n",
    "            h_hat:               [batch_size x self.state_size]`\n",
    "            Theta:               [batch_size x self.state_size]`\n",
    "            Gamma:               [batch_size x self.state_size]`\n",
    "            input_trace          [batch_size x self.state_size x self.input_size]`\n",
    "            recurrent_trace      [batch_size x self.state_size x self.state_size]`\n",
    "            input_sensitivity    [batch_size x self.state_size x self.input_size]`\n",
    "            recurrent_sensitivity[batch_size x self.state_size x self.state_size]`\n",
    "            kernel coeff         [batch_size x self.state_size]`\n",
    "        Returns:\n",
    "          A pair containing the new output, and the new state as SNNStateTuple\n",
    "          output has the following shape \n",
    "            h:                   [batch_size x self.state_size]`\n",
    "            h_hat:               [batch_size x self.state_size]`\n",
    "            Theta:               [batch_size x self.state_size]`\n",
    "            Gamma                [batch_size x self.state_size]`\n",
    "            input_trace          [batch_size x self.state_size x self.input_size]`\n",
    "            recurrent_trace      [batch_size x self.state_size x self.state_size]`  \n",
    "        \"\"\"\n",
    "        if self._state_is_tuple:\n",
    "            h, h_hat, Theta, Gamma, input_trace, recurrent_trace, input_sensitivity, recurrent_sensitivity, kernel_coeff= state\n",
    "        else:\n",
    "            logging.error(\"State has to be tuple for this type of cell\")\n",
    "        \n",
    "        if self._linear is None: \n",
    "            self._linear = _Linear([inputs, h], self._num_units, True)\n",
    "        psi_new=self._gaussian_noise_perturbation(h,self._noise_std)\n",
    "        # propagate data forward \n",
    "        h_new=self._activation(self._linear([inputs,h]))\n",
    "        # propagate noisy data forward \n",
    "        h_hat_update=tf.add(h_hat,psi_new)\n",
    "        h_hat_new= self._activation(self._linear([inputs, h_hat_update]))\n",
    "        # TODO : check of weights get reused \n",
    "        # integrate over perturbations\n",
    "        Theta_new=tf.add(tf.multiply(self._eligibility_kernel(-kernel_coeff),\n",
    "                              Theta),psi_new)\n",
    "        # derivative of perturbation w.r.t to kernel_coeff\n",
    "        Gamma_new=tf.subtract(tf.multiply(self._eligibility_kernel(-kernel_coeff),\n",
    "                              Gamma),\n",
    "                             tf.multiply(self._eligibility_kernel(-kernel_coeff),\n",
    "                              Theta))\n",
    "        # update elgibility traces for input and recurrent units \n",
    "        g_new=self._linear([inputs,h])\n",
    "        pre_activation=self._activation(g_new)\n",
    "\n",
    "        # expand recurrent and input activation \n",
    "        recurrent_expand,inputs_expand=self._tensor_expand_dim(h,inputs,self._num_units)\n",
    "        logging.warn(\"%s: inputs_expand \", inputs_expand.get_shape())\n",
    "        activation_gradients=tf.gradients(pre_activation,g_new)[0] # convert list to a tensor \n",
    "\n",
    "        #logging.warn(\"%s: code \", type(activatation_gradients))\n",
    "        gradient_expansion=tf.expand_dims(activation_gradients,axis=-1)\n",
    "        logging.warn(\"%s: gradient_expansion \", gradient_expansion.get_shape())\n",
    "        #logging.warn(\"%s: code \", gradient_expansion)\n",
    "        input_trace_update=tf.multiply(gradient_expansion,inputs_expand)\n",
    "        logging.warn(\"%s: input_trace_update \", input_trace_update.get_shape())\n",
    "        recurrent_trace_update=tf.multiply(gradient_expansion,recurrent_expand)\n",
    "        logging.warn(\"%s: recurrent_trace_update \", recurrent_trace_update.get_shape())\n",
    "        logging.warn(\"%s: input_trace \", input_trace.get_shape())\n",
    "        kernel_decay=tf.expand_dims(self._eligibility_kernel(-kernel_coeff),axis=-1)\n",
    "        logging.warn(\"%s: kernel_decay \", kernel_decay.get_shape())\n",
    "        input_trace_decay=tf.multiply(kernel_decay,input_trace)\n",
    "        logging.warn(\"%s: input_trace_decay \", input_trace_decay.get_shape())\n",
    "        input_trace_new=tf.add(input_trace_decay,input_trace_update)\n",
    "        logging.warn(\"%s: input_trace_new \", input_trace_new.get_shape())\n",
    "        \n",
    "        recurrent_trace_decay=tf.multiply(kernel_decay,recurrent_trace)\n",
    "        logging.warn(\"%s: recurrent_trace_decay \", recurrent_trace_decay.get_shape())\n",
    "        recurrent_trace_new=tf.add(recurrent_trace_decay,recurrent_trace_update)\n",
    "\n",
    "        # TODO implement online updating for sensitivity and kernel coeff\n",
    "        \n",
    "        \n",
    "        if self._state_is_tuple: \n",
    "            new_state=KernelRNNStateTuple(h_new,h_hat_new,Theta_new,Gamma_new,input_trace_new,\n",
    "                                          recurrent_trace_new,input_sensitivity,recurrent_sensitivity,kernel_coeff)\n",
    "        if self._output_is_tuple:\n",
    "            new_output=KernelRNNOutputTuple(h_new,h_hat_new,Theta_new,Gamma_new,input_trace_new,recurrent_trace_new)\n",
    "        else:\n",
    "            new_output=h_new\n",
    "\n",
    "        return new_output, new_state\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test the network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "    \n",
    "    # using variable scope to initialize to identity \n",
    "    with tf.variable_scope('recurrent',initializer=tf.initializers.identity()) as scope: \n",
    "        # Define a lstm cell with tensorflow\n",
    "        kernel_cell = KernelRNNCell(num_units=num_hidden,num_inputs=num_input)\n",
    "        # Get lstm cell output\n",
    "        outputs, states = tf.nn.dynamic_rnn(kernel_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[:,-1,:], weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:(?, 128, 2): inputs_expand \n",
      "WARNING:tensorflow:(?, 128, 1): gradient_expansion \n",
      "WARNING:tensorflow:(?, 128, 2): input_trace_update \n",
      "WARNING:tensorflow:(?, 128, 128): recurrent_trace_update \n",
      "WARNING:tensorflow:(?, 128, 2): input_trace \n",
      "WARNING:tensorflow:(?, 128, 1): kernel_decay \n",
      "WARNING:tensorflow:(?, 128, 2): input_trace_decay \n",
      "WARNING:tensorflow:(?, 128, 2): input_trace_new \n",
      "WARNING:tensorflow:(?, 128, 128): recurrent_trace_decay \n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "graph=tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Define weights\n",
    "    weights = {\n",
    "        'out': tf.Variable(tf.random_normal([num_hidden, num_classes]),name='output_weight')\n",
    "    }\n",
    "    biases = {\n",
    "        'out': tf.Variable(tf.random_normal([num_classes]),name='output_bias')\n",
    "    }\n",
    "    X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "    Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "    logits = kernel_RNN(X, weights, biases)\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'init' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-41539c3ace88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"variable: \"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'init' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(KeRNL)",
   "language": "python",
   "name": "kernl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
