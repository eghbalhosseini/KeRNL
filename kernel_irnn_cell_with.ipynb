{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import collections\n",
    "import hashlib\n",
    "import numbers\n",
    "\n",
    "from tensorflow.python.eager import context\n",
    "from tensorflow.python.framework import constant_op\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.framework import tensor_util\n",
    "from tensorflow.python.layers import base as base_layer\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import clip_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.ops import partitioned_variables\n",
    "from tensorflow.python.ops import random_ops\n",
    "from tensorflow.python.ops import tensor_array_ops\n",
    "from tensorflow.python.ops import variable_scope as vs\n",
    "from tensorflow.python.ops import variables as tf_variables\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "from tensorflow.python.util import nest\n",
    "from tensorflow.contrib.rnn.python.ops.core_rnn_cell import _Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a tuple object for the cell, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_KernelRNNStateTuple = collections.namedtuple(\"KernelRNNStateTuple\", (\"h\",\"h_hat\",\"Theta\", \"Gamma\",\"input_trace\",\"recurrent_trace\",\"input_sensitivity\",\"recurrent_sensitivity\",\"kernel_coeff\"))\n",
    "_KernelRNNOutputTuple = collections.namedtuple(\"KernelRNNOutputTuple\", (\"h\",\"h_hat\",\"Theta\",\"Gamma\", \"input_trace\",\"recurrent_trace\"))\n",
    "\n",
    "class KernelRNNStateTuple(_KernelRNNStateTuple):\n",
    "  \"\"\"Tuple used by kernel RNN Cells for `state_variables `.\n",
    "  Stores 9 elements: `(h, h_hat, Theta, Gamma, input_trace,recurrent_trace, input_sensitivity,recurrent_sensitivity, kernel_coeff`, in that order. \n",
    "  always is used for this type of cell\n",
    "  \"\"\"\n",
    "  __slots__ = ()\n",
    "\n",
    "  @property\n",
    "  def dtype(self):\n",
    "    (h, h_hat,Theta, Gamma, input_trace,recurrent_trace, input_sensitivity, recurrent_sensitivity, kernel_coeff ) = self\n",
    "    if h.dtype != h_hat.dtype:\n",
    "      raise TypeError(\"Inconsistent internal state: %s vs %s\" %\n",
    "                      (str(h.dtype), str(h_hat.dtype)))\n",
    "    return h_hat.dtype\n",
    "\n",
    "\n",
    "class KernelRNNOutputTuple(_KernelRNNOutputTuple):\n",
    "  \"\"\"Tuple used by kernel Cells for output state.\n",
    "  Stores 6 elements: `(h,h_hat, Theta, Gamma, input_trace, recurrent_trace)`, \n",
    "  Only used when `output_is_tuple=True`.\n",
    "  \"\"\"\n",
    "  __slots__ = ()\n",
    "\n",
    "  @property\n",
    "  def dtype(self):\n",
    "    (h, h_hat, Theta, Gamma, input_trace, recurrent_trace) = self\n",
    "    if h.dtype != h_hat.dtype:\n",
    "      raise TypeError(\"Inconsistent internal state: %s vs %s\" %\n",
    "                      (str(h.dtype), str(h_hat.dtype)))\n",
    "    return h_hat.dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## expand dimensions for incoming recurrent and input activations for multiplication with current acivation \n",
    "def _tensor_expand_dim(x,y,output_size):\n",
    "    \"\"\"input - x : a 2D tensor with batch x n \n",
    "    y is a 2D with size batch x m\n",
    "    outputs is 3D tensor with size batch x n x n and batch x n x m \n",
    "    \"\"\" \n",
    "    shape_x=x.get_shape()\n",
    "    shape_y=y.get_shape()\n",
    "    #y=tf.cast(y,tf.float32)\n",
    "    # define a matrix for removing the diagonal in recurrent spikes \n",
    "    diag_zero= lambda:tf.subtract(tf.constant(1.0,shape=[shape_x[1],shape_x[1]]),\n",
    "                                                    tf.eye(output_size))\n",
    "    x_diag_fixer = tf.Variable(initial_value=diag_zero, dtype=tf.float32)\n",
    "    # expand x  \n",
    "    x_temp=tf.reshape(tf.tile(x,[1,output_size]),[-1,output_size,shape_x[1]])\n",
    "    # remove diagonal \n",
    "    x_expand=tf.multiply(x_temp,x_diag_fixer)\n",
    "    # expand y  \n",
    "    y_expand=tf.reshape(tf.tile(y,[1,output_size]),[-1,output_size,shape_y[1]])\n",
    "    return x_expand, y_expand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a Kernel RNN cell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelRNNCell(tf.contrib.rnn.RNNCell):\n",
    "\"\"\"Kernel recurrent neural network Cell\n",
    "  Args:\n",
    "    num_units: int, The number of units in the cell.\n",
    "    activation: Nonlinearity to use.  Default: `Relu`.\n",
    "    eligibility_kernel: kernel funtion to use for elibility \n",
    "    reuse: (optional) Python boolean describing whether to reuse variables\n",
    "     in an existing scope.  If not `True`, and the existing scope already has\n",
    "     the given variables, an error is raised.\n",
    "    kernel_initializer: (optional) The initializer to use for the weight and\n",
    "    projection matrices.\n",
    "    bias_initializer: (optional) The initializer to use for the bias.\n",
    "  \"\"\"\n",
    "    def __init__(self,\n",
    "                num_units,\n",
    "                num_inputs,\n",
    "                activation=None,\n",
    "                reuse=None,\n",
    "                eligibility_kernel=None,\n",
    "                state_is_tuple=True,\n",
    "                output_is_tuple=False,\n",
    "                noise_std=1.0,\n",
    "                batch_KeRNL=True):\n",
    "        super(KernelRNNCell,self).__init__(_reuse=reuse)\n",
    "        self._num_units = num_units\n",
    "        self._num_inputs= num_inputs\n",
    "        self._activation = activation or math_ops.tanh\n",
    "        self._eligibility_kernel = eligibility_kernel or math_ops.exp\n",
    "        self._noise_std=noise_std\n",
    "        self._linear = None\n",
    "        self._state_is_tuple=state_is_tuple\n",
    "        self._output_is_tuple= output_is_tuple\n",
    "        self._batch_KeRNL=batch_KeRNL\n",
    "        self._tensor_expand_dim=_tensor_expand_dim\n",
    "    \n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return (KernelRNNStateTuple(self._num_units, \n",
    "                                    self._num_units, \n",
    "                                    self._num_units, \n",
    "                                    self._num_units, \n",
    "                                    np.array([self._num_units,self._num_inputs]), \n",
    "                                    np.array([self._num_units,self._num_units]),\n",
    "                                    np.array([self._num_units,self._num_inputs]),\n",
    "                                    np.array([self._num_units,self._num_units]),\n",
    "                                    self._num_units)\n",
    "                if self._state_is_tuple else self._num_units)\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return (KernelRNNOutputTuple(self._num_units, \n",
    "                                     self._num_units, \n",
    "                                     self._num_units, \n",
    "                                     self._num_units,\n",
    "                                     np.array([self._num_units,self._num_inputs]), \n",
    "                                     np.array([self._num_units,self._num_units]))\n",
    "                if self._output_is_tuple else self._num_units)\n",
    "\n",
    "    # call function routine \n",
    "    def call(self, inputs, state):\n",
    "    \"\"\"Kernel RNN cell (KernelRNN).\n",
    "    Args:\n",
    "      inputs: `2-D` tensor with shape `[batch_size x input_size]`.\n",
    "      state: An `KernelRNNStateTuple` of state tensors, shaped as following \n",
    "        h:                   [batch_size x self.state_size]`\n",
    "        h_hat:               [batch_size x self.state_size]`\n",
    "        Theta:               [batch_size x self.state_size]`\n",
    "        Gamma:               [batch_size x self.state_size]`\n",
    "        input_trace          [batch_size x self.state_size x self.input_size]`\n",
    "        recurrent_trace      [batch_size x self.state_size x self.state_size]`\n",
    "        input_sensitivity    [batch_size x self.state_size x self.input_size]`\n",
    "        recurrent_sensitivity[batch_size x self.state_size x self.state_size]`\n",
    "        kernel coeff         [batch_size x self.state_size]`\n",
    "    Returns:\n",
    "      A pair containing the new output, and the new state as SNNStateTuple\n",
    "      output has the following shape \n",
    "        h:                   [batch_size x self.state_size]`\n",
    "        h_hat:               [batch_size x self.state_size]`\n",
    "        Theta:               [batch_size x self.state_size]`\n",
    "        Gamma                [batch_size x self.state_size]`\n",
    "        input_trace          [batch_size x self.state_size x self.input_size]`\n",
    "        recurrent_trace      [batch_size x self.state_size x self.state_size]`\n",
    "            \n",
    "    \"\"\"\n",
    "        if self._state_is_tuple:\n",
    "            h, h_hat, Theta, Gamma, input_trace, recurrent_trace, input_sensitivity, recurrent_sensitivity, kernel_coeff= state\n",
    "        else:\n",
    "            logging.error(\"State has to be tuple for this type of cell\")\n",
    "        \n",
    "        if self._linear is None: \n",
    "            self._linear=_linear\n",
    "\n",
    "        psi_new=tf.random_normal(shape=h_hat.get_shape(), mean=0.0,stddev=self._noise_std)    \n",
    "        # propagate data forward \n",
    "        h_new=self._activation(self._linear([inputs,h],self._num_units,True))\n",
    "        # propagate noisy data forward \n",
    "        h_hat_new= self._activation(self._linear(inputs, h_hat+psi_new,self._num_units,True))\n",
    "        # TODO : check of weights get reused \n",
    "        # integrate over perturbations\n",
    "        Theta_new=tf.add(tf.multiply(self._eligibility_kernel(-kernel_coeff),\n",
    "                              Theta),psi_new)\n",
    "        # derivative of perturbation w.r.t to kernel_coeff\n",
    "        Gamma_new=tf.subtract(tf.multiply(self._eligibility_kernel(-kernel_coeff),\n",
    "                              Gamma),\n",
    "                             tf.multiply(self._eligibility_kernel(-kernel_coeff),\n",
    "                              Theta))\n",
    "        # update elgibility traces for input and recurrent units \n",
    "        g_new=self._linear([inputs,h],self._num_units,True)\n",
    "        pre_activation=self._activation(g_new)\n",
    "        # expand recurrent and input activation \n",
    "        recurrent_expand,inputs_expand=self._tensor_expand_dim(h,inputs,self._num_units)\n",
    "        input_trace_update=tf.multiply(tf.expand_dims(tf.gradients(pre_activation,g_new),2),inputs_expand)\n",
    "        recurrent_trace_update=tf.multiply(tf.expand_dims(tf.gradients(pre_activation,g_new),2),recurrent_expand)\n",
    "        input_trace_new=tf.add(tf.multiply(self._eligibility_kernel(-kernel_coeff),\n",
    "                              input_trace),input_trace_update)\n",
    "        recurrent_trace_new=tf.add(tf.multiply(self._eligibility_kernel(-kernel_coeff),\n",
    "                              recurrent_trace),recurrent_trace_update)\n",
    "\n",
    "        # TODO implement online updating for sensitivity and kernel coeff\n",
    "        \n",
    "        \n",
    "        if self._state_is_tuple: \n",
    "            new_state=KernelRNNStateTuple(h_new,h_hat_new,Theta_new,Gamma_new,input_trace_new,\n",
    "                                          recurrent_trace_new,input_sensitivity,recurrent_sensitivity,kernel_coeff)\n",
    "        if self._output_is_tuple:\n",
    "            new_output=KernelRNNOutputTuple(h)\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(KeRNL)",
   "language": "python",
   "name": "kernl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
