{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import collections\n",
    "import hashlib\n",
    "import numbers\n",
    "import matplotlib.cm as cm\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "%matplotlib inline\n",
    "\n",
    "from tensorflow.python.eager import context\n",
    "from tensorflow.python.framework import constant_op\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.framework import tensor_util\n",
    "from tensorflow.python.layers import base as base_layer\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import clip_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.ops import partitioned_variables\n",
    "from tensorflow.python.ops import random_ops\n",
    "from tensorflow.python.ops import tensor_array_ops\n",
    "from tensorflow.python.ops import variable_scope as vs\n",
    "from tensorflow.python.ops import variables as tf_variables\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "from tensorflow.python.util import nest\n",
    "from tensorflow.contrib.rnn.python.ops.core_rnn_cell import _Linear\n",
    "from matplotlib.colors import LinearSegmentedColormap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_INPUT_WEIGHT_NAME = \"W_in\"\n",
    "_RECURRENT_WEIGHT_NAME = \"W_rec\"\n",
    "## crossing function\n",
    "def _calcualte_crossings(x,threshold):\n",
    "    \"\"\"input :x : a 2D tensor with batch x n \n",
    "    outputs a tensor with the same size as x \n",
    "    and values of 0 or 1 depending on comparison between \n",
    "    x and threshold\"\"\" \n",
    "    @tf.custom_gradient\n",
    "    def crossings(x):\n",
    "        dtype=x.dtype\n",
    "        shape=x.get_shape()\n",
    "        thresholds=tf.constant(threshold,shape=[shape[0].value,shape[1].value],dtype=dtype)\n",
    "        # if it has one row \n",
    "        res=tf.greater_equal(x,thresholds)\n",
    "        def grad(dy):\n",
    "            # calculate 1-|x|\n",
    "            temp=1-tf.abs(x)\n",
    "            dyres=tf.maximum(temp,0.0)\n",
    "            return dyres\n",
    "        return tf.cast(res,dtype=dtype), grad\n",
    "    z=crossings(x)\n",
    "    return z \n",
    "\n",
    "## calculate input current to the neuron\n",
    "def _linear_rec_in_current(x,y,output_size):\n",
    "    \"\"\"input - x : a 2D tensor with batch x n \n",
    "    y is a 2D with size batch x m\n",
    "    outputs a tensor  with size batch x output_size,\n",
    "    \"\"\" \n",
    "    shape_x=x.get_shape()\n",
    "    shape_y=y.get_shape()\n",
    "    # \n",
    "    scope=vs.get_variable_scope()\n",
    "    with vs.variable_scope(scope) as outer_scope:\n",
    "        weight_rec=tf.get_variable(_RECURRENT_WEIGHT_NAME,[shape_x[1],output_size]) # [n x n]\n",
    "        weight_in=tf.get_variable(_INPUT_WEIGHT_NAME,[shape_y[1],output_size]) # [m x n]\n",
    "        #w_in_test= lambda:tf.constant(0.0,shape=[shape_y[1],shape_y[2]]) # [n x n]\n",
    "        #w_rec_test= lambda:tf.constant(0.0,shape=[shape_x[1],shape_x[2]]) # [n x n]\n",
    "        #weight_rec=tf.Variable(initial_value=w_rec_test,dtype=tf.float32) # [n x n]\n",
    "        #weight_in=tf.Variable(initial_value=w_in_test,dtype=tf.float32) # [n x n]\n",
    "        # apply_weights \n",
    "        #recurrent\n",
    "        res_rec_final=tf.matmul(x,weight_rec)\n",
    "        #input\n",
    "        res_in_final=tf.matmul(y,weight_in)\n",
    "        # sum both \n",
    "        res=tf.add(res_in_final,res_rec_final)\n",
    "        return res\n",
    "\n",
    "## define tuples for the cell \n",
    "_LSNNStateTuple = collections.namedtuple(\"LSNNStateTuple\", (\"v_mem\",\"spike\",\"t_reset\", \"I_syn\"))\n",
    "_LSNNOutputTuple = collections.namedtuple(\"LSNNOutputTuple\", (\"v_mem\",\"spike\",\"t_reset\", \"I_syn\"))\n",
    "\n",
    "class LSNNStateTuple(_LSNNStateTuple):\n",
    "  \"\"\"Tuple used by LSNN Cells for `state_variables `, and output state.\n",
    "  Stores five elements: `(v_mem,spike, t_reset, I_syn)`, in that order. Where `v_mem` is the hidden state\n",
    "  , spike is output, `S_rec` and 'S_in' are spike history, and t_reset refractory history.\n",
    "  Only used when `state_is_tuple=True`.\n",
    "  \"\"\"\n",
    "  __slots__ = ()\n",
    "\n",
    "  @property\n",
    "  def dtype(self):\n",
    "    (v_mem, spike,t_reset, I_syn) = self\n",
    "    if v_mem.dtype != spike.dtype:\n",
    "      raise TypeError(\"Inconsistent internal state: %s vs %s\" %\n",
    "                      (str(v_mem.dtype), str(spike.dtype)))\n",
    "    return spike.dtype\n",
    "\n",
    "\n",
    "class LSNNOutputTuple(_LSNNOutputTuple):\n",
    "  \"\"\"Tuple used by SNN Cells for output state.\n",
    "  Stores six elements: `(v_mem,spike,t_reset,I_syn)`, \n",
    "  Only used when `output_is_tuple=True`.\n",
    "  \"\"\"\n",
    "  __slots__ = ()\n",
    "\n",
    "  @property\n",
    "  def dtype(self):\n",
    "    (v_mem,spike,t_reset,I_syn) = self\n",
    "    if v_mem.dtype != spike.dtype:\n",
    "      raise TypeError(\"Inconsistent internal state: %s vs %s\" %\n",
    "                      (str(v_mem.dtype), str(spike.dtype)))\n",
    "    return spike.dtype\n",
    "\n",
    "\n",
    "## define LSNNcell \n",
    "class LSNNCell(tf.contrib.rnn.RNNCell):\n",
    "  \"\"\"LSNN Cell\n",
    "  Args:\n",
    "    num_units: \n",
    "  \"\"\"\n",
    "  def __init__(self,\n",
    "               num_units,\n",
    "               tau_m=5.0,\n",
    "               v_theta=1.0,\n",
    "               v_reset=0.0,\n",
    "               R_mem=2.0,\n",
    "               tau_s=5.0,\n",
    "               tau_refract=3.0,\n",
    "               dt=1.0,\n",
    "               activation=None,\n",
    "               reuse=None,\n",
    "               kernel_initializer=None,\n",
    "               bias_initializer=None,\n",
    "               state_is_tuple=False,\n",
    "               output_is_tuple=False):\n",
    "        \n",
    "    super(LSNNCell, self).__init__(_reuse=reuse)\n",
    "    self._num_units = num_units\n",
    "    self.tau_m=tau_m\n",
    "    self.v_theta=v_theta\n",
    "    self.v_reset=v_reset\n",
    "    self.R_mem=R_mem\n",
    "    self.tau_s=tau_s\n",
    "    self.tau_refract=tau_refract\n",
    "    self.dt=dt\n",
    "    self._activation = activation or math_ops.tanh\n",
    "    self._kernel_initializer = kernel_initializer\n",
    "    self._bias_initializer = bias_initializer\n",
    "    self._state_is_tuple= state_is_tuple\n",
    "    self._output_is_tuple= output_is_tuple\n",
    "    self._calculate_crossing= _calcualte_crossings\n",
    "    self._linear_rec_in_current = _linear_rec_in_current\n",
    "\n",
    "\n",
    "  @property\n",
    "  def state_size(self):\n",
    "    return (LSNNStateTuple(self._num_units,\n",
    "                          self._num_units,\n",
    "                          self._num_units,\n",
    "                          self._num_units) if self._state_is_tuple else self._num_units)\n",
    "                          \n",
    "\n",
    "#  @property\n",
    "#  def output_size(self):\n",
    "#    return self._num_units\n",
    "  @property\n",
    "  def output_size(self):\n",
    "    return (LSNNOutputTuple(self._num_units,\n",
    "                          self._num_units,\n",
    "                          self._num_units,\n",
    "                          self._num_units) if self._output_is_tuple else self._num_units)\n",
    "    \n",
    "\n",
    "  def call(self, inputs, state):\n",
    "    \"\"\" (LSNN).\n",
    "    Args:\n",
    "      inputs: `2-D` tensor with shape `[batch_size x input_size]`.\n",
    "      state: An `SNNStateTuple` of state tensors, shaped as following \n",
    "              `[batch_size x self.state_size]`\n",
    "              `[batch_size x self.state_size]`\n",
    "              `[batch_size x self.state_size]`\n",
    "              `[batch_size x self.state_size]`.\n",
    "              \"v_mem\",\"spike\",\"t_reset\", \"I_syn\"\n",
    "    Returns:\n",
    "      A pair containing the new output, and the new state as SNNStateTuple\n",
    "    \"\"\"\n",
    "    if self._state_is_tuple:\n",
    "        v_mem,spike, t_reset, I_syn = state\n",
    "    else: \n",
    "        logging.warn(\"%s: Please use float \", self)\n",
    "    # initialize crossing function \n",
    "    #if self._calculate_crossing is None:\n",
    "    #    self._calculate_crossing = _calcualte_crossings(v_mem,self.v_theta)\n",
    "        \n",
    "    #if self._tensor_expand_dim is None:\n",
    "    #    self._tensor_expand_dim = _tensor_expand_dim(spike,inputs,self._num_units)\n",
    "    \n",
    "    \n",
    "    # compoutation \n",
    "    # 1-get spikes\n",
    "    spike_temp=self._calculate_crossing(v_mem,self.v_theta)\n",
    "    # hold spike at zero for t_reset>0\n",
    "    t_subtract= tf.subtract(t_reset,1.0)\n",
    "    t_update=tf.clip_by_value(t_subtract,0.0,100)\n",
    "        # get eligible neurons for update \n",
    "    eligilible_update=tf.cast(tf.equal(t_update,0.0),tf.float32)\n",
    "    spike=tf.multiply(eligilible_update,spike_temp)\n",
    "    \n",
    "    t_reset_new=tf.add(tf.multiply(spike,self.tau_refract),t_update)\n",
    "    \n",
    "    \n",
    "    v_reseting=tf.multiply(self.v_theta,spike)\n",
    "    v_update=tf.subtract(v_mem,v_reseting)\n",
    "        \n",
    "    \n",
    "    # calculate new Isyn = W*S \n",
    "    I_syn_new=self._linear_rec_in_current(spike,inputs,self._num_units)\n",
    "    \n",
    "    ## update membrane potential \n",
    "        # update refractory period \n",
    "    t_subtract= tf.subtract(t_reset,1.0)\n",
    "    t_update=tf.clip_by_value(t_subtract,0.0,100)\n",
    "    t_reset_new=tf.add(tf.multiply(spike,self.tau_refract),t_update)\n",
    "        # get eligible neurons for update \n",
    "    eligilible_update=tf.cast(tf.equal(t_reset_new,0.0),tf.float32)\n",
    "    \n",
    "    # calculate factor for updating \n",
    "    alpha=tf.exp(tf.negative(tf.divide(self.dt,self.tau_m)))\n",
    "        # update voltage\n",
    "    v_mem_new=tf.add(tf.scalar_mul(alpha,v_update),tf.scalar_mul(tf.multiply(1-alpha,self.R_mem),I_syn_new))\n",
    "    \n",
    "    ## return variables \n",
    "    \n",
    "    if self._state_is_tuple:\n",
    "        new_state = LSNNStateTuple( v_mem_new, spike,t_reset_new, I_syn_new )\n",
    "    if self._output_is_tuple:\n",
    "        new_output = LSNNOutputTuple(v_mem_new, spike,t_reset_new, I_syn_new ) \n",
    "    else:\n",
    "        new_output = spike\n",
    "    return new_output, new_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_OUTPUT_WEIGHT_NAME = \"W_out\"\n",
    "\n",
    "## calculate input current to the neuron\n",
    "def _linear_output(x,output_size):\n",
    "    \"\"\"input - x : a 2D tensor with batch x n \n",
    "    outputs a tensor  with size batch x output_size,\n",
    "    \"\"\" \n",
    "    shape_x=x.get_shape()\n",
    "    # \n",
    "    scope=vs.get_variable_scope()\n",
    "    with vs.variable_scope(scope) as outer_scope:\n",
    "        weight_out=tf.get_variable(_OUTPUT_WEIGHT_NAME,[shape_x[1],output_size]) # [n x n]\n",
    "\n",
    "        # apply_weights \n",
    "        res_out_final=tf.matmul(x,weight_out)\n",
    "        return res_out_final\n",
    "\n",
    "\n",
    "\n",
    "## define LSNNOutcell \n",
    "class LSNNOutCell(tf.contrib.rnn.RNNCell):\n",
    "  \"\"\"LSNN Cell\n",
    "  Args:\n",
    "    num_units: \n",
    "  \"\"\"\n",
    "  def __init__(self,\n",
    "               num_units,\n",
    "               tau_m=5.0,\n",
    "               dt=1.0,\n",
    "               reuse=None,\n",
    "               kernel_initializer=None,\n",
    "               bias_initializer=None):\n",
    "        \n",
    "    super(LSNNOutCell, self).__init__(_reuse=reuse)\n",
    "    self._num_units = num_units\n",
    "    self.tau_m=tau_m\n",
    "    self.dt=dt\n",
    "    self._kernel_initializer = kernel_initializer\n",
    "    self._bias_initializer = bias_initializer\n",
    "    self._linear_rec_in_current = _linear_output\n",
    "\n",
    "\n",
    "  @property\n",
    "  def state_size(self):\n",
    "    return  self._num_units\n",
    "                          \n",
    "\n",
    "#  @property\n",
    "#  def output_size(self):\n",
    "#    return self._num_units\n",
    "  @property\n",
    "  def output_size(self):\n",
    "    return  self._num_units\n",
    "    \n",
    "\n",
    "  def call(self, inputs, state):\n",
    "    \"\"\" (LSNN).\n",
    "    Args:\n",
    "      inputs: `2-D` tensor with shape `[batch_size x input_size]`.\n",
    "      state: An `SNNStateTuple` of state tensors, shaped as following \n",
    "              `[batch_size x self.state_size]`\n",
    "              `[batch_size x self.state_size]`\n",
    "              `[batch_size x self.state_size]`\n",
    "              `[batch_size x self.state_size]`.\n",
    "              \"v_mem\",\"spike\",\"t_reset\", \"I_syn\"\n",
    "    Returns:\n",
    "      A pair containing the new output, and the new state as SNNStateTuple\n",
    "    \"\"\"\n",
    "    v_mem = state\n",
    " \n",
    "    # calculate new Isyn = W*S \n",
    "    I_syn_new=self._linear_rec_in_current(inputs,self._num_units)\n",
    "    \n",
    "    ## update membrane potential \n",
    "    \n",
    "    # calculate factor for updating \n",
    "    alpha=tf.exp(tf.negative(tf.divide(self.dt,self.tau_m)))\n",
    "        # update voltage\n",
    "    v_mem_new=tf.add(tf.scalar_mul(alpha,v_mem),I_syn_new)\n",
    "    \n",
    "    ## return variables\n",
    "    new_state =  v_mem_new\n",
    "    new_output = v_mem_new\n",
    "    \n",
    "    return new_output, new_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate spikes based on the threshold crossing for the neurons \n",
    "def _calculate_spikes(x,threshold):\n",
    "    \"\"\"input - x : a 2D tensor with batch x n ex 10*1\n",
    "    outputs a tensor with size batch x output_size, where outputsize is twice the size of thresholds_size \n",
    "    \"\"\" \n",
    "    shape_x=x.get_shape()\n",
    "    #\n",
    "    logging.warn(\"%s: Please use float \", shape_x)\n",
    "    x_aux=tf.random_uniform(shape=[shape_x[0].value,shape_x[1].value],dtype=tf.float32)\n",
    "    logging.warn(\"%s: Please use float \", x_aux.get_shape())\n",
    "    res_out=tf.divide(tf.negative(tf.sign(x_aux-threshold)-1),2)\n",
    "    \n",
    "    return res_out\n",
    "\n",
    "\n",
    "## define LSNNOutcell \n",
    "class LSNNSimpleInCell(tf.contrib.rnn.RNNCell):\n",
    "    def __init__(self,\n",
    "               num_units=40,\n",
    "               reuse=None,\n",
    "               kernel_initializer=None,\n",
    "               bias_initializer=None):\n",
    "        super(LSNNSimpleInCell, self).__init__(_reuse=reuse)\n",
    "        self._num_units = num_units\n",
    "        self._kernel_initializer = kernel_initializer\n",
    "        self._bias_initializer = bias_initializer\n",
    "        self._calculate_spikes = _calculate_spikes\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return  self._num_units\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return  self._num_units\n",
    "    \n",
    "    def call(self, inputs, state):\n",
    "        spike_state = state\n",
    "        # calculate new Isyn = W*S \n",
    "        new_spikes=self._calculate_spikes(spike_state,inputs)\n",
    "        new_state =  new_spikes\n",
    "        new_output = new_spikes\n",
    "        return new_output, new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# uplading mnist data \n",
    "\n",
    "old_v = tf.logging.get_verbosity()\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "train_data = mnist.train.images  # Returns np.array\n",
    "train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "eval_data = mnist.test.images  # Returns np.array\n",
    "eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)\n",
    "\n",
    "tf.logging.set_verbosity(old_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of batches: 5500\n",
      "WARNING:tensorflow:(10, 40): Please use float \n",
      "WARNING:tensorflow:(10, 40): Please use float \n"
     ]
    }
   ],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.002\n",
    "training_steps = 5000\n",
    "batch_size = 10\n",
    "display_step = 250\n",
    "total_batch = int(mnist.train.num_examples / batch_size)\n",
    "print(\"Total number of batches:\", total_batch)\n",
    "\n",
    "num_input = 1 # MNIST data input (img shape: 28*28)\n",
    "timesteps = 28 * 28 # timesteps\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "num_input_units=40 # hidden layer num of features\n",
    "num_hidden_units=200 # hidden layer num of features\n",
    "num_output_units=10 # output layer - classifier \n",
    "def SNN(x):\n",
    "    Input_Cell = LSNNSimpleInCell(num_units=num_input_units)\n",
    "    LSNN_cell = LSNNCell(num_units=num_hidden_units,state_is_tuple=True,output_is_tuple=False,tau_s=10.0)\n",
    "    LSNNOut_cell = LSNNOutCell(num_units=num_output_units)\n",
    "    encoding_spikes, _ = tf.nn.dynamic_rnn(cell=Input_Cell, dtype=tf.float32, inputs=x)\n",
    "    spikes, _ = tf.nn.dynamic_rnn(cell=LSNN_cell, dtype=tf.float32, inputs=encoding_spikes)\n",
    "    output_voltage, _ = tf.nn.dynamic_rnn(cell=LSNNOut_cell, dtype=tf.float32, inputs=spikes)\n",
    "    return output_voltage\n",
    "# \n",
    "tf.reset_default_graph()\n",
    "graph=tf.Graph()\n",
    "with graph.as_default():\n",
    "    # input to graph\n",
    "    X=tf.placeholder('float',shape=[batch_size,timesteps,num_input])\n",
    "    # define out\n",
    "    output_voltages=SNN(X)\n",
    "    init=tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(init)\n",
    "    batch_x,batch_y=mnist.train.next_batch(batch_size)\n",
    "    batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "    outputs=sess.run(output_voltages, feed_dict={X: batch_x})\n",
    "    \n",
    "voltages=outputs[9,:,:]\n",
    "plt.figure(figsize=[15,10])\n",
    "ax1=plt.subplot(211)\n",
    "colors_map=cm.viridis(np.linspace(0,1,voltages.shape[1]))\n",
    "for t in range(num_output_units):\n",
    "    ax1.plot(voltages[:,t],color=colors_map[t,:])\n",
    "    plt.xlim([0,timesteps])\n",
    "\n",
    "ax2=plt.subplot(212)\n",
    "ax2.imshow(voltages.transpose())\n",
    "ax2.set_aspect(20)\n",
    "\n",
    "#ax3=plt.subplot(313)\n",
    "#ax3.imshow(np.transpose(batch_x[9]))\n",
    "#ax3.set_aspect(30)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(batch_x[9].reshape(28,28))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(KeRNL)",
   "language": "python",
   "name": "kernl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
