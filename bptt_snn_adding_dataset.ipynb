{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python libraries\n",
    "import numpy as np\n",
    "\n",
    "import collections\n",
    "import hashlib\n",
    "import numbers\n",
    "from sys import getsizeof\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "\n",
    "# tensorflow and its dependencies\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.eager import context\n",
    "from tensorflow.python.framework import constant_op\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.framework import tensor_util\n",
    "from tensorflow.python.layers import base as base_layer\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import clip_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.ops import partitioned_variables\n",
    "from tensorflow.python.ops import random_ops\n",
    "from tensorflow.python.ops import tensor_array_ops\n",
    "from tensorflow.python.ops import variable_scope as vs\n",
    "from tensorflow.python.ops import variables as tf_variables\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "from tensorflow.python.util import nest\n",
    "from tensorflow.contrib.rnn.python.ops.core_rnn_cell import _Linear\n",
    "from tensorflow.contrib import slim\n",
    "\n",
    "## user defined modules\n",
    "## user defined modules\n",
    "# kernel rnn cell\n",
    "import spiking_cell_bare as spiking_cell\n",
    "import adding_problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "# Training Parameters\n",
    "weight_learning_rate = 1e-5\n",
    "training_steps = 100\n",
    "batch_size = 25\n",
    "training_size=batch_size*training_steps\n",
    "epochs=5\n",
    "test_size=1000\n",
    "display_step = 10\n",
    "grad_clip=100\n",
    "# Network Parameters\n",
    "num_input = 2 # adding problem data input (first input are the random digits , second input is the mask)\n",
    "time_steps = 50\n",
    "num_units_input_layer=50\n",
    "num_hidden = 100 # hidden layer num of features\n",
    "num_output = 1 # value of the addition estimation\n",
    "#\n",
    "# save dir\n",
    "log_dir = os.environ['HOME']+\"/MyData/KeRNL/logs/bptt_snn_addition_dataset/bp_snn_add_T_%1.0e_eta_W_%1.0e_batch_%1.0e_hum_hidd_%1.0e_gc_%1.0e_steps_%1.0e_epoch_%1.0e_run_%s\" %(time_steps,weight_learning_rate,batch_size,num_hidden,grad_clip,training_steps,epochs, datetime.now().strftime(\"%Y%m%d_%H%M\"))\n",
    "log_dir\n",
    "# create a training and testing dataset\n",
    "training_x, training_y = adding_problem.get_batch(batch_size=training_size,time_steps=time_steps)\n",
    "testing_x, testing_y = adding_problem.get_batch(batch_size=test_size,time_steps=time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _hinton_identity_initializer(shape,dtype=None,partition_info=None,verify_shape=None, max_val=1):\n",
    "    if dtype is None:\n",
    "        dtype=tf.float32\n",
    "    #extract second dimension\n",
    "    W_rec=tf.eye(shape[-1],dtype=dtype)\n",
    "    new_shape=[shape[0]-shape[-1],shape[-1]]\n",
    "    W_in=tf.random_normal(new_shape,mean=0,stddev=0.001)\n",
    "    return tf.concat([W_in,W_rec],axis=0)\n",
    "\n",
    "## define KeRNL unit\n",
    "def bptt_snn_all_states(x,context):\n",
    "    with tf.variable_scope('input_layer') as scope:\n",
    "        input_layer_cell=spiking_cell.input_spike_cell(num_units=num_units_input_layer)\n",
    "        output_l1, states_l1 = tf.nn.dynamic_rnn(input_layer_cell, dtype=tf.float32, inputs=x)\n",
    "    with tf.variable_scope('hidden_layer') as scope:\n",
    "        hidden_layer_cell=spiking_cell.conductance_spike_cell(num_units=num_hidden,output_is_tuple=True,tau_refract=2.0,tau_m=20.0,kernel_initializer=_hinton_identity_initializer)\n",
    "        output_hidden, states_hidden = tf.nn.dynamic_rnn(hidden_layer_cell, dtype=tf.float32, inputs=tf.concat([output_l1,context],-1))\n",
    "    with tf.variable_scope('output_layer') as scope :\n",
    "        output_layer_cell=spiking_cell.output_spike_cell(num_units=num_output)\n",
    "        output_voltage, voltage_states=tf.nn.dynamic_rnn(output_layer_cell,dtype=tf.float32,inputs=output_hidden.spike)\n",
    "    return output_voltage,output_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensor(\"input_layer/rnn/while/rnn/input_spike_cell/strided_slice:0\", shape=(), dtype=int32): Please use float \n",
      "WARNING:tensorflow:(?, 50): Please use float \n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "graph=tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    BATCH_SIZE=tf.placeholder(tf.int64)\n",
    "    X = tf.placeholder(\"float\", [None, time_steps, num_input])\n",
    "    Y = tf.placeholder(\"float\", [None, num_output])\n",
    "    # define a dataset\n",
    "    dataset=tf.data.Dataset.from_tensor_slices((X,Y)).batch(BATCH_SIZE).repeat()\n",
    "    dataset = dataset.shuffle(buffer_size=200)\n",
    "    iter = dataset.make_initializable_iterator()\n",
    "    inputs,labels =iter.get_next()\n",
    "    # define a function for extraction of variable names\n",
    "    bptt_output,bptt_hidden_states=bptt_snn_all_states(tf.expand_dims(inputs[:,:,0],axis=-1),tf.expand_dims(inputs[:,:,1],axis=-1))\n",
    "    trainables=tf.trainable_variables()\n",
    "    variable_names=[v.name for v in tf.trainable_variables()]\n",
    "    #\n",
    "    find_join_index = lambda x, name_1,name_2 : [a and b for a,b in zip([np.unicode_.find(k.name, name_1)>-1 for k in x] ,[np.unicode_.find(k.name, name_2)>-1 for k in x])].index(True)\n",
    "    # find trainable parameters for bptt\n",
    "    with tf.name_scope('bptt_Trainables') as scope:\n",
    "        bptt_output_weight_index= find_join_index(trainables,'output_layer','kernel')\n",
    "        bptt_kernel_index= find_join_index(trainables,'hidden_layer','kernel')\n",
    "        bptt_weight_training_indices=np.asarray([bptt_kernel_index,bptt_output_weight_index],dtype=np.int)\n",
    "        bptt_weight_trainables= [trainables[k] for k in bptt_weight_training_indices]\n",
    "\n",
    "    with tf.name_scope('bptt_train_weights') as scope:\n",
    "        bptt_weight_optimizer = tf.train.RMSPropOptimizer(learning_rate=weight_learning_rate)\n",
    "        bptt_loss_output_prediction=tf.losses.mean_squared_error(labels,bptt_output[:,-1,:])\n",
    "        bptt_grad_cost_trainables=tf.gradients(bptt_loss_output_prediction,bptt_weight_trainables)\n",
    "        bptt_weight_grads_and_vars=list(zip(bptt_grad_cost_trainables,bptt_weight_trainables))\n",
    "        bptt_cropped_weight_grads_and_vars=[(tf.clip_by_norm(grad, grad_clip),var) if  np.unicode_.find(var.name,'output')==-1 else (grad,var) for grad,var in bptt_weight_grads_and_vars]\n",
    "        bptt_weight_train_op = bptt_weight_optimizer.apply_gradients(bptt_cropped_weight_grads_and_vars)\n",
    "\n",
    "\n",
    "            ##################\n",
    "            ## bptt train ####\n",
    "            ##################\n",
    "\n",
    "    with tf.name_scope(\"bptt_evaluate\") as scope:\n",
    "        bptt_loss_cross_validiation=tf.losses.mean_squared_error(labels,bptt_output[:,-1,:])\n",
    "\n",
    "    with tf.name_scope('cross_validation_summary') as scope:\n",
    "        tf.summary.scalar('cross_validation_summary',bptt_loss_cross_validiation+1e-10)\n",
    "        bptt_evaluate_summary_op=tf.summary.merge_all(scope=\"cross_validation_summary\")\n",
    "\n",
    "                ##################\n",
    "                # SUMMARIES ######\n",
    "                ##################\n",
    "\n",
    "    with tf.name_scope(\"bptt_weight_summaries\") as scope:\n",
    "\n",
    "        #\n",
    "        tf.summary.histogram('bptt_kernel_grad',bptt_grad_cost_trainables[0]+1e-10)\n",
    "        tf.summary.histogram('bptt_kernel', trainables[0]+1e-10)\n",
    "                    # bptt output weight\n",
    "        tf.summary.histogram('bptt_output_weight_grad',bptt_grad_cost_trainables[1]+1e-10)\n",
    "        tf.summary.histogram('bptt_output_weights', trainables[1]+1e-10)\n",
    "                    # bptt loss and accuracy\n",
    "        tf.summary.scalar('bptt_loss_output_prediction',bptt_loss_output_prediction+1e-10)\n",
    "\n",
    "        # bptt senstivity tensor and temporal filter\n",
    "        bptt_merged_summary_op=tf.summary.merge_all(scope=\"bptt_weight_summaries\")\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['variable: ', 'hidden_layer/rnn/conductance_spike_cell/kernel:0']\n",
      "['variable: ', -1]\n",
      "['shape: ', (151, 100)]\n",
      "['variable: ', 'output_layer/rnn/output_spike_cell/kernel:0']\n",
      "['variable: ', 0]\n",
      "['shape: ', (100, 1)]\n",
      "['variable: ', 'output_layer/rnn/output_spike_cell/bias:0']\n",
      "['variable: ', 0]\n",
      "['shape: ', (1,)]\n"
     ]
    }
   ],
   "source": [
    "# verify initialization\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "with tf.Session(graph=graph,) as sess : \n",
    "    sess.run(init)\n",
    "    values,trainable_vars = sess.run([variable_names,trainables])\n",
    "    for k, v in zip(variable_names,values):\n",
    "        print([\"variable: \" , k])\n",
    "        #print([\"value: \" , v])\n",
    "        print([\"variable: \" , np.unicode_.find(k,'output')]) \n",
    "        print([\"shape: \" , v.shape])\n",
    "        #print(v) training_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(log_dir).mkdir(exist_ok=True, parents=True)\n",
    "filelist = [ f for f in os.listdir(log_dir) if f.endswith(\".local\") ]\n",
    "for f in filelist:\n",
    "    os.remove(os.path.join(log_dir, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 1,bptt train Loss: 1.181\n",
      "Epoch: 1, Batch: 2,bptt train Loss: 0.877\n",
      "Epoch: 1, Batch: 11,bptt train Loss: 1.282\n",
      "Epoch: 1, Batch: 21,bptt train Loss: 1.412\n",
      "Epoch: 1, Batch: 31,bptt train Loss: 1.175\n",
      "Epoch: 1, Batch: 41,bptt train Loss: 1.214\n",
      "Epoch: 1, Batch: 51,bptt train Loss: 1.224\n",
      "Epoch: 1, Batch: 61,bptt train Loss: 1.071\n",
      "Epoch: 1, Batch: 71,bptt train Loss: 1.365\n",
      "Epoch: 1, Batch: 81,bptt train Loss: 1.252\n",
      "Epoch: 1, Batch: 91,bptt train Loss: 0.971\n",
      "Epoch: 1, cross validation loss :1.181\n",
      "Epoch: 2, Batch: 1,bptt train Loss: 1.386\n",
      "Epoch: 2, Batch: 2,bptt train Loss: 1.182\n",
      "Epoch: 2, Batch: 11,bptt train Loss: 1.412\n",
      "Epoch: 2, Batch: 21,bptt train Loss: 1.411\n",
      "Epoch: 2, Batch: 31,bptt train Loss: 1.165\n",
      "Epoch: 2, Batch: 41,bptt train Loss: 1.145\n",
      "Epoch: 2, Batch: 51,bptt train Loss: 1.450\n",
      "Epoch: 2, Batch: 61,bptt train Loss: 1.530\n",
      "Epoch: 2, Batch: 71,bptt train Loss: 1.229\n",
      "Epoch: 2, Batch: 81,bptt train Loss: 0.828\n",
      "Epoch: 2, Batch: 91,bptt train Loss: 1.386\n",
      "Epoch: 2, cross validation loss :1.181\n",
      "Epoch: 3, Batch: 1,bptt train Loss: 1.430\n",
      "Epoch: 3, Batch: 2,bptt train Loss: 1.229\n",
      "Epoch: 3, Batch: 11,bptt train Loss: 1.488\n",
      "Epoch: 3, Batch: 21,bptt train Loss: 1.269\n",
      "Epoch: 3, Batch: 31,bptt train Loss: 1.416\n",
      "Epoch: 3, Batch: 41,bptt train Loss: 1.185\n",
      "Epoch: 3, Batch: 51,bptt train Loss: 0.970\n",
      "Epoch: 3, Batch: 61,bptt train Loss: 1.329\n",
      "Epoch: 3, Batch: 71,bptt train Loss: 1.400\n",
      "Epoch: 3, Batch: 81,bptt train Loss: 1.079\n",
      "Epoch: 3, Batch: 91,bptt train Loss: 1.382\n",
      "Epoch: 3, cross validation loss :1.181\n",
      "Epoch: 4, Batch: 1,bptt train Loss: 1.383\n",
      "Epoch: 4, Batch: 2,bptt train Loss: 1.277\n",
      "Epoch: 4, Batch: 11,bptt train Loss: 1.229\n",
      "Epoch: 4, Batch: 21,bptt train Loss: 1.473\n",
      "Epoch: 4, Batch: 31,bptt train Loss: 1.185\n",
      "Epoch: 4, Batch: 41,bptt train Loss: 1.230\n",
      "Epoch: 4, Batch: 51,bptt train Loss: 1.412\n",
      "Epoch: 4, Batch: 61,bptt train Loss: 1.416\n",
      "Epoch: 4, Batch: 71,bptt train Loss: 1.230\n",
      "Epoch: 4, Batch: 81,bptt train Loss: 0.929\n",
      "Epoch: 4, Batch: 91,bptt train Loss: 1.271\n",
      "Epoch: 4, cross validation loss :1.181\n",
      "Epoch: 5, Batch: 1,bptt train Loss: 1.446\n",
      "Epoch: 5, Batch: 2,bptt train Loss: 0.883\n",
      "Epoch: 5, Batch: 11,bptt train Loss: 1.400\n",
      "Epoch: 5, Batch: 21,bptt train Loss: 0.916\n",
      "Epoch: 5, Batch: 31,bptt train Loss: 1.293\n",
      "Epoch: 5, Batch: 41,bptt train Loss: 1.232\n",
      "Epoch: 5, Batch: 51,bptt train Loss: 1.382\n",
      "Epoch: 5, Batch: 61,bptt train Loss: 0.945\n",
      "Epoch: 5, Batch: 71,bptt train Loss: 1.013\n",
      "Epoch: 5, Batch: 81,bptt train Loss: 1.181\n",
      "Epoch: 5, Batch: 91,bptt train Loss: 1.277\n",
      "Epoch: 5, cross validation loss :1.181\n",
      "Optimization Finished!\n",
      "Model saved in path: /home/eghbal/MyData/KeRNL/logs/bptt_snn_addition_dataset/bp_snn_add_T_5e+01_eta_W_1e-05_batch_2e+01_hum_hidd_1e+02_gc_1e+02_steps_1e+02_epoch_5e+00_run_20190306_1333/model.ckpt-99\n"
     ]
    }
   ],
   "source": [
    "# write graph into tensorboard \n",
    "tb_writer = tf.summary.FileWriter(log_dir,graph)\n",
    "# run a training session \n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(epochs):\n",
    "        sess.run(iter.initializer,feed_dict={X: training_x, Y: training_y , BATCH_SIZE: batch_size})\n",
    "        for step in range(training_steps): \n",
    "            bptt_train, bptt_loss,bptt_merged_summary=sess.run([bptt_weight_train_op,bptt_loss_output_prediction,bptt_merged_summary_op])\n",
    "            tb_writer.add_summary(bptt_merged_summary, global_step=step)\n",
    "\n",
    "            if step % display_step==0 or step==1 : \n",
    "                print('Epoch: {}, Batch: {},bptt train Loss: {:.3f}'.format(epoch+1,step + 1, bptt_loss))\n",
    "                \n",
    "        # run test at the end of each epoch \n",
    "        sess.run(iter.initializer, feed_dict={ X: testing_x, Y: testing_y, BATCH_SIZE: testing_x.shape[0]})    \n",
    "        bptt_test_loss, bptt_evaluate_summary=sess.run([bptt_loss_cross_validiation,bptt_evaluate_summary_op])        \n",
    "        tb_writer.add_summary(bptt_evaluate_summary, global_step=step)\n",
    "        print('Epoch: {}, cross validation loss :{:.3f}'.format(epoch+1,bptt_test_loss))\n",
    "            \n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    save_path = saver.save(sess, log_dir+\"/model.ckpt\", global_step=step,write_meta_graph=True)\n",
    "    print(\"Model saved in path: %s\" % save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
