{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python libraries\n",
    "import numpy as np\n",
    "\n",
    "import collections\n",
    "import hashlib\n",
    "import numbers\n",
    "from sys import getsizeof\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "\n",
    "# tensorflow and its dependencies\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.eager import context\n",
    "from tensorflow.python.framework import constant_op\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.framework import tensor_util\n",
    "from tensorflow.python.layers import base as base_layer\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import clip_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.ops import partitioned_variables\n",
    "from tensorflow.python.ops import random_ops\n",
    "from tensorflow.python.ops import tensor_array_ops\n",
    "from tensorflow.python.ops import variable_scope as vs\n",
    "from tensorflow.python.ops import variables as tf_variables\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "from tensorflow.python.util import nest\n",
    "from tensorflow.contrib.rnn.python.ops.core_rnn_cell import _Linear\n",
    "from tensorflow.contrib import slim\n",
    "\n",
    "## user defined modules\n",
    "## user defined modules\n",
    "# kernel rnn cell\n",
    "import spiking_cell_bare as spiking_cell\n",
    "import adding_problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "# Training Parameters\n",
    "weight_learning_rate = 1e-8\n",
    "training_steps = 1000\n",
    "batch_size = 25\n",
    "training_size=batch_size*training_steps\n",
    "epochs=5\n",
    "test_size=1000\n",
    "display_step = 10\n",
    "grad_clip=100\n",
    "# Network Parameters\n",
    "num_input = 2 # adding problem data input (first input are the random digits , second input is the mask)\n",
    "time_steps = 50\n",
    "num_units_input_layer=50\n",
    "num_hidden = 100 # hidden layer num of features\n",
    "num_output = 1 # value of the addition estimation\n",
    "#\n",
    "# save dir\n",
    "log_dir = os.environ['HOME']+\"/MyData/KeRNL/logs/bptt_snn_addition_dataset/bp_snn_add_T_%1.0e_eta_W_%1.0e_batch_%1.0e_hum_hidd_%1.0e_gc_%1.0e_steps_%1.0e_epoch_%1.0e_run_%s\" %(time_steps,weight_learning_rate,batch_size,num_hidden,grad_clip,training_steps,epochs, datetime.now().strftime(\"%Y%m%d_%H%M\"))\n",
    "log_dir\n",
    "# create a training and testing dataset\n",
    "training_x, training_y = adding_problem.get_batch(batch_size=training_size,time_steps=time_steps)\n",
    "testing_x, testing_y = adding_problem.get_batch(batch_size=test_size,time_steps=time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _hinton_identity_initializer(shape,dtype=None,partition_info=None,verify_shape=None, max_val=1):\n",
    "    if dtype is None:\n",
    "        dtype=tf.float32\n",
    "    #extract second dimension\n",
    "    W_rec=tf.eye(shape[-1],dtype=dtype)\n",
    "    new_shape=[shape[0]-shape[-1],shape[-1]]\n",
    "    W_in=tf.random_normal(new_shape,mean=0,stddev=0.001)\n",
    "    return tf.concat([W_in,W_rec],axis=0)\n",
    "\n",
    "## define KeRNL unit\n",
    "def bptt_snn_all_states(x,context):\n",
    "    with tf.variable_scope('input_layer') as scope:\n",
    "        input_layer_cell=spiking_cell.input_spike_cell(num_units=num_units_input_layer)\n",
    "        output_l1, states_l1 = tf.nn.dynamic_rnn(input_layer_cell, dtype=tf.float32, inputs=x)\n",
    "    with tf.variable_scope('hidden_layer') as scope:\n",
    "        hidden_layer_cell=spiking_cell.conductance_spike_cell(num_units=num_hidden,output_is_tuple=True,tau_refract=2.0,tau_m=20.0,kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        output_hidden, states_hidden = tf.nn.dynamic_rnn(hidden_layer_cell, dtype=tf.float32, inputs=tf.concat([output_l1,context],-1))\n",
    "    with tf.variable_scope('output_layer') as scope :\n",
    "        output_layer_cell=spiking_cell.output_spike_cell(num_units=num_output)\n",
    "        output_voltage, voltage_states=tf.nn.dynamic_rnn(output_layer_cell,dtype=tf.float32,inputs=output_hidden.spike)\n",
    "    return output_voltage,output_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensor(\"input_layer/rnn/while/rnn/input_spike_cell/strided_slice:0\", shape=(), dtype=int32): Please use float \n",
      "WARNING:tensorflow:(?, 50): Please use float \n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "graph=tf.Graph()\n",
    "with graph.as_default():\n",
    "    BATCH_SIZE=tf.placeholder(tf.int64)\n",
    "    X = tf.placeholder(\"float\", [None, time_steps, num_input])\n",
    "    Y = tf.placeholder(\"float\", [None, num_output])\n",
    "    # define a dataset\n",
    "    dataset=tf.data.Dataset.from_tensor_slices((X,Y)).batch(BATCH_SIZE).repeat()\n",
    "    dataset = dataset.shuffle(buffer_size=200)\n",
    "    iter = dataset.make_initializable_iterator()\n",
    "    inputs,labels =iter.get_next()\n",
    "    # define a function for extraction of variable names\n",
    "    bptt_output,bptt_hidden_states=bptt_snn_all_states(tf.expand_dims(inputs[:,:,0],axis=-1),tf.expand_dims(inputs[:,:,1],axis=-1))\n",
    "    trainables=tf.trainable_variables()\n",
    "    variable_names=[v.name for v in tf.trainable_variables()]\n",
    "    #\n",
    "    find_join_index = lambda x, name_1,name_2 : [a and b for a,b in zip([np.unicode_.find(k.name, name_1)>-1 for k in x] ,[np.unicode_.find(k.name, name_2)>-1 for k in x])].index(True)\n",
    "    # find trainable parameters for bptt\n",
    "    with tf.name_scope('bptt_Trainables') as scope:\n",
    "        bptt_output_weight_index= find_join_index(trainables,'output_layer','kernel')\n",
    "        bptt_kernel_index= find_join_index(trainables,'hidden_layer','kernel')\n",
    "        bptt_weight_training_indices=np.asarray([bptt_kernel_index,bptt_output_weight_index],dtype=np.int)\n",
    "        bptt_weight_trainables= [trainables[k] for k in bptt_weight_training_indices]\n",
    "\n",
    "    with tf.name_scope('bptt_train_weights') as scope:\n",
    "        bptt_weight_optimizer = tf.train.RMSPropOptimizer(learning_rate=weight_learning_rate)\n",
    "        bptt_loss_output_prediction=tf.losses.mean_squared_error(labels,bptt_output[:,-1,:])\n",
    "        #bptt_grad_cost_trainables=tf.gradients(bptt_loss_output_prediction,bptt_weight_trainables)\n",
    "        bptt_grad_cost_trainables=bptt_weight_optimizer.compute_gradients(bptt_loss_output_prediction,trainables)\n",
    "        bptt_weight_train_op = bptt_weight_optimizer.apply_gradients(bptt_grad_cost_trainables)\n",
    "        #bptt_weight_grads_and_vars=list(zip(bptt_grad_cost_trainables,bptt_weight_trainables))\n",
    "        #bptt_cropped_weight_grads_and_vars=[(tf.clip_by_norm(grad, grad_clip),var) if  np.unicode_.find(var.name,'output')==-1 else (grad,var) for grad,var in bptt_weight_grads_and_vars]\n",
    "        #bptt_weight_train_op = bptt_weight_optimizer.apply_gradients(bptt_cropped_weight_grads_and_vars)\n",
    "\n",
    "\n",
    "            ##################\n",
    "            ## bptt train ####\n",
    "            ##################\n",
    "\n",
    "    with tf.name_scope(\"bptt_evaluate\") as scope:\n",
    "        bptt_loss_cross_validiation=tf.losses.mean_squared_error(labels,bptt_output[:,-1,:])\n",
    "\n",
    "    with tf.name_scope('cross_validation_summary') as scope:\n",
    "        tf.summary.scalar('cross_validation_summary',bptt_loss_cross_validiation+1e-10)\n",
    "        bptt_evaluate_summary_op=tf.summary.merge_all(scope=\"cross_validation_summary\")\n",
    "\n",
    "                ##################\n",
    "                # SUMMARIES ######\n",
    "                ##################\n",
    "\n",
    "    with tf.name_scope(\"bptt_weight_summaries\") as scope:\n",
    "\n",
    "        #\n",
    "        #tf.summary.histogram('bptt_kernel_grad',bptt_grad_cost_trainables[0]+1e-10)\n",
    "        #tf.summary.histogram('bptt_kernel', trainables[0]+1e-10)\n",
    "                    # bptt output weight\n",
    "        #tf.summary.histogram('bptt_output_weight_grad',bptt_grad_cost_trainables[1]+1e-10)\n",
    "        #tf.summary.histogram('bptt_output_weights', trainables[1]+1e-10)\n",
    "                    # bptt loss and accuracy\n",
    "        tf.summary.scalar('bptt_loss_output_prediction',bptt_loss_output_prediction+1e-10)\n",
    "\n",
    "        # bptt senstivity tensor and temporal filter\n",
    "        bptt_merged_summary_op=tf.summary.merge_all(scope=\"bptt_weight_summaries\")\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['variable: ', 'hidden_layer/rnn/conductance_spike_cell/kernel:0']\n",
      "['variable: ', -1]\n",
      "['shape: ', (151, 100)]\n",
      "['variable: ', 'output_layer/rnn/output_spike_cell/kernel:0']\n",
      "['variable: ', 0]\n",
      "['shape: ', (100, 1)]\n",
      "['variable: ', 'output_layer/rnn/output_spike_cell/bias:0']\n",
      "['variable: ', 0]\n",
      "['shape: ', (1,)]\n"
     ]
    }
   ],
   "source": [
    "# verify initialization\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "with tf.Session(graph=graph,) as sess : \n",
    "    sess.run(init)\n",
    "    values,trainable_vars = sess.run([variable_names,trainables])\n",
    "    for k, v in zip(variable_names,values):\n",
    "        print([\"variable: \" , k])\n",
    "        #print([\"value: \" , v])\n",
    "        print([\"variable: \" , np.unicode_.find(k,'output')]) \n",
    "        print([\"shape: \" , v.shape])\n",
    "        #print(v) training_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(log_dir).mkdir(exist_ok=True, parents=True)\n",
    "filelist = [ f for f in os.listdir(log_dir) if f.endswith(\".local\") ]\n",
    "for f in filelist:\n",
    "    os.remove(os.path.join(log_dir, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 1,bptt train Loss: 1.115\n",
      "Epoch: 1, Batch: 2,bptt train Loss: 0.935\n",
      "Epoch: 1, Batch: 11,bptt train Loss: 0.917\n",
      "Epoch: 1, Batch: 21,bptt train Loss: 1.430\n",
      "Epoch: 1, Batch: 31,bptt train Loss: 0.932\n",
      "Epoch: 1, Batch: 41,bptt train Loss: 1.013\n",
      "Epoch: 1, Batch: 51,bptt train Loss: 1.487\n",
      "Epoch: 1, Batch: 61,bptt train Loss: 1.233\n",
      "Epoch: 1, Batch: 71,bptt train Loss: 1.309\n",
      "Epoch: 1, Batch: 81,bptt train Loss: 1.067\n",
      "Epoch: 1, Batch: 91,bptt train Loss: 1.224\n",
      "Epoch: 1, Batch: 101,bptt train Loss: 1.331\n",
      "Epoch: 1, Batch: 111,bptt train Loss: 0.959\n",
      "Epoch: 1, Batch: 121,bptt train Loss: 1.199\n",
      "Epoch: 1, Batch: 131,bptt train Loss: 0.990\n",
      "Epoch: 1, Batch: 141,bptt train Loss: 1.070\n",
      "Epoch: 1, Batch: 151,bptt train Loss: 1.311\n",
      "Epoch: 1, Batch: 161,bptt train Loss: 1.252\n",
      "Epoch: 1, Batch: 171,bptt train Loss: 1.033\n",
      "Epoch: 1, Batch: 181,bptt train Loss: 1.470\n",
      "Epoch: 1, Batch: 191,bptt train Loss: 1.065\n",
      "Epoch: 1, Batch: 201,bptt train Loss: 1.283\n",
      "Epoch: 1, Batch: 211,bptt train Loss: 1.212\n",
      "Epoch: 1, Batch: 221,bptt train Loss: 1.208\n",
      "Epoch: 1, Batch: 231,bptt train Loss: 1.263\n",
      "Epoch: 1, Batch: 241,bptt train Loss: 1.307\n",
      "Epoch: 1, Batch: 251,bptt train Loss: 0.924\n",
      "Epoch: 1, Batch: 261,bptt train Loss: 1.258\n",
      "Epoch: 1, Batch: 271,bptt train Loss: 1.314\n",
      "Epoch: 1, Batch: 281,bptt train Loss: 1.020\n",
      "Epoch: 1, Batch: 291,bptt train Loss: 1.246\n",
      "Epoch: 1, Batch: 301,bptt train Loss: 1.082\n",
      "Epoch: 1, Batch: 311,bptt train Loss: 1.170\n",
      "Epoch: 1, Batch: 321,bptt train Loss: 1.228\n",
      "Epoch: 1, Batch: 331,bptt train Loss: 1.285\n",
      "Epoch: 1, Batch: 341,bptt train Loss: 1.161\n",
      "Epoch: 1, Batch: 351,bptt train Loss: 1.283\n",
      "Epoch: 1, Batch: 361,bptt train Loss: 1.115\n",
      "Epoch: 1, Batch: 371,bptt train Loss: 1.022\n",
      "Epoch: 1, Batch: 381,bptt train Loss: 0.956\n",
      "Epoch: 1, Batch: 391,bptt train Loss: 1.158\n",
      "Epoch: 1, Batch: 401,bptt train Loss: 0.910\n",
      "Epoch: 1, Batch: 411,bptt train Loss: 1.131\n",
      "Epoch: 1, Batch: 421,bptt train Loss: 1.133\n",
      "Epoch: 1, Batch: 431,bptt train Loss: 1.269\n",
      "Epoch: 1, Batch: 441,bptt train Loss: 1.085\n",
      "Epoch: 1, Batch: 451,bptt train Loss: 1.312\n",
      "Epoch: 1, Batch: 461,bptt train Loss: 1.342\n",
      "Epoch: 1, Batch: 471,bptt train Loss: 1.456\n",
      "Epoch: 1, Batch: 481,bptt train Loss: 0.988\n",
      "Epoch: 1, Batch: 491,bptt train Loss: 1.129\n",
      "Epoch: 1, Batch: 501,bptt train Loss: 1.054\n",
      "Epoch: 1, Batch: 511,bptt train Loss: 1.285\n",
      "Epoch: 1, Batch: 521,bptt train Loss: 1.070\n",
      "Epoch: 1, Batch: 531,bptt train Loss: 1.297\n",
      "Epoch: 1, Batch: 541,bptt train Loss: 1.169\n",
      "Epoch: 1, Batch: 551,bptt train Loss: 1.195\n",
      "Epoch: 1, Batch: 561,bptt train Loss: 1.020\n",
      "Epoch: 1, Batch: 571,bptt train Loss: 1.033\n",
      "Epoch: 1, Batch: 581,bptt train Loss: 1.045\n",
      "Epoch: 1, Batch: 591,bptt train Loss: 1.174\n",
      "Epoch: 1, Batch: 601,bptt train Loss: 1.370\n",
      "Epoch: 1, Batch: 611,bptt train Loss: 1.170\n",
      "Epoch: 1, Batch: 621,bptt train Loss: 1.093\n",
      "Epoch: 1, Batch: 631,bptt train Loss: 1.625\n",
      "Epoch: 1, Batch: 641,bptt train Loss: 1.291\n",
      "Epoch: 1, Batch: 651,bptt train Loss: 1.298\n",
      "Epoch: 1, Batch: 661,bptt train Loss: 1.302\n",
      "Epoch: 1, Batch: 671,bptt train Loss: 1.343\n",
      "Epoch: 1, Batch: 681,bptt train Loss: 1.307\n",
      "Epoch: 1, Batch: 691,bptt train Loss: 1.062\n",
      "Epoch: 1, Batch: 701,bptt train Loss: 1.060\n",
      "Epoch: 1, Batch: 711,bptt train Loss: 1.222\n",
      "Epoch: 1, Batch: 721,bptt train Loss: 0.984\n",
      "Epoch: 1, Batch: 731,bptt train Loss: 1.356\n",
      "Epoch: 1, Batch: 741,bptt train Loss: 1.493\n",
      "Epoch: 1, Batch: 751,bptt train Loss: 1.116\n",
      "Epoch: 1, Batch: 761,bptt train Loss: 1.069\n",
      "Epoch: 1, Batch: 771,bptt train Loss: 1.275\n",
      "Epoch: 1, Batch: 781,bptt train Loss: 1.088\n",
      "Epoch: 1, Batch: 791,bptt train Loss: 1.089\n",
      "Epoch: 1, Batch: 801,bptt train Loss: 1.244\n",
      "Epoch: 1, Batch: 811,bptt train Loss: 1.111\n",
      "Epoch: 1, Batch: 821,bptt train Loss: 1.240\n",
      "Epoch: 1, Batch: 831,bptt train Loss: 1.503\n",
      "Epoch: 1, Batch: 841,bptt train Loss: 1.284\n",
      "Epoch: 1, Batch: 851,bptt train Loss: 1.331\n",
      "Epoch: 1, Batch: 861,bptt train Loss: 1.428\n",
      "Epoch: 1, Batch: 871,bptt train Loss: 1.206\n",
      "Epoch: 1, Batch: 881,bptt train Loss: 1.463\n",
      "Epoch: 1, Batch: 891,bptt train Loss: 1.226\n",
      "Epoch: 1, Batch: 901,bptt train Loss: 1.356\n",
      "Epoch: 1, Batch: 911,bptt train Loss: 1.452\n",
      "Epoch: 1, Batch: 921,bptt train Loss: 1.065\n",
      "Epoch: 1, Batch: 931,bptt train Loss: 1.019\n",
      "Epoch: 1, Batch: 941,bptt train Loss: 1.196\n",
      "Epoch: 1, Batch: 951,bptt train Loss: 1.149\n",
      "Epoch: 1, Batch: 961,bptt train Loss: 1.150\n",
      "Epoch: 1, Batch: 971,bptt train Loss: 1.312\n",
      "Epoch: 1, Batch: 981,bptt train Loss: 1.002\n",
      "Epoch: 1, Batch: 991,bptt train Loss: 1.229\n",
      "Epoch: 1, cross validation loss :1.160\n",
      "Epoch: 2, Batch: 1,bptt train Loss: 1.167\n",
      "Epoch: 2, Batch: 2,bptt train Loss: 1.047\n",
      "Epoch: 2, Batch: 11,bptt train Loss: 1.105\n",
      "Epoch: 2, Batch: 21,bptt train Loss: 1.664\n",
      "Epoch: 2, Batch: 31,bptt train Loss: 1.214\n",
      "Epoch: 2, Batch: 41,bptt train Loss: 1.503\n",
      "Epoch: 2, Batch: 51,bptt train Loss: 1.091\n",
      "Epoch: 2, Batch: 61,bptt train Loss: 1.436\n",
      "Epoch: 2, Batch: 71,bptt train Loss: 1.110\n",
      "Epoch: 2, Batch: 81,bptt train Loss: 1.542\n",
      "Epoch: 2, Batch: 91,bptt train Loss: 1.141\n",
      "Epoch: 2, Batch: 101,bptt train Loss: 1.067\n",
      "Epoch: 2, Batch: 111,bptt train Loss: 1.714\n",
      "Epoch: 2, Batch: 121,bptt train Loss: 1.343\n",
      "Epoch: 2, Batch: 131,bptt train Loss: 1.161\n",
      "Epoch: 2, Batch: 141,bptt train Loss: 1.190\n",
      "Epoch: 2, Batch: 151,bptt train Loss: 1.026\n",
      "Epoch: 2, Batch: 161,bptt train Loss: 1.158\n",
      "Epoch: 2, Batch: 171,bptt train Loss: 1.082\n",
      "Epoch: 2, Batch: 181,bptt train Loss: 1.067\n",
      "Epoch: 2, Batch: 191,bptt train Loss: 1.349\n",
      "Epoch: 2, Batch: 201,bptt train Loss: 1.423\n",
      "Epoch: 2, Batch: 211,bptt train Loss: 1.042\n",
      "Epoch: 2, Batch: 221,bptt train Loss: 1.119\n",
      "Epoch: 2, Batch: 231,bptt train Loss: 1.475\n",
      "Epoch: 2, Batch: 241,bptt train Loss: 1.195\n",
      "Epoch: 2, Batch: 251,bptt train Loss: 1.184\n",
      "Epoch: 2, Batch: 261,bptt train Loss: 1.172\n",
      "Epoch: 2, Batch: 271,bptt train Loss: 1.234\n",
      "Epoch: 2, Batch: 281,bptt train Loss: 1.001\n",
      "Epoch: 2, Batch: 291,bptt train Loss: 1.095\n",
      "Epoch: 2, Batch: 301,bptt train Loss: 1.227\n",
      "Epoch: 2, Batch: 311,bptt train Loss: 1.403\n",
      "Epoch: 2, Batch: 321,bptt train Loss: 1.149\n",
      "Epoch: 2, Batch: 331,bptt train Loss: 1.014\n",
      "Epoch: 2, Batch: 341,bptt train Loss: 1.424\n",
      "Epoch: 2, Batch: 351,bptt train Loss: 1.045\n",
      "Epoch: 2, Batch: 361,bptt train Loss: 1.004\n",
      "Epoch: 2, Batch: 371,bptt train Loss: 1.174\n",
      "Epoch: 2, Batch: 381,bptt train Loss: 1.069\n",
      "Epoch: 2, Batch: 391,bptt train Loss: 1.189\n",
      "Epoch: 2, Batch: 401,bptt train Loss: 1.140\n",
      "Epoch: 2, Batch: 411,bptt train Loss: 1.150\n",
      "Epoch: 2, Batch: 421,bptt train Loss: 1.503\n",
      "Epoch: 2, Batch: 431,bptt train Loss: 1.314\n",
      "Epoch: 2, Batch: 441,bptt train Loss: 1.372\n",
      "Epoch: 2, Batch: 451,bptt train Loss: 0.896\n",
      "Epoch: 2, Batch: 461,bptt train Loss: 1.102\n",
      "Epoch: 2, Batch: 471,bptt train Loss: 1.018\n",
      "Epoch: 2, Batch: 481,bptt train Loss: 1.532\n",
      "Epoch: 2, Batch: 491,bptt train Loss: 1.655\n",
      "Epoch: 2, Batch: 501,bptt train Loss: 1.482\n",
      "Epoch: 2, Batch: 511,bptt train Loss: 1.155\n",
      "Epoch: 2, Batch: 521,bptt train Loss: 0.936\n",
      "Epoch: 2, Batch: 531,bptt train Loss: 1.151\n",
      "Epoch: 2, Batch: 541,bptt train Loss: 0.990\n",
      "Epoch: 2, Batch: 551,bptt train Loss: 1.221\n",
      "Epoch: 2, Batch: 561,bptt train Loss: 1.155\n",
      "Epoch: 2, Batch: 571,bptt train Loss: 1.125\n",
      "Epoch: 2, Batch: 581,bptt train Loss: 1.113\n",
      "Epoch: 2, Batch: 591,bptt train Loss: 0.945\n",
      "Epoch: 2, Batch: 601,bptt train Loss: 1.129\n",
      "Epoch: 2, Batch: 611,bptt train Loss: 1.184\n",
      "Epoch: 2, Batch: 621,bptt train Loss: 1.135\n",
      "Epoch: 2, Batch: 631,bptt train Loss: 0.918\n",
      "Epoch: 2, Batch: 641,bptt train Loss: 1.311\n",
      "Epoch: 2, Batch: 651,bptt train Loss: 1.340\n",
      "Epoch: 2, Batch: 661,bptt train Loss: 1.132\n",
      "Epoch: 2, Batch: 671,bptt train Loss: 0.975\n",
      "Epoch: 2, Batch: 681,bptt train Loss: 1.268\n",
      "Epoch: 2, Batch: 691,bptt train Loss: 1.356\n",
      "Epoch: 2, Batch: 701,bptt train Loss: 1.326\n",
      "Epoch: 2, Batch: 711,bptt train Loss: 1.187\n",
      "Epoch: 2, Batch: 721,bptt train Loss: 1.350\n",
      "Epoch: 2, Batch: 731,bptt train Loss: 1.158\n",
      "Epoch: 2, Batch: 741,bptt train Loss: 1.255\n",
      "Epoch: 2, Batch: 751,bptt train Loss: 1.067\n",
      "Epoch: 2, Batch: 761,bptt train Loss: 1.132\n",
      "Epoch: 2, Batch: 771,bptt train Loss: 1.190\n",
      "Epoch: 2, Batch: 781,bptt train Loss: 0.970\n",
      "Epoch: 2, Batch: 791,bptt train Loss: 1.555\n",
      "Epoch: 2, Batch: 801,bptt train Loss: 1.288\n",
      "Epoch: 2, Batch: 811,bptt train Loss: 1.393\n",
      "Epoch: 2, Batch: 821,bptt train Loss: 1.531\n",
      "Epoch: 2, Batch: 831,bptt train Loss: 1.257\n",
      "Epoch: 2, Batch: 841,bptt train Loss: 1.276\n",
      "Epoch: 2, Batch: 851,bptt train Loss: 1.365\n",
      "Epoch: 2, Batch: 861,bptt train Loss: 1.842\n",
      "Epoch: 2, Batch: 871,bptt train Loss: 1.075\n",
      "Epoch: 2, Batch: 881,bptt train Loss: 1.258\n",
      "Epoch: 2, Batch: 891,bptt train Loss: 0.950\n",
      "Epoch: 2, Batch: 901,bptt train Loss: 1.367\n",
      "Epoch: 2, Batch: 911,bptt train Loss: 1.011\n",
      "Epoch: 2, Batch: 921,bptt train Loss: 1.423\n",
      "Epoch: 2, Batch: 931,bptt train Loss: 1.055\n",
      "Epoch: 2, Batch: 941,bptt train Loss: 1.098\n",
      "Epoch: 2, Batch: 951,bptt train Loss: 0.897\n",
      "Epoch: 2, Batch: 961,bptt train Loss: 1.149\n",
      "Epoch: 2, Batch: 971,bptt train Loss: 1.525\n",
      "Epoch: 2, Batch: 981,bptt train Loss: 1.007\n",
      "Epoch: 2, Batch: 991,bptt train Loss: 1.293\n",
      "Epoch: 2, cross validation loss :1.156\n",
      "Epoch: 3, Batch: 1,bptt train Loss: 1.482\n",
      "Epoch: 3, Batch: 2,bptt train Loss: 1.134\n",
      "Epoch: 3, Batch: 11,bptt train Loss: 1.237\n",
      "Epoch: 3, Batch: 21,bptt train Loss: 1.334\n",
      "Epoch: 3, Batch: 31,bptt train Loss: 1.252\n",
      "Epoch: 3, Batch: 41,bptt train Loss: 1.560\n",
      "Epoch: 3, Batch: 51,bptt train Loss: 1.136\n",
      "Epoch: 3, Batch: 61,bptt train Loss: 1.255\n",
      "Epoch: 3, Batch: 71,bptt train Loss: 0.997\n",
      "Epoch: 3, Batch: 81,bptt train Loss: 1.083\n",
      "Epoch: 3, Batch: 91,bptt train Loss: 1.252\n",
      "Epoch: 3, Batch: 101,bptt train Loss: 1.062\n",
      "Epoch: 3, Batch: 111,bptt train Loss: 1.278\n",
      "Epoch: 3, Batch: 121,bptt train Loss: 1.154\n",
      "Epoch: 3, Batch: 131,bptt train Loss: 0.910\n",
      "Epoch: 3, Batch: 141,bptt train Loss: 1.143\n",
      "Epoch: 3, Batch: 151,bptt train Loss: 1.155\n",
      "Epoch: 3, Batch: 161,bptt train Loss: 1.048\n",
      "Epoch: 3, Batch: 171,bptt train Loss: 1.207\n",
      "Epoch: 3, Batch: 181,bptt train Loss: 0.979\n",
      "Epoch: 3, Batch: 191,bptt train Loss: 0.897\n",
      "Epoch: 3, Batch: 201,bptt train Loss: 0.841\n",
      "Epoch: 3, Batch: 211,bptt train Loss: 1.396\n",
      "Epoch: 3, Batch: 221,bptt train Loss: 1.031\n",
      "Epoch: 3, Batch: 231,bptt train Loss: 1.252\n",
      "Epoch: 3, Batch: 241,bptt train Loss: 1.258\n",
      "Epoch: 3, Batch: 251,bptt train Loss: 1.187\n",
      "Epoch: 3, Batch: 261,bptt train Loss: 1.371\n",
      "Epoch: 3, Batch: 271,bptt train Loss: 1.192\n",
      "Epoch: 3, Batch: 281,bptt train Loss: 1.121\n",
      "Epoch: 3, Batch: 291,bptt train Loss: 1.295\n",
      "Epoch: 3, Batch: 301,bptt train Loss: 1.139\n",
      "Epoch: 3, Batch: 311,bptt train Loss: 1.397\n",
      "Epoch: 3, Batch: 321,bptt train Loss: 1.251\n",
      "Epoch: 3, Batch: 331,bptt train Loss: 1.240\n",
      "Epoch: 3, Batch: 341,bptt train Loss: 1.180\n",
      "Epoch: 3, Batch: 351,bptt train Loss: 1.049\n",
      "Epoch: 3, Batch: 361,bptt train Loss: 1.297\n",
      "Epoch: 3, Batch: 371,bptt train Loss: 1.183\n",
      "Epoch: 3, Batch: 381,bptt train Loss: 0.959\n",
      "Epoch: 3, Batch: 391,bptt train Loss: 1.226\n",
      "Epoch: 3, Batch: 401,bptt train Loss: 0.886\n",
      "Epoch: 3, Batch: 411,bptt train Loss: 1.074\n",
      "Epoch: 3, Batch: 421,bptt train Loss: 1.081\n",
      "Epoch: 3, Batch: 431,bptt train Loss: 1.109\n",
      "Epoch: 3, Batch: 441,bptt train Loss: 0.929\n",
      "Epoch: 3, Batch: 451,bptt train Loss: 1.100\n",
      "Epoch: 3, Batch: 461,bptt train Loss: 1.245\n",
      "Epoch: 3, Batch: 471,bptt train Loss: 1.310\n",
      "Epoch: 3, Batch: 481,bptt train Loss: 1.052\n",
      "Epoch: 3, Batch: 491,bptt train Loss: 1.624\n",
      "Epoch: 3, Batch: 501,bptt train Loss: 1.197\n",
      "Epoch: 3, Batch: 511,bptt train Loss: 1.108\n",
      "Epoch: 3, Batch: 521,bptt train Loss: 1.436\n",
      "Epoch: 3, Batch: 531,bptt train Loss: 1.308\n",
      "Epoch: 3, Batch: 541,bptt train Loss: 1.089\n",
      "Epoch: 3, Batch: 551,bptt train Loss: 1.441\n",
      "Epoch: 3, Batch: 561,bptt train Loss: 1.293\n",
      "Epoch: 3, Batch: 571,bptt train Loss: 1.211\n",
      "Epoch: 3, Batch: 581,bptt train Loss: 1.116\n",
      "Epoch: 3, Batch: 591,bptt train Loss: 1.261\n",
      "Epoch: 3, Batch: 601,bptt train Loss: 1.337\n",
      "Epoch: 3, Batch: 611,bptt train Loss: 1.726\n",
      "Epoch: 3, Batch: 621,bptt train Loss: 1.161\n",
      "Epoch: 3, Batch: 631,bptt train Loss: 0.842\n",
      "Epoch: 3, Batch: 641,bptt train Loss: 1.130\n",
      "Epoch: 3, Batch: 651,bptt train Loss: 1.229\n",
      "Epoch: 3, Batch: 661,bptt train Loss: 1.370\n",
      "Epoch: 3, Batch: 671,bptt train Loss: 1.323\n",
      "Epoch: 3, Batch: 681,bptt train Loss: 1.153\n",
      "Epoch: 3, Batch: 691,bptt train Loss: 1.118\n",
      "Epoch: 3, Batch: 701,bptt train Loss: 1.019\n",
      "Epoch: 3, Batch: 711,bptt train Loss: 1.176\n",
      "Epoch: 3, Batch: 721,bptt train Loss: 0.949\n",
      "Epoch: 3, Batch: 731,bptt train Loss: 1.270\n",
      "Epoch: 3, Batch: 741,bptt train Loss: 1.321\n",
      "Epoch: 3, Batch: 751,bptt train Loss: 1.085\n",
      "Epoch: 3, Batch: 761,bptt train Loss: 0.877\n",
      "Epoch: 3, Batch: 771,bptt train Loss: 1.257\n",
      "Epoch: 3, Batch: 781,bptt train Loss: 1.120\n",
      "Epoch: 3, Batch: 791,bptt train Loss: 1.277\n",
      "Epoch: 3, Batch: 801,bptt train Loss: 0.942\n",
      "Epoch: 3, Batch: 811,bptt train Loss: 1.063\n",
      "Epoch: 3, Batch: 821,bptt train Loss: 1.475\n",
      "Epoch: 3, Batch: 831,bptt train Loss: 1.431\n",
      "Epoch: 3, Batch: 841,bptt train Loss: 1.265\n",
      "Epoch: 3, Batch: 851,bptt train Loss: 1.429\n",
      "Epoch: 3, Batch: 861,bptt train Loss: 1.358\n",
      "Epoch: 3, Batch: 871,bptt train Loss: 1.291\n",
      "Epoch: 3, Batch: 881,bptt train Loss: 1.205\n",
      "Epoch: 3, Batch: 891,bptt train Loss: 1.287\n",
      "Epoch: 3, Batch: 901,bptt train Loss: 1.017\n",
      "Epoch: 3, Batch: 911,bptt train Loss: 0.875\n",
      "Epoch: 3, Batch: 921,bptt train Loss: 1.412\n",
      "Epoch: 3, Batch: 931,bptt train Loss: 1.051\n",
      "Epoch: 3, Batch: 941,bptt train Loss: 1.598\n",
      "Epoch: 3, Batch: 951,bptt train Loss: 1.134\n",
      "Epoch: 3, Batch: 961,bptt train Loss: 0.777\n",
      "Epoch: 3, Batch: 971,bptt train Loss: 1.140\n",
      "Epoch: 3, Batch: 981,bptt train Loss: 1.085\n",
      "Epoch: 3, Batch: 991,bptt train Loss: 0.995\n",
      "Epoch: 3, cross validation loss :1.149\n",
      "Epoch: 4, Batch: 1,bptt train Loss: 1.118\n",
      "Epoch: 4, Batch: 2,bptt train Loss: 1.039\n",
      "Epoch: 4, Batch: 11,bptt train Loss: 1.523\n",
      "Epoch: 4, Batch: 21,bptt train Loss: 1.370\n",
      "Epoch: 4, Batch: 31,bptt train Loss: 1.083\n",
      "Epoch: 4, Batch: 41,bptt train Loss: 1.213\n",
      "Epoch: 4, Batch: 51,bptt train Loss: 0.993\n",
      "Epoch: 4, Batch: 61,bptt train Loss: 1.765\n",
      "Epoch: 4, Batch: 71,bptt train Loss: 1.702\n",
      "Epoch: 4, Batch: 81,bptt train Loss: 1.063\n",
      "Epoch: 4, Batch: 91,bptt train Loss: 1.206\n",
      "Epoch: 4, Batch: 101,bptt train Loss: 1.266\n",
      "Epoch: 4, Batch: 111,bptt train Loss: 1.409\n",
      "Epoch: 4, Batch: 121,bptt train Loss: 1.253\n",
      "Epoch: 4, Batch: 131,bptt train Loss: 1.361\n",
      "Epoch: 4, Batch: 141,bptt train Loss: 1.323\n",
      "Epoch: 4, Batch: 151,bptt train Loss: 1.393\n",
      "Epoch: 4, Batch: 161,bptt train Loss: 1.370\n",
      "Epoch: 4, Batch: 171,bptt train Loss: 1.033\n",
      "Epoch: 4, Batch: 181,bptt train Loss: 1.282\n",
      "Epoch: 4, Batch: 191,bptt train Loss: 1.164\n",
      "Epoch: 4, Batch: 201,bptt train Loss: 1.183\n",
      "Epoch: 4, Batch: 211,bptt train Loss: 1.063\n",
      "Epoch: 4, Batch: 221,bptt train Loss: 1.092\n",
      "Epoch: 4, Batch: 231,bptt train Loss: 1.322\n",
      "Epoch: 4, Batch: 241,bptt train Loss: 1.239\n",
      "Epoch: 4, Batch: 251,bptt train Loss: 0.997\n",
      "Epoch: 4, Batch: 261,bptt train Loss: 1.368\n",
      "Epoch: 4, Batch: 271,bptt train Loss: 1.102\n",
      "Epoch: 4, Batch: 281,bptt train Loss: 1.454\n",
      "Epoch: 4, Batch: 291,bptt train Loss: 1.109\n",
      "Epoch: 4, Batch: 301,bptt train Loss: 1.135\n",
      "Epoch: 4, Batch: 311,bptt train Loss: 0.849\n",
      "Epoch: 4, Batch: 321,bptt train Loss: 1.208\n",
      "Epoch: 4, Batch: 331,bptt train Loss: 1.511\n",
      "Epoch: 4, Batch: 341,bptt train Loss: 1.004\n",
      "Epoch: 4, Batch: 351,bptt train Loss: 1.455\n",
      "Epoch: 4, Batch: 361,bptt train Loss: 1.056\n",
      "Epoch: 4, Batch: 371,bptt train Loss: 1.101\n",
      "Epoch: 4, Batch: 381,bptt train Loss: 1.295\n",
      "Epoch: 4, Batch: 391,bptt train Loss: 1.165\n",
      "Epoch: 4, Batch: 401,bptt train Loss: 1.245\n",
      "Epoch: 4, Batch: 411,bptt train Loss: 1.172\n",
      "Epoch: 4, Batch: 421,bptt train Loss: 1.079\n",
      "Epoch: 4, Batch: 431,bptt train Loss: 1.260\n",
      "Epoch: 4, Batch: 441,bptt train Loss: 1.152\n",
      "Epoch: 4, Batch: 451,bptt train Loss: 1.111\n",
      "Epoch: 4, Batch: 461,bptt train Loss: 1.086\n",
      "Epoch: 4, Batch: 471,bptt train Loss: 0.902\n",
      "Epoch: 4, Batch: 481,bptt train Loss: 1.061\n",
      "Epoch: 4, Batch: 491,bptt train Loss: 1.062\n",
      "Epoch: 4, Batch: 501,bptt train Loss: 1.511\n",
      "Epoch: 4, Batch: 511,bptt train Loss: 1.622\n",
      "Epoch: 4, Batch: 521,bptt train Loss: 1.100\n",
      "Epoch: 4, Batch: 531,bptt train Loss: 1.202\n",
      "Epoch: 4, Batch: 541,bptt train Loss: 1.134\n",
      "Epoch: 4, Batch: 551,bptt train Loss: 1.262\n",
      "Epoch: 4, Batch: 561,bptt train Loss: 0.851\n",
      "Epoch: 4, Batch: 571,bptt train Loss: 1.513\n",
      "Epoch: 4, Batch: 581,bptt train Loss: 1.017\n",
      "Epoch: 4, Batch: 591,bptt train Loss: 1.151\n",
      "Epoch: 4, Batch: 601,bptt train Loss: 0.816\n",
      "Epoch: 4, Batch: 611,bptt train Loss: 1.199\n",
      "Epoch: 4, Batch: 621,bptt train Loss: 1.143\n",
      "Epoch: 4, Batch: 631,bptt train Loss: 1.336\n",
      "Epoch: 4, Batch: 641,bptt train Loss: 1.088\n",
      "Epoch: 4, Batch: 651,bptt train Loss: 1.386\n",
      "Epoch: 4, Batch: 661,bptt train Loss: 1.085\n",
      "Epoch: 4, Batch: 671,bptt train Loss: 1.145\n",
      "Epoch: 4, Batch: 681,bptt train Loss: 1.140\n",
      "Epoch: 4, Batch: 691,bptt train Loss: 1.438\n",
      "Epoch: 4, Batch: 701,bptt train Loss: 1.474\n",
      "Epoch: 4, Batch: 711,bptt train Loss: 0.899\n",
      "Epoch: 4, Batch: 721,bptt train Loss: 1.301\n",
      "Epoch: 4, Batch: 731,bptt train Loss: 1.302\n",
      "Epoch: 4, Batch: 741,bptt train Loss: 1.170\n",
      "Epoch: 4, Batch: 751,bptt train Loss: 1.240\n",
      "Epoch: 4, Batch: 761,bptt train Loss: 0.978\n",
      "Epoch: 4, Batch: 771,bptt train Loss: 1.118\n",
      "Epoch: 4, Batch: 781,bptt train Loss: 1.468\n",
      "Epoch: 4, Batch: 791,bptt train Loss: 1.275\n",
      "Epoch: 4, Batch: 801,bptt train Loss: 1.523\n",
      "Epoch: 4, Batch: 811,bptt train Loss: 1.050\n",
      "Epoch: 4, Batch: 821,bptt train Loss: 1.071\n",
      "Epoch: 4, Batch: 831,bptt train Loss: 1.074\n",
      "Epoch: 4, Batch: 841,bptt train Loss: 1.145\n",
      "Epoch: 4, Batch: 851,bptt train Loss: 1.123\n",
      "Epoch: 4, Batch: 861,bptt train Loss: 0.804\n",
      "Epoch: 4, Batch: 871,bptt train Loss: 1.045\n",
      "Epoch: 4, Batch: 881,bptt train Loss: 1.144\n",
      "Epoch: 4, Batch: 891,bptt train Loss: 1.320\n",
      "Epoch: 4, Batch: 901,bptt train Loss: 1.351\n",
      "Epoch: 4, Batch: 911,bptt train Loss: 1.099\n",
      "Epoch: 4, Batch: 921,bptt train Loss: 1.500\n",
      "Epoch: 4, Batch: 931,bptt train Loss: 1.036\n",
      "Epoch: 4, Batch: 941,bptt train Loss: 1.038\n",
      "Epoch: 4, Batch: 951,bptt train Loss: 1.122\n",
      "Epoch: 4, Batch: 961,bptt train Loss: 1.952\n",
      "Epoch: 4, Batch: 971,bptt train Loss: 1.131\n",
      "Epoch: 4, Batch: 981,bptt train Loss: 1.538\n",
      "Epoch: 4, Batch: 991,bptt train Loss: 1.113\n",
      "Epoch: 4, cross validation loss :1.138\n",
      "Epoch: 5, Batch: 1,bptt train Loss: 0.806\n",
      "Epoch: 5, Batch: 2,bptt train Loss: 1.333\n",
      "Epoch: 5, Batch: 11,bptt train Loss: 1.067\n",
      "Epoch: 5, Batch: 21,bptt train Loss: 1.145\n",
      "Epoch: 5, Batch: 31,bptt train Loss: 1.264\n",
      "Epoch: 5, Batch: 41,bptt train Loss: 1.193\n",
      "Epoch: 5, Batch: 51,bptt train Loss: 1.146\n",
      "Epoch: 5, Batch: 61,bptt train Loss: 1.272\n",
      "Epoch: 5, Batch: 71,bptt train Loss: 1.062\n",
      "Epoch: 5, Batch: 81,bptt train Loss: 1.108\n",
      "Epoch: 5, Batch: 91,bptt train Loss: 1.252\n",
      "Epoch: 5, Batch: 101,bptt train Loss: 1.319\n",
      "Epoch: 5, Batch: 111,bptt train Loss: 1.104\n",
      "Epoch: 5, Batch: 121,bptt train Loss: 1.263\n",
      "Epoch: 5, Batch: 131,bptt train Loss: 1.046\n",
      "Epoch: 5, Batch: 141,bptt train Loss: 1.175\n",
      "Epoch: 5, Batch: 151,bptt train Loss: 1.333\n",
      "Epoch: 5, Batch: 161,bptt train Loss: 1.503\n",
      "Epoch: 5, Batch: 171,bptt train Loss: 1.406\n",
      "Epoch: 5, Batch: 181,bptt train Loss: 1.266\n",
      "Epoch: 5, Batch: 191,bptt train Loss: 1.468\n",
      "Epoch: 5, Batch: 201,bptt train Loss: 1.231\n",
      "Epoch: 5, Batch: 211,bptt train Loss: 1.264\n",
      "Epoch: 5, Batch: 221,bptt train Loss: 1.034\n",
      "Epoch: 5, Batch: 231,bptt train Loss: 1.059\n",
      "Epoch: 5, Batch: 241,bptt train Loss: 1.001\n",
      "Epoch: 5, Batch: 251,bptt train Loss: 0.986\n",
      "Epoch: 5, Batch: 261,bptt train Loss: 1.286\n",
      "Epoch: 5, Batch: 271,bptt train Loss: 1.172\n",
      "Epoch: 5, Batch: 281,bptt train Loss: 0.925\n",
      "Epoch: 5, Batch: 291,bptt train Loss: 1.162\n",
      "Epoch: 5, Batch: 301,bptt train Loss: 1.132\n",
      "Epoch: 5, Batch: 311,bptt train Loss: 1.065\n",
      "Epoch: 5, Batch: 321,bptt train Loss: 1.433\n",
      "Epoch: 5, Batch: 331,bptt train Loss: 0.812\n",
      "Epoch: 5, Batch: 341,bptt train Loss: 1.334\n",
      "Epoch: 5, Batch: 351,bptt train Loss: 1.146\n",
      "Epoch: 5, Batch: 361,bptt train Loss: 1.060\n",
      "Epoch: 5, Batch: 371,bptt train Loss: 0.693\n",
      "Epoch: 5, Batch: 381,bptt train Loss: 0.966\n",
      "Epoch: 5, Batch: 391,bptt train Loss: 1.447\n",
      "Epoch: 5, Batch: 401,bptt train Loss: 1.369\n",
      "Epoch: 5, Batch: 411,bptt train Loss: 1.075\n",
      "Epoch: 5, Batch: 421,bptt train Loss: 1.026\n",
      "Epoch: 5, Batch: 431,bptt train Loss: 1.148\n",
      "Epoch: 5, Batch: 441,bptt train Loss: 1.097\n",
      "Epoch: 5, Batch: 451,bptt train Loss: 1.181\n",
      "Epoch: 5, Batch: 461,bptt train Loss: 1.789\n",
      "Epoch: 5, Batch: 471,bptt train Loss: 1.058\n",
      "Epoch: 5, Batch: 481,bptt train Loss: 1.563\n",
      "Epoch: 5, Batch: 491,bptt train Loss: 1.669\n",
      "Epoch: 5, Batch: 501,bptt train Loss: 0.983\n",
      "Epoch: 5, Batch: 511,bptt train Loss: 1.026\n",
      "Epoch: 5, Batch: 521,bptt train Loss: 1.200\n",
      "Epoch: 5, Batch: 531,bptt train Loss: 0.950\n",
      "Epoch: 5, Batch: 541,bptt train Loss: 1.171\n",
      "Epoch: 5, Batch: 551,bptt train Loss: 1.113\n",
      "Epoch: 5, Batch: 561,bptt train Loss: 1.301\n",
      "Epoch: 5, Batch: 571,bptt train Loss: 1.431\n",
      "Epoch: 5, Batch: 581,bptt train Loss: 1.381\n",
      "Epoch: 5, Batch: 591,bptt train Loss: 1.192\n",
      "Epoch: 5, Batch: 601,bptt train Loss: 1.374\n",
      "Epoch: 5, Batch: 611,bptt train Loss: 1.042\n",
      "Epoch: 5, Batch: 621,bptt train Loss: 1.251\n",
      "Epoch: 5, Batch: 631,bptt train Loss: 1.109\n",
      "Epoch: 5, Batch: 641,bptt train Loss: 1.063\n",
      "Epoch: 5, Batch: 651,bptt train Loss: 0.870\n",
      "Epoch: 5, Batch: 661,bptt train Loss: 1.247\n",
      "Epoch: 5, Batch: 671,bptt train Loss: 1.109\n",
      "Epoch: 5, Batch: 681,bptt train Loss: 1.188\n",
      "Epoch: 5, Batch: 691,bptt train Loss: 1.208\n",
      "Epoch: 5, Batch: 701,bptt train Loss: 1.681\n",
      "Epoch: 5, Batch: 711,bptt train Loss: 1.280\n",
      "Epoch: 5, Batch: 721,bptt train Loss: 1.205\n",
      "Epoch: 5, Batch: 731,bptt train Loss: 1.347\n",
      "Epoch: 5, Batch: 741,bptt train Loss: 0.948\n",
      "Epoch: 5, Batch: 751,bptt train Loss: 1.484\n",
      "Epoch: 5, Batch: 761,bptt train Loss: 1.270\n",
      "Epoch: 5, Batch: 771,bptt train Loss: 1.090\n",
      "Epoch: 5, Batch: 781,bptt train Loss: 1.132\n",
      "Epoch: 5, Batch: 791,bptt train Loss: 1.044\n",
      "Epoch: 5, Batch: 801,bptt train Loss: 1.254\n",
      "Epoch: 5, Batch: 811,bptt train Loss: 1.203\n",
      "Epoch: 5, Batch: 821,bptt train Loss: 1.274\n",
      "Epoch: 5, Batch: 831,bptt train Loss: 0.935\n",
      "Epoch: 5, Batch: 841,bptt train Loss: 0.938\n",
      "Epoch: 5, Batch: 851,bptt train Loss: 1.095\n",
      "Epoch: 5, Batch: 861,bptt train Loss: 1.081\n",
      "Epoch: 5, Batch: 871,bptt train Loss: 1.056\n",
      "Epoch: 5, Batch: 881,bptt train Loss: 1.428\n",
      "Epoch: 5, Batch: 891,bptt train Loss: 1.065\n",
      "Epoch: 5, Batch: 901,bptt train Loss: 1.220\n",
      "Epoch: 5, Batch: 911,bptt train Loss: 1.174\n",
      "Epoch: 5, Batch: 921,bptt train Loss: 1.201\n",
      "Epoch: 5, Batch: 931,bptt train Loss: 1.454\n",
      "Epoch: 5, Batch: 941,bptt train Loss: 1.347\n",
      "Epoch: 5, Batch: 951,bptt train Loss: 1.381\n",
      "Epoch: 5, Batch: 961,bptt train Loss: 0.928\n",
      "Epoch: 5, Batch: 971,bptt train Loss: 1.113\n",
      "Epoch: 5, Batch: 981,bptt train Loss: 1.284\n",
      "Epoch: 5, Batch: 991,bptt train Loss: 1.239\n",
      "Epoch: 5, cross validation loss :1.145\n",
      "Optimization Finished!\n",
      "Model saved in path: /home/eghbal/MyData/KeRNL/logs/bptt_snn_addition_dataset/bp_snn_add_T_5e+01_eta_W_1e-08_batch_2e+01_hum_hidd_1e+02_gc_1e+02_steps_1e+03_epoch_5e+00_run_20190308_1511/model.ckpt-999\n"
     ]
    }
   ],
   "source": [
    "# write graph into tensorboard \n",
    "tb_writer = tf.summary.FileWriter(log_dir,graph)\n",
    "# run a training session \n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(epochs):\n",
    "        sess.run(iter.initializer,feed_dict={X: training_x, Y: training_y , BATCH_SIZE: batch_size})\n",
    "        for step in range(training_steps): \n",
    "            bptt_train, bptt_loss,bptt_merged_summary=sess.run([bptt_weight_train_op,bptt_loss_output_prediction,bptt_merged_summary_op])\n",
    "            tb_writer.add_summary(bptt_merged_summary, global_step=step)\n",
    "\n",
    "            if step % display_step==0 or step==1 : \n",
    "                print('Epoch: {}, Batch: {},bptt train Loss: {:.3f}'.format(epoch+1,step + 1, bptt_loss))\n",
    "                \n",
    "        # run test at the end of each epoch \n",
    "        sess.run(iter.initializer, feed_dict={ X: testing_x, Y: testing_y, BATCH_SIZE: testing_x.shape[0]})    \n",
    "        bptt_test_loss, bptt_evaluate_summary=sess.run([bptt_loss_cross_validiation,bptt_evaluate_summary_op])        \n",
    "        tb_writer.add_summary(bptt_evaluate_summary, global_step=step)\n",
    "        print('Epoch: {}, cross validation loss :{:.3f}'.format(epoch+1,bptt_test_loss))\n",
    "            \n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    save_path = saver.save(sess, log_dir+\"/model.ckpt\", global_step=step,write_meta_graph=True)\n",
    "    print(\"Model saved in path: %s\" % save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
