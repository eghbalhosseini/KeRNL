{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code for creating a Kernel based relu-RNN learning for sequential MNIST\n",
    "adapted from : Roth, Christopher, Ingmar Kanitscheider, and Ila Fiete. 2018. “Kernel RNN Learning (KeRNL),” September. https://openreview.net/forum?id=ryGfnoC5KQ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import collections\n",
    "import hashlib\n",
    "import numbers\n",
    "import matplotlib.cm as cm\n",
    "from sys import getsizeof\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "from tensorflow.python.eager import context\n",
    "from tensorflow.python.framework import constant_op\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.framework import tensor_util\n",
    "from tensorflow.python.layers import base as base_layer\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import clip_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.ops import partitioned_variables\n",
    "from tensorflow.python.ops import random_ops\n",
    "from tensorflow.python.ops import tensor_array_ops\n",
    "from tensorflow.python.ops import variable_scope as vs\n",
    "from tensorflow.python.ops import variables as tf_variables\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "from tensorflow.python.util import nest\n",
    "from tensorflow.contrib.rnn.python.ops.core_rnn_cell import _Linear\n",
    "from tensorflow.contrib import slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# uplading mnist data \n",
    "\n",
    "old_v = tf.logging.get_verbosity()\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "train_data = mnist.train.images  # Returns np.array\n",
    "train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "eval_data = mnist.test.images  # Returns np.array\n",
    "eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)\n",
    "\n",
    "tf.logging.set_verbosity(old_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 1e-4\n",
    "training_steps = 5000\n",
    "batch_size = 128\n",
    "display_step = 200\n",
    "test_len=128\n",
    "grad_clip=100\n",
    "# Network Parameters\n",
    "num_input = 1 # MNIST data input (img shape: 28*28)\n",
    "timesteps = 28*28 # timesteps\n",
    "num_hidden = 128 # hidden layer num of features\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_KernelRNNStateTuple = collections.namedtuple(\"KernelRNNStateTuple\", (\"h\",\"h_hat\",\"Theta\", \"Gamma\",\"input_trace\",\"recurrent_trace\",\"sensitivty_tensor\",\"kernel_coeff\"))\n",
    "_KernelRNNOutputTuple = collections.namedtuple(\"KernelRNNOutputTuple\", (\"h\",\"h_hat\",\"Theta\",\"Gamma\", \"input_trace\",\"recurrent_trace\",\"sensitivty_tensor\",\"kernel_coeff\"))\n",
    "\n",
    "class KernelRNNStateTuple(_KernelRNNStateTuple):\n",
    "  \"\"\"Tuple used by kernel RNN Cells for `state_variables `.\n",
    "  Stores 8 elements: `(h, h_hat, Theta, Gamma, input_trace,recurrent_trace, sensitivty_tensor, kernel_coeff`, in that order. \n",
    "  always is used for this type of cell\n",
    "  \"\"\"\n",
    "  __slots__ = ()\n",
    "\n",
    "  @property\n",
    "  def dtype(self):\n",
    "    (h, h_hat,Theta , Gamma, input_trace,recurrent_trace, sensitivity_tensor, kernel_coeff ) = self\n",
    "    if h.dtype != h_hat.dtype:\n",
    "      raise TypeError(\"Inconsistent internal state: %s vs %s\" %\n",
    "                      (str(h.dtype), str(h_hat.dtype)))\n",
    "    return h_hat.dtype\n",
    "\n",
    "\n",
    "class KernelRNNOutputTuple(_KernelRNNOutputTuple):\n",
    "  \"\"\"Tuple used by kernel Cells for output state.\n",
    "  Stores 6 elements: `(h,h_hat, Theta, Gamma, input_trace, recurrent_trace)`, \n",
    "  Only used when `output_is_tuple=True`.\n",
    "  \"\"\"\n",
    "  __slots__ = ()\n",
    "\n",
    "  @property\n",
    "  def dtype(self):\n",
    "    (h, h_hat,Theta , Gamma, input_trace,recurrent_trace, sensitivity_tensor, kernel_coeff) = self\n",
    "    if h.dtype != h_hat.dtype:\n",
    "      raise TypeError(\"Inconsistent internal state: %s vs %s\" %\n",
    "                      (str(h.dtype), str(h_hat.dtype)))\n",
    "    return h_hat.dtype\n",
    "#################################################################\n",
    "def _tensor_expand_dim(x,y,output_size):\n",
    "    \"\"\"input - x : a 2D tensor with batch x n \n",
    "    y is a 2D with size batch x m\n",
    "    outputs is 3D tensor with size batch x n x n and batch x n x m \n",
    "    \"\"\" \n",
    "    shape_x=x.get_shape()\n",
    "    shape_y=y.get_shape()\n",
    "    #y=tf.cast(y,tf.float32)\n",
    "    # define a matrix for removing the diagonal in recurrent spikes \n",
    "    diag_zero= lambda:tf.subtract(tf.constant(1.0,shape=[shape_x[1],shape_x[1]]),\n",
    "                                                    tf.eye(output_size))\n",
    "    x_diag_fixer = tf.Variable(initial_value=diag_zero, dtype=tf.float32)\n",
    "    # expand x  \n",
    "    x_temp=tf.reshape(tf.tile(x,[1,output_size]),[-1,output_size,shape_x[1]])\n",
    "    # remove diagonal \n",
    "    x_expand=tf.multiply(x_temp,x_diag_fixer)\n",
    "    # expand y  \n",
    "    y_expand=tf.reshape(tf.tile(y,[1,output_size]),[-1,output_size,shape_y[1]])\n",
    "    return x_expand, y_expand\n",
    "\n",
    "def _create_pertubation(x,mean,std):\n",
    "    shape=x.get_shape()\n",
    "    logging.warn(\"%s: Please use float \", [shape[0].value,shape[1].value])\n",
    "    scope=vs.get_variable_scope()\n",
    "    with vs.variable_scope (scope) as perturbation_scope:\n",
    "        perturbation=tf.constant(1.0,shape=[shape[0].value,shape[1].value])\n",
    "    #perturbation=tf.constant(tf.random_normal(shape=[shape[0].value,shape[1].value], mean=mean,stddev=std))\n",
    "    \n",
    "    return perturbation\n",
    "\n",
    "def _gaussian_noise_perturbation(input_layer, std):\n",
    "    noise = tf.random_normal(shape=tf.shape(input_layer), mean=0.0, stddev=std, dtype=tf.float32) \n",
    "    return tf.multiply(input_layer,0) + noise\n",
    "\n",
    "\n",
    "def _kernel_coeff_initializer(shape,dtype=None,partition_info=None,verify_shape=None, max_val=1):\n",
    "    if dtype is None: \n",
    "        dtype=tf.float32\n",
    "        \n",
    "    return tf.random_uniform(shape,0,max_val,dtype=dtype)\n",
    "###################################################################\n",
    "_KERNEL_COEF_NAME= \"kernel_coeff\"\n",
    "_SENSITIVITY_TENSOR_NAME= \"sensitivity_tensor\"\n",
    "\n",
    "class KernelRNNCell(tf.contrib.rnn.RNNCell):\n",
    "    \"\"\"Kernel recurrent neural network Cell\n",
    "      Args:\n",
    "        num_units: int, The number of units in the cell.\n",
    "        activation: Nonlinearity to use.  Default: `Relu`.\n",
    "        eligibility_kernel: kernel funtion to use for elibility \n",
    "        reuse: (optional) Python boolean describing whether to reuse variables\n",
    "         in an existing scope.  If not `True`, and the existing scope already has\n",
    "         the given variables, an error is raised.\n",
    "        kernel_initializer: (optional) The initializer to use for the weight and\n",
    "        projection matrices.\n",
    "        bias_initializer: (optional) The initializer to use for the bias.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 num_units,\n",
    "                 num_inputs,\n",
    "                 time_steps=1,\n",
    "                 noise_std=1.0,\n",
    "                 activation=None,\n",
    "                 reuse=None,\n",
    "                 eligibility_kernel=None,\n",
    "                 state_is_tuple=True,\n",
    "                 output_is_tuple=False,\n",
    "                 batch_KeRNL=True,\n",
    "                 sensitivity_initializer=None,\n",
    "                 kernel_coeff_initializer=None,\n",
    "                 kernel_initializer=None,\n",
    "                 bias_initializer=None):\n",
    "        \n",
    "        super(KernelRNNCell,self).__init__(_reuse=reuse)\n",
    "        self._num_units = num_units\n",
    "        self._num_inputs= num_inputs\n",
    "        self._time_steps= time_steps\n",
    "        self._activation = activation or math_ops.tanh\n",
    "        self._eligibility_kernel = eligibility_kernel or math_ops.exp\n",
    "        self._noise_std=noise_std\n",
    "        self._linear = None\n",
    "        self._state_is_tuple=state_is_tuple\n",
    "        self._output_is_tuple= output_is_tuple\n",
    "        self._batch_KeRNL=batch_KeRNL\n",
    "        self._tensor_expand_dim=_tensor_expand_dim\n",
    "        self._gaussian_noise_perturbation=_gaussian_noise_perturbation\n",
    "        self._sensitivity_initializer=sensitivity_initializer\n",
    "        self._kernel_coeff_initializer=kernel_coeff_initializer\n",
    "        self._kernel_initializer=kernel_initializer\n",
    "        self._bias_initializer=bias_initializer\n",
    "    \n",
    "    @property\n",
    "    # h,h_hat,Theta, Gamma,input_trace,recurrent_trace,sensitivty_tensor,kernel_coeff\n",
    "    def state_size(self):\n",
    "        return (KernelRNNStateTuple(self._num_units, \n",
    "                                    self._num_units, \n",
    "                                    self._num_units, \n",
    "                                    self._num_units, \n",
    "                                    np.array([self._num_units,self._num_inputs]), \n",
    "                                    np.array([self._num_units,self._num_units]),\n",
    "                                    np.array([self._num_units,self._num_units]),\n",
    "                                    self._num_units)\n",
    "                if self._state_is_tuple else self._num_units)\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return (KernelRNNOutputTuple(self._num_units, \n",
    "                                    self._num_units, \n",
    "                                    self._num_units, \n",
    "                                    self._num_units, \n",
    "                                    np.array([self._num_units,self._num_inputs]), \n",
    "                                    np.array([self._num_units,self._num_units]),\n",
    "                                    np.array([self._num_units,self._num_units]),\n",
    "                                    self._num_units)\n",
    "                if self._output_is_tuple else self._num_units)\n",
    "\n",
    "    # call function routine \n",
    "    def call(self, inputs, state):\n",
    "        \"\"\"Kernel RNN cell (KernelRNN).\n",
    "        Args:\n",
    "          inputs: `2-D` tensor with shape `[batch_size x input_size]`.\n",
    "          state: An `KernelRNNStateTuple` of state tensors, shaped as following \n",
    "            h:                   [batch_size x self.state_size]`\n",
    "            h_hat:               [batch_size x self.state_size]`\n",
    "            Theta:               [batch_size x self.state_size]`\n",
    "            Gamma:               [batch_size x self.state_size]`\n",
    "            input_trace          [batch_size x self.state_size x self.input_size]`\n",
    "            recurrent_trace      [batch_size x self.state_size x self.state_size]`\n",
    "            sensitivity_tensor   [batch_size x self.state_size x self.state_size]`\n",
    "            kernel coeff         [batch_size x self.state_size]`\n",
    "        Returns:\n",
    "          A pair containing the new output, and the new state as SNNStateTuple\n",
    "          output has the following shape \n",
    "            h:                   [batch_size x self.state_size]`\n",
    "            h_hat:               [batch_size x self.state_size]`\n",
    "            Theta:               [batch_size x self.state_size]`\n",
    "            Gamma:               [batch_size x self.state_size]`\n",
    "            input_trace          [batch_size x self.state_size x self.input_size]`\n",
    "            recurrent_trace      [batch_size x self.state_size x self.state_size]`\n",
    "            sensitivity_tensor   [batch_size x self.state_size x self.state_size]`\n",
    "            kernel coeff         [batch_size x self.state_size]`\n",
    "        \"\"\"\n",
    "        # initialize kernel_coeff\n",
    "        scope=vs.get_variable_scope()\n",
    "        if self._kernel_coeff_initializer is None:\n",
    "            kernel_initializer=init_ops.constant_initializer(1/self._time_steps,dtype=tf.float32)\n",
    "        else: \n",
    "            kernel_initializer=self._kernel_coeff_initializer\n",
    "        with vs.variable_scope(scope,initializer=kernel_initializer) as kernel_scope:\n",
    "            kernel_coeff=tf.get_variable(_KERNEL_COEF_NAME,shape=[self._num_units],dtype=tf.float32,trainable=True)\n",
    "        \n",
    "        # initialize Sensitivity_tensor\n",
    "        scope=vs.get_variable_scope()\n",
    "        if self._sensitivity_initializer is None:\n",
    "            sensitivity_initializer=init_ops.truncated_normal_initializer\n",
    "        else: \n",
    "            sensitivity_initializer=self._sensitivity_initializer\n",
    "        with vs.variable_scope(scope,initializer=sensitivity_initializer) as sensitivity_scope:\n",
    "            sensitivity_tensor=tf.get_variable(_SENSITIVITY_TENSOR_NAME,shape=[self._num_units,self._num_units],dtype=tf.float32,trainable=True)\n",
    "        \n",
    "            \n",
    "        \n",
    "        if self._state_is_tuple:\n",
    "            h, h_hat, Theta, Gamma, input_trace, recurrent_trace, sensitivity_tensor, kernel_coeff= state\n",
    "        else:\n",
    "            logging.error(\"State has to be tuple for this type of cell\")\n",
    "        \n",
    "        if self._linear is None: \n",
    "            self._linear = _Linear([inputs, h], self._num_units, True)\n",
    "        psi_new=self._gaussian_noise_perturbation(h,self._noise_std)\n",
    "        # propagate data forward \n",
    "        h_new=self._activation(self._linear([inputs,h]))\n",
    "        # propagate noisy data forward \n",
    "        h_hat_update=tf.add(h_hat,psi_new)\n",
    "        h_hat_new= self._activation(self._linear([inputs, h_hat_update]))\n",
    "        # TODO : check of weights get reused \n",
    "        # integrate over perturbations\n",
    "        Theta_new=tf.add(tf.multiply(self._eligibility_kernel(-kernel_coeff),\n",
    "                              Theta),psi_new)\n",
    "        # derivative of perturbation w.r.t to kernel_coeff\n",
    "        Gamma_new=tf.subtract(tf.multiply(self._eligibility_kernel(-kernel_coeff),\n",
    "                              Gamma),\n",
    "                             tf.multiply(self._eligibility_kernel(-kernel_coeff),\n",
    "                              Theta))\n",
    "        # update elgibility traces for input and recurrent units \n",
    "        recurrent_expand,inputs_expand=self._tensor_expand_dim(h,inputs,self._num_units)\n",
    "        #\n",
    "        g_new=self._linear([inputs,h])\n",
    "        pre_activation=self._activation(g_new)\n",
    "        activation_gradients=tf.gradients(pre_activation,g_new)[0] # convert list to a tensor \n",
    "        gradient_expansion=tf.expand_dims(activation_gradients,axis=-1)\n",
    "        input_trace_update=tf.multiply(gradient_expansion,inputs_expand)\n",
    "        recurrent_trace_update=tf.multiply(gradient_expansion,recurrent_expand)\n",
    "        #logging.warn(\"%s: input_trace \", input_trace.get_shape())\n",
    "        kernel_decay=tf.expand_dims(self._eligibility_kernel(-kernel_coeff),axis=-1)\n",
    "        # update input trace \n",
    "        input_trace_decay=tf.multiply(kernel_decay,input_trace)\n",
    "        input_trace_new=tf.add(input_trace_decay,input_trace_update)\n",
    "\n",
    "        # update recurrent trace  \n",
    "        recurrent_trace_decay=tf.multiply(kernel_decay,recurrent_trace)\n",
    "        recurrent_trace_new=tf.add(recurrent_trace_decay,recurrent_trace_update)\n",
    "\n",
    "        # TODO implement online updating for sensitivity and kernel coeff\n",
    "        \n",
    "        \n",
    "        if self._state_is_tuple: \n",
    "            new_state=KernelRNNStateTuple(h_new,h_hat_new,Theta_new,Gamma_new,input_trace_new,\n",
    "                                          recurrent_trace_new,sensitivity_tensor,kernel_coeff)\n",
    "        if self._output_is_tuple:\n",
    "            new_output=KernelRNNOutputTuple(h_new,h_hat_new,Theta_new,Gamma_new,input_trace_new,\n",
    "                                          recurrent_trace_new,sensitivity_tensor,kernel_coeff)\n",
    "        else:\n",
    "            new_output=h_new\n",
    "\n",
    "        return new_output, new_state\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "    with tf.variable_scope('recurrent',initializer=tf.initializers.identity()) as scope: \n",
    "        # Define a lstm cell with tensorflow\n",
    "        kernel_cell = KernelRNNCell(num_units=num_hidden,num_inputs=num_input,time_steps=timesteps)\n",
    "        # Get lstm cell output\n",
    "        kernel_outputs, kernel_states = tf.nn.dynamic_rnn(kernel_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(kernel_outputs[:,-1,:], weights['out']) + biases['out'], kernel_outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "graph=tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Define weights\n",
    "    weights = {\n",
    "        'out': tf.Variable(tf.random_normal([num_hidden, num_classes]),name='output_weight')\n",
    "    }\n",
    "    biases = {\n",
    "        'out': tf.Variable(tf.random_normal([num_classes]),name='output_bias')\n",
    "    }\n",
    "    X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "    Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "    logits,output = kernel_RNN(X, weights, biases)\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "    variable_names=[v.name for v in tf.trainable_variables()]\n",
    "    # Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate)\n",
    "    # compute gradients \n",
    "    grads_and_vars=optimizer.compute_gradients(loss_op)\n",
    "    # clip the gradient based on norm clipping:  g^ <-- threshold/l2_norm(g^)*g^\n",
    "    #cropped_grads_and_vars=[(tf.clip_by_norm(grad, 100),var) if  np.unicode_.find(var.name,'output')==-1 else (grad,var) for grad,var in grads_and_vars]\n",
    "    train_op = optimizer.minimize(loss_op)\n",
    "    # Evaluate model (with test logits, for dropout to be disabled)\n",
    "    correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    # Initialize the variables (i.e. assign their default value)\n",
    "    init = tf.global_variables_initializer()\n",
    "        # predictions \n",
    "        #prediction=tf.nn.softmax(logits)\n",
    "    tf.summary.histogram('prediction',prediction+1e-8)\n",
    "    tf.summary.histogram('logits',logits+1e-8)\n",
    "    tf.summary.scalar('loss',loss_op)\n",
    "    merged_summary_op=tf.summary.merge_all()\n",
    "    # define loss \n",
    "        #loss_op=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=Y))\n",
    "        # optimization loop \n",
    "        #tf.summary.scalar('loss',loss_op)\n",
    "        #tf.summary.histogram('logits',logits)    \n",
    "        #optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        #gradients=optimizer.compute_gradients(loss_op)\n",
    "        #capped_gvs = [(tf.clip_by_norm(grad, 1.), var) if not var.name.startswith(\"dense\") else (grad, var) for grad, var in gradients]\n",
    "        #for _, var in gradients:\n",
    "        #    if var.name.startswith(\"dense\"):\n",
    "        #        print(var.name)   \n",
    "        #train_op=optimizer.apply_gradients(capped_gvs)\n",
    "        # initialize variables \n",
    "    merged_summary_op=tf.summary.merge_all()\n",
    "        \n",
    "        #saver=tf.train.Saver()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['variable: ', 'output_weight:0']\n",
      "['value: ', array([[-0.5562263 , -1.5679277 , -0.32520884, ..., -0.4588559 ,\n",
      "         1.7650766 ,  1.5129395 ],\n",
      "       [-1.0631367 ,  0.09719802,  0.49336594, ...,  0.3012817 ,\n",
      "        -0.00972463,  0.7842659 ],\n",
      "       [-1.3930802 ,  0.2786544 ,  0.5650163 , ...,  3.3679826 ,\n",
      "         0.98051184, -0.88528967],\n",
      "       ...,\n",
      "       [ 1.3211502 ,  0.14028516, -1.0883702 , ..., -0.4652753 ,\n",
      "         0.11319174,  0.8593472 ],\n",
      "       [ 1.277668  ,  0.21948794,  0.18592085, ..., -0.41221184,\n",
      "        -0.3877456 ,  1.730171  ],\n",
      "       [ 0.94079083,  0.83154386, -0.53782547, ..., -1.162312  ,\n",
      "        -0.4911751 ,  1.3081511 ]], dtype=float32)]\n",
      "['variable: ', 0]\n",
      "['shape: ', (128, 10)]\n",
      "['variable: ', 'output_bias:0']\n",
      "['value: ', array([-0.9497624 , -0.63880116, -0.6099232 , -0.55430096,  1.1011579 ,\n",
      "        1.337941  , -0.70203036, -1.3880402 , -0.7286829 ,  0.69202995],\n",
      "      dtype=float32)]\n",
      "['variable: ', 0]\n",
      "['shape: ', (10,)]\n",
      "['variable: ', 'recurrent/rnn/kernel_rnn_cell/kernel_coeff:0']\n",
      "['value: ', array([0.00127551, 0.00127551, 0.00127551, 0.00127551, 0.00127551,\n",
      "       0.00127551, 0.00127551, 0.00127551, 0.00127551, 0.00127551,\n",
      "       0.00127551, 0.00127551, 0.00127551, 0.00127551, 0.00127551,\n",
      "       0.00127551, 0.00127551, 0.00127551, 0.00127551, 0.00127551,\n",
      "       0.00127551, 0.00127551, 0.00127551, 0.00127551, 0.00127551,\n",
      "       0.00127551, 0.00127551, 0.00127551, 0.00127551, 0.00127551,\n",
      "       0.00127551, 0.00127551, 0.00127551, 0.00127551, 0.00127551,\n",
      "       0.00127551, 0.00127551, 0.00127551, 0.00127551, 0.00127551,\n",
      "       0.00127551, 0.00127551, 0.00127551, 0.00127551, 0.00127551,\n",
      "       0.00127551, 0.00127551, 0.00127551, 0.00127551, 0.00127551,\n",
      "       0.00127551, 0.00127551, 0.00127551, 0.00127551, 0.00127551,\n",
      "       0.00127551, 0.00127551, 0.00127551, 0.00127551, 0.00127551,\n",
      "       0.00127551, 0.00127551, 0.00127551, 0.00127551, 0.00127551,\n",
      "       0.00127551, 0.00127551, 0.00127551, 0.00127551, 0.00127551,\n",
      "       0.00127551, 0.00127551, 0.00127551, 0.00127551, 0.00127551,\n",
      "       0.00127551, 0.00127551, 0.00127551, 0.00127551, 0.00127551,\n",
      "       0.00127551, 0.00127551, 0.00127551, 0.00127551, 0.00127551,\n",
      "       0.00127551, 0.00127551, 0.00127551, 0.00127551, 0.00127551,\n",
      "       0.00127551, 0.00127551, 0.00127551, 0.00127551, 0.00127551,\n",
      "       0.00127551, 0.00127551, 0.00127551, 0.00127551, 0.00127551,\n",
      "       0.00127551, 0.00127551, 0.00127551, 0.00127551, 0.00127551,\n",
      "       0.00127551, 0.00127551, 0.00127551, 0.00127551, 0.00127551,\n",
      "       0.00127551, 0.00127551, 0.00127551, 0.00127551, 0.00127551,\n",
      "       0.00127551, 0.00127551, 0.00127551, 0.00127551, 0.00127551,\n",
      "       0.00127551, 0.00127551, 0.00127551, 0.00127551, 0.00127551,\n",
      "       0.00127551, 0.00127551, 0.00127551], dtype=float32)]\n",
      "['variable: ', -1]\n",
      "['shape: ', (128,)]\n",
      "['variable: ', 'recurrent/rnn/kernel_rnn_cell/sensitivity_tensor:0']\n",
      "['value: ', array([[-1.2398295 ,  0.13620298, -0.01797176, ..., -0.8629357 ,\n",
      "         0.44551164, -0.46363938],\n",
      "       [-0.04272575,  0.88499516,  1.1388842 , ..., -1.0360416 ,\n",
      "         0.7494906 , -0.5788593 ],\n",
      "       [-0.30127645, -0.14290395, -0.944587  , ...,  1.1280376 ,\n",
      "        -0.7849274 , -0.7394482 ],\n",
      "       ...,\n",
      "       [ 0.6303212 , -1.1234643 , -0.8029639 , ..., -1.2516207 ,\n",
      "        -0.51140445, -1.0574858 ],\n",
      "       [-0.7806891 , -1.3080094 ,  1.0735054 , ...,  0.07216673,\n",
      "        -0.1860579 ,  0.8330304 ],\n",
      "       [ 0.43154627,  0.46085712,  0.1061144 , ...,  0.65708107,\n",
      "        -0.20769086, -1.7033584 ]], dtype=float32)]\n",
      "['variable: ', -1]\n",
      "['shape: ', (128, 128)]\n",
      "['variable: ', 'recurrent/rnn/kernel_rnn_cell/kernel:0']\n",
      "['value: ', array([[1., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 1., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 1., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 1., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 1.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)]\n",
      "['variable: ', -1]\n",
      "['shape: ', (129, 128)]\n",
      "['variable: ', 'recurrent/rnn/kernel_rnn_cell/bias:0']\n",
      "['value: ', array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "['variable: ', -1]\n",
      "['shape: ', (128,)]\n",
      "['variable: ', 'recurrent/rnn/while/rnn/kernel_rnn_cell/Variable:0']\n",
      "['value: ', array([[0., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 0., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 0., ..., 1., 1., 1.],\n",
      "       ...,\n",
      "       [1., 1., 1., ..., 0., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 0., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 0.]], dtype=float32)]\n",
      "['variable: ', -1]\n",
      "['shape: ', (128, 128)]\n"
     ]
    }
   ],
   "source": [
    "# verify initialization \n",
    "with tf.Session(graph=graph) as sess : \n",
    "    sess.run(init)\n",
    "    values = sess.run(variable_names)\n",
    "    for k, v in zip(variable_names,values):\n",
    "        print([\"variable: \" , k])\n",
    "        print([\"value: \" , v])\n",
    "        print([\"variable: \" , np.unicode_.find(k,'output')]) \n",
    "        print([\"shape: \" , v.shape])\n",
    "        #print(v) \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/irnn/bptt_gc_%d_eta_%d_batch_%d_run_%s\" %(grad_clip,learning_rate,batch_size, datetime.now().strftime(\"%Y%m%d_%H%M\"))\n",
    "Path(log_dir).mkdir(exist_ok=True, parents=True)\n",
    "filelist = [ f for f in os.listdir(log_dir) if f.endswith(\".local\") ]\n",
    "for f in filelist:\n",
    "    os.remove(os.path.join(log_dir, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2, Train Loss: 2.804, Train Acc: 0.055\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.3125\n"
     ]
    }
   ],
   "source": [
    "# write graph into tensorboard \n",
    "tb_writer = tf.summary.FileWriter(log_dir,graph)\n",
    "# run a training session \n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(init)\n",
    "    for step in range(1,50):#range(1,training_steps+1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        batch_x=batch_x.reshape((batch_size,timesteps,num_input))\n",
    "        # run optimizaer \n",
    "        sess.run(train_op,feed_dict={X:batch_x, Y:batch_y})\n",
    "        loss_train, acc_train= sess.run([loss_op, accuracy],feed_dict={X:batch_x, Y:batch_y})\n",
    "        merged_summary=sess.run(merged_summary_op,feed_dict={X:batch_x, Y:batch_y})\n",
    "        tb_writer.add_summary(merged_summary, global_step=step)\n",
    "        #tb_writer.flush()\n",
    "        # show interim performance \n",
    "        if step % display_step==0 or step==1 : \n",
    "            # get batch loss and accuracy \n",
    "            print('Step: {}, Train Loss: {:.3f}, Train Acc: {:.3f}'.format(\n",
    "            step + 1, loss_train, acc_train))\n",
    "            # write summary \n",
    "            #tb_writer.add_summary(acc,global_step=step)\n",
    "            #tb_writer.flush()\n",
    "            # evaluate performance on test data \n",
    "            test_X=mnist.test.images[:test_len].reshape((-1, timesteps, num_input))\n",
    "            test_Y=mnist.test.labels[:test_len]\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    test_data = mnist.test.images[:test_len].reshape((-1, timesteps, num_input))\n",
    "    test_label = mnist.test.labels[:test_len]\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={X: test_data, Y: test_label}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logs/irnn/irnn/bptt_gc_100_eta_0_run_20190121_1220'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the name of trainable variables in the graph"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(KeRNL)",
   "language": "python",
   "name": "kernl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
