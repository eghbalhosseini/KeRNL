{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import collections\n",
    "import hashlib\n",
    "import numbers\n",
    "\n",
    "from tensorflow.python.eager import context\n",
    "from tensorflow.python.framework import constant_op\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.framework import tensor_util\n",
    "from tensorflow.python.layers import base as base_layer\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import clip_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.ops import partitioned_variables\n",
    "from tensorflow.python.ops import random_ops\n",
    "from tensorflow.python.ops import tensor_array_ops\n",
    "from tensorflow.python.ops import variable_scope as vs\n",
    "from tensorflow.python.ops import variables as tf_variables\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "from tensorflow.python.util import nest\n",
    "from tensorflow.contrib.rnn.python.ops.core_rnn_cell import _Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here  I try to use the native RNN cell type in Tensorflow to implement a spiking network layer. An RNN is defined as any cell that has input, states, and a call function that takes the input and state, and produce next_state, and output;\n",
    "https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/RNNCell\n",
    "\n",
    "in the implementation, we assume spikes from m input cells are arrived, and train both $W_{in}$ and $W_{rec}$\n",
    "state of the network are the three following variables $v_{membrane},\\ t_{reset},\\ g(t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "_INPUT_WEIGHT_NAME = \"W_in\"\n",
    "_RECURRENT_WEIGHT_NAME = \"W_rec\"\n",
    "\n",
    "def _calcualte_crossings(x,threshold):\n",
    "    \"\"\"input :x : a 2D tensor with batch x n \n",
    "    outputs a tensor with the same size as x \n",
    "    and values of 0 or 1 depending on comparison between \n",
    "    x and threshold\"\"\" \n",
    "    @tf.custom_gradient\n",
    "    def crossings(x):\n",
    "        dtype=x.dtype\n",
    "        shape=x.get_shape()\n",
    "        thresholds=tf.constant(threshold,shape=[shape[0].value,shape[1].value],dtype=dtype)\n",
    "        # if it has one row \n",
    "        res=tf.greater_equal(x,thresholds)\n",
    "        def grad(dy):\n",
    "            # calculate 1-|x|\n",
    "            temp=1-tf.abs(x)\n",
    "            dyres=tf.maximum(temp,0.0)\n",
    "            return dyres\n",
    "        return tf.cast(res,dtype=dtype), grad\n",
    "    z=crossings(x)\n",
    "    return z \n",
    "\n",
    "_SNNStateTuple = collections.namedtuple(\"SNNStateTuple\", (\"v_mem\",\"spike\",\"t_reset\", \"S_rec\",\"S_in\"))\n",
    "\n",
    "class SNNStateTuple(_SNNStateTuple):\n",
    "  \"\"\"Tuple used by SNN Cells for `state_variables `, and output state.\n",
    "\n",
    "  Stores five elements: `(v_mem,spike, t_reset, S_rec, S_in)`, in that order. Where `v_mem` is the hidden state\n",
    "  , spike is output, `S_rec` and 'S_in' are spike history, and t_reset refractory history.\n",
    "\n",
    "  Only used when `state_is_tuple=True`.\n",
    "  \"\"\"\n",
    "  __slots__ = ()\n",
    "\n",
    "  @property\n",
    "  def dtype(self):\n",
    "    (v_mem, spike,t_reset, S_rec, S_in ) = self\n",
    "    if v_mem.dtype != spike.dtype:\n",
    "      raise TypeError(\"Inconsistent internal state: %s vs %s\" %\n",
    "                      (str(v_mem.dtype), str(spike.dtype)))\n",
    "    return spike.dtype\n",
    "\n",
    "\n",
    "class SNNCell(tf.contrib.rnn.RNNCell):\n",
    "  \"\"\"Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078).\n",
    "\n",
    "  Args:\n",
    "    num_units: int, The number of units in the GRU cell.\n",
    "    activation: Nonlinearity to use.  Default: `tanh`.\n",
    "    reuse: (optional) Python boolean describing whether to reuse variables\n",
    "     in an existing scope.  If not `True`, and the existing scope already has\n",
    "     the given variables, an error is raised.\n",
    "    kernel_initializer: (optional) The initializer to use for the weight and\n",
    "    projection matrices.\n",
    "    bias_initializer: (optional) The initializer to use for the bias.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               num_units,\n",
    "               tau_m=5.0,\n",
    "               v_theta=1.0,\n",
    "               v_reset=0.0,\n",
    "               tau_s=5.0,\n",
    "               tau_refract=3.0,\n",
    "               activation=None,\n",
    "               reuse=None,\n",
    "               kernel_initializer=None,\n",
    "               bias_initializer=None,\n",
    "               state_is_tuple=False):\n",
    "    super(SNNCell, self).__init__(_reuse=reuse)\n",
    "    self._num_units = num_units\n",
    "    self.tau_m=tau_m\n",
    "    self.v_theta=v_theta\n",
    "    self.v_reset=v_reset\n",
    "    self.tau_s=tau_s\n",
    "    self.tau_refract=tau_refract\n",
    "    self._activation = activation or math_ops.tanh\n",
    "    self._kernel_initializer = kernel_initializer\n",
    "    self._bias_initializer = bias_initializer\n",
    "    self._state_is_tuple= state_is_tuple\n",
    "    self._weight_linear = None\n",
    "    self._calculate_crossing= None\n",
    "\n",
    "  @property\n",
    "  def state_size(self):\n",
    "    return (SNNStateTuple(self._num_units,\n",
    "                          self._num_units,\n",
    "                          self._num_units,\n",
    "                          [self._num_units,self._num_units],\n",
    "                          [self._num_units,self._num_units]) if self._state_is_tuple else self._num_units)\n",
    "                          \n",
    "\n",
    "  @property\n",
    "  def output_size(self):\n",
    "    return self._num_units\n",
    "\n",
    "  def call(self, inputs, state):\n",
    "    \"\"\"Spiking Neuron Cell (SNN).\n",
    "\n",
    "    Args:\n",
    "      inputs: `2-D` tensor with shape `[batch_size x input_size]`.\n",
    "      state: An `SNNStateTuple` of state tensors, shaped as following \n",
    "              `[batch_size x self.state_size]`\n",
    "              `[batch_size x self.state_size]`\n",
    "              `[batch_size x self.state_size]`\n",
    "              `[batch_size x self.state_size x self.state_size]`\n",
    "              `[batch_size x self.state_size x self.state_size]`\n",
    "        `[batch_size x self.state_size]`.\n",
    "\n",
    "    Returns:\n",
    "      A pair containing the new output, and the new state as SNNStateTuple\n",
    "    \"\"\"\n",
    "    if self._state_is_tuple:\n",
    "        v_mem,spike, t_reset, S_rec, S_in =state\n",
    "    else: \n",
    "        logging.warn(\"%s: Please use state tuple \", self)\n",
    "    # initialize crossing function \n",
    "    if self._calculate_crossing is None:\n",
    "        self._calculate_crossing = _calcualte_crossings(v_mem,self.v_theta)\n",
    "    \n",
    "    # get spikes \n",
    "    spike=self._calculate_crossing(v_mem)\n",
    "    v_reseting=tf.scalar_mul(self.v_theta,spike)\n",
    "    v_update=tf.subtract(v_mem,v_reseting)\n",
    "    # get conductance \n",
    "    \n",
    "    # update membrane \n",
    "    \n",
    "    # return variables \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if self._gate_linear is None:\n",
    "      bias_ones = self._bias_initializer\n",
    "      if self._bias_initializer is None:\n",
    "        bias_ones = init_ops.constant_initializer(1.0, dtype=inputs.dtype)\n",
    "      with vs.variable_scope(\"gates\"):  # Reset gate and update gate.\n",
    "        self._gate_linear = _Linear(\n",
    "            [inputs, state],\n",
    "            2 * self._num_units,\n",
    "            True,\n",
    "            bias_initializer=bias_ones,\n",
    "            kernel_initializer=self._kernel_initializer)\n",
    "\n",
    "    value = math_ops.sigmoid(self._gate_linear([inputs, state]))\n",
    "    r, u = array_ops.split(value=value, num_or_size_splits=2, axis=1)\n",
    "\n",
    "    r_state = r * state\n",
    "    if self._candidate_linear is None:\n",
    "      with vs.variable_scope(\"candidate\"):\n",
    "        self._candidate_linear = _Linear(\n",
    "            [inputs, r_state],\n",
    "            self._num_units,\n",
    "            True,\n",
    "            bias_initializer=self._bias_initializer,\n",
    "            kernel_initializer=self._kernel_initializer)\n",
    "    c = self._activation(self._candidate_linear([inputs, r_state]))\n",
    "    new_h = u * state + (1 - u) * c\n",
    "    return new_h, new_h\n",
    "\n",
    "\n",
    "#@tf.custom_gradient   \n",
    "#def calculate_crossing_op(self,x):\n",
    "#    x_norm=tf.divide(tf.subtract(x,tf.constant(self.v_theta,shape=[self.state_size,1])),\n",
    "#                     tf.constant(self.v_theta,shape=[self.state_size,1]))\n",
    "#    temp=tf.greater_equal(x,tf.constant(self.v_theta,shape=[self.state_size,1],dtype=tf.float32))\n",
    "#    def grad(dy):            \n",
    "#        return tf.maximum(tf.constant(0.0,dtype=tf.float32),tf.subtract(tf.constant(1.0,dtype=tf.float32),tf.abs(x_norm)))  \n",
    "#    return temp, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"rnn/transpose_1:0\", shape=(3, 5, 100), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[-0.09491698,  0.04844373,  0.15038799, ...,  0.04932952,\n",
       "         -0.0933247 ,  0.12681684],\n",
       "        [-0.19831467,  0.14181629,  0.2857799 , ...,  0.0807085 ,\n",
       "         -0.16672042,  0.2721757 ],\n",
       "        [-0.30884397,  0.2543453 ,  0.40748757, ...,  0.10056059,\n",
       "         -0.2279777 ,  0.41125804],\n",
       "        [-0.42058885,  0.36732054,  0.516152  , ...,  0.11257821,\n",
       "         -0.28062946,  0.53184116],\n",
       "        [-0.52734804,  0.47143427,  0.6124262 , ...,  0.11921699,\n",
       "         -0.3262758 ,  0.6309334 ]],\n",
       "\n",
       "       [[-0.14542075,  0.06875345,  0.2145503 , ...,  0.03947791,\n",
       "         -0.0980949 ,  0.16520159],\n",
       "        [-0.27697694,  0.18345661,  0.3850823 , ...,  0.0647111 ,\n",
       "         -0.17341049,  0.32615378],\n",
       "        [-0.3999127 ,  0.30627543,  0.519294  , ...,  0.08065322,\n",
       "         -0.23507242,  0.46613288],\n",
       "        [-0.51322204,  0.42067796,  0.62571687, ...,  0.09016851,\n",
       "         -0.28729486,  0.5807098 ],\n",
       "        [-0.6145111 ,  0.52154833,  0.71157813, ...,  0.09531144,\n",
       "         -0.33204275,  0.6717588 ]],\n",
       "\n",
       "       [[-0.19943693,  0.0886716 ,  0.27064466, ...,  0.03134901,\n",
       "         -0.10121056,  0.19434777],\n",
       "        [-0.35598955,  0.21953097,  0.47080386, ...,  0.05149115,\n",
       "         -0.17746848,  0.36518276],\n",
       "        [-0.48652536,  0.34822246,  0.61346674, ...,  0.06422977,\n",
       "         -0.23875532,  0.5042186 ],\n",
       "        [-0.59734404,  0.4621831 ,  0.71552384, ...,  0.071767  ,\n",
       "         -0.29001787,  0.6135941 ],\n",
       "        [-0.6905024 ,  0.5597697 ,  0.7905605 , ...,  0.07580677,\n",
       "         -0.3335915 ,  0.6986257 ]]], dtype=float32)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values231 = np.array([\n",
    "    [[1], [2], [3]],\n",
    "    [[2], [3], [4]]\n",
    "])\n",
    "\n",
    "# Batch size = 3, sequence length = 5, number input = 2, shape=(3, 5, 2)\n",
    "values352 = np.array([\n",
    "    [[1, 4], [2, 5], [3, 6], [4, 7], [5, 8]],\n",
    "    [[2, 5], [3, 6], [4, 7], [5, 8], [6, 9]],\n",
    "    [[3, 6], [4, 7], [5, 8], [6, 9], [7, 10]]\n",
    "])\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "tf_values231 = tf.constant(values352, dtype=tf.float32)\n",
    "lstm_cell = SNNCell(num_units=100,state_is_tuple=False)\n",
    "outputs, state = tf.nn.dynamic_rnn(cell=lstm_cell, dtype=tf.float32, inputs=tf_values231)\n",
    "\n",
    "print(outputs)\n",
    "# tf.Tensor 'rnn_3/transpose:0' shape=(2, 3, 100) dtype=float32\n",
    "#print(state.c)\n",
    "# tf.Tensor 'rnn_3/while/Exit_2:0' shape=(2, 100) dtype=float32\n",
    "#print(state.h)\n",
    "# tf.Tensor 'rnn_3/while/Exit_3:0' shape=(2, 100) dtype=float32\n",
    "cell_outputs=[]\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    output_run, state_run = sess.run([outputs, state])\n",
    "    cell_outputs.append(output_run)\n",
    "    \n",
    "output_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'IdentityN_1:0' shape=(1, 1) dtype=float32>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(KeRNL)",
   "language": "python",
   "name": "kernl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
