{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import collections\n",
    "import hashlib\n",
    "import numbers\n",
    "\n",
    "from tensorflow.python.eager import context\n",
    "from tensorflow.python.framework import constant_op\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.framework import tensor_util\n",
    "from tensorflow.python.layers import base as base_layer\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import clip_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.ops import partitioned_variables\n",
    "from tensorflow.python.ops import random_ops\n",
    "from tensorflow.python.ops import tensor_array_ops\n",
    "from tensorflow.python.ops import variable_scope as vs\n",
    "from tensorflow.python.ops import variables as tf_variables\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "from tensorflow.python.util import nest\n",
    "from tensorflow.contrib.rnn.python.ops.core_rnn_cell import _Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here  I try to use the native RNN cell type in Tensorflow to implement a spiking network layer. An RNN is defined as any cell that has input, states, and a call function that takes the input and state, and produce next_state, and output;\n",
    "https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/RNNCell\n",
    "\n",
    "in the implementation, we assume spikes from m input cells are arrived, and train both $W_{in}$ and $W_{rec}$\n",
    "state of the network are the three following variables $v_{membrane},\\ t_{reset},\\ g(t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "_INPUT_WEIGHT_NAME = \"W_in\"\n",
    "_RECURRENT_WEIGHT_NAME = \"W_rec\"\n",
    "## crossing function\n",
    "def _calcualte_crossings(x,threshold):\n",
    "    \"\"\"input :x : a 2D tensor with batch x n \n",
    "    outputs a tensor with the same size as x \n",
    "    and values of 0 or 1 depending on comparison between \n",
    "    x and threshold\"\"\" \n",
    "    @tf.custom_gradient\n",
    "    def crossings(x):\n",
    "        dtype=x.dtype\n",
    "        shape=x.get_shape()\n",
    "        thresholds=tf.constant(threshold,shape=[shape[0].value,shape[1].value],dtype=dtype)\n",
    "        # if it has one row \n",
    "        res=tf.greater_equal(x,thresholds)\n",
    "        def grad(dy):\n",
    "            # calculate 1-|x|\n",
    "            temp=1-tf.abs(x)\n",
    "            dyres=tf.maximum(temp,0.0)\n",
    "            return dyres\n",
    "        return tf.cast(res,dtype=dtype), grad\n",
    "    z=crossings(x)\n",
    "    return z \n",
    "\n",
    "## conductance update function \n",
    "def _tensor_linear_rec_in(x,y,output_size):\n",
    "    \"\"\"input - x : a 3D tensor with batch x n x n \n",
    "    y is a 3D with size batch x n x m\n",
    "    outputs a tensor  with size batch x output_size,\n",
    "    \"\"\" \n",
    "    shape_x=x.get_shape()\n",
    "    shape_y=y.get_shape()\n",
    "    # \n",
    "    scope=vs.get_variable_scope()\n",
    "    with vs.variable_scope(scope) as outer_scope:\n",
    "        #weight_rec=tf.get_variable(_RECURRENT_WEIGHT_NAME,[shape_x[1],shape_x[2]]) # [n x n]\n",
    "        #weight_in=tf.get_variable(_INPUT_WEIGHT_NAME,[shape_y[1],shape_y[2]]) # [n x m]\n",
    "        w_in_test= lambda:tf.constant(0.0,shape=[shape_y[1],shape_y[2]]) # [n x n]\n",
    "        w_rec_test= lambda:tf.constant(0.0,shape=[shape_x[1],shape_x[2]]) # [n x n]\n",
    "        weight_rec=tf.Variable(initial_value=w_rec_test,dtype=tf.float32) # [n x n]\n",
    "        weight_in=tf.Variable(initial_value=w_in_test,dtype=tf.float32) # [n x n]\n",
    "        # apply_weights \n",
    "        #recurrent\n",
    "        res_rec_aux=tf.multiply(x,weight_rec)\n",
    "        res_rec_final=tf.reduce_sum(res_rec_aux,tf.rank(res_rec_aux)-1)\n",
    "        #input\n",
    "        res_in_aux=tf.multiply(y,weight_in)\n",
    "        res_in_final=tf.reduce_sum(res_in_aux,tf.rank(res_in_aux)-1)\n",
    "        # sum both \n",
    "        res=tf.add(res_in_final,res_rec_final)\n",
    "        return res\n",
    "\n",
    "## expand dimensions for incoming spikes \n",
    "def _tensor_expand_dim(x,y,output_size):\n",
    "    \"\"\"input - x : a 2D tensor with batch x n \n",
    "    y is a 2D with size batch x m\n",
    "    outputs is 3D tensor with size batch x n x n and batch x n x m \n",
    "    \"\"\" \n",
    "    shape_x=x.get_shape()\n",
    "    shape_y=y.get_shape()\n",
    "    #y=tf.cast(y,tf.float32)\n",
    "    # define a matrix for removing the diagonal in recurrent spikes \n",
    "    diag_zero= lambda:tf.subtract(tf.constant(1.0,shape=[shape_x[1],shape_x[1]]),\n",
    "                                                    tf.eye(output_size))\n",
    "    x_diag_fixer = tf.Variable(initial_value=diag_zero, dtype=tf.float32)\n",
    "    # expand x  \n",
    "    x_temp=tf.reshape(tf.tile(x,[1,output_size]),[-1,output_size,shape_x[1]])\n",
    "    # remove diagonal \n",
    "    x_expand=tf.multiply(x_temp,x_diag_fixer)\n",
    "    # expand y  \n",
    "    y_expand=tf.reshape(tf.tile(y,[1,output_size]),[-1,output_size,shape_y[1]])\n",
    "    return x_expand, y_expand\n",
    "\n",
    "\n",
    "\n",
    "## define tuples for the cell \n",
    "_SNNStateTuple = collections.namedtuple(\"SNNStateTuple\", (\"v_mem\",\"spike\",\"t_reset\", \"S_rec\",\"S_in\"))\n",
    "_SNNOutputTuple = collections.namedtuple(\"SNNOutputTuple\", (\"v_mem\",\"spike\",\"g_mem\",\"t_reset\", \"S_rec\",\"S_in\"))\n",
    "\n",
    "class SNNStateTuple(_SNNStateTuple):\n",
    "  \"\"\"Tuple used by SNN Cells for `state_variables `, and output state.\n",
    "  Stores five elements: `(v_mem,spike, t_reset, S_rec, S_in)`, in that order. Where `v_mem` is the hidden state\n",
    "  , spike is output, `S_rec` and 'S_in' are spike history, and t_reset refractory history.\n",
    "  Only used when `state_is_tuple=True`.\n",
    "  \"\"\"\n",
    "  __slots__ = ()\n",
    "\n",
    "  @property\n",
    "  def dtype(self):\n",
    "    (v_mem, spike,t_reset, S_rec, S_in ) = self\n",
    "    if v_mem.dtype != spike.dtype:\n",
    "      raise TypeError(\"Inconsistent internal state: %s vs %s\" %\n",
    "                      (str(v_mem.dtype), str(spike.dtype)))\n",
    "    return spike.dtype\n",
    "\n",
    "\n",
    "class SNNOutputTuple(_SNNOutputTuple):\n",
    "  \"\"\"Tuple used by SNN Cells for output state.\n",
    "  Stores six elements: `(v_mem,spike ,G_mem ,t_reset, S_rec, S_in)`, \n",
    "  Only used when `output_is_tuple=True`.\n",
    "  \"\"\"\n",
    "  __slots__ = ()\n",
    "\n",
    "  @property\n",
    "  def dtype(self):\n",
    "    (v_mem,spike,g_mem ,t_reset, S_rec, S_in) = self\n",
    "    if v_mem.dtype != spike.dtype:\n",
    "      raise TypeError(\"Inconsistent internal state: %s vs %s\" %\n",
    "                      (str(v_mem.dtype), str(spike.dtype)))\n",
    "    return spike.dtype\n",
    "\n",
    "\n",
    "## define SNNcell \n",
    "class SNNCell(tf.contrib.rnn.RNNCell):\n",
    "  \"\"\"Spiking Neural Network Cell\n",
    "  Args:\n",
    "    num_units: int, The number of units in the GRU cell.\n",
    "    activation: Nonlinearity to use.  Default: `tanh`.\n",
    "    reuse: (optional) Python boolean describing whether to reuse variables\n",
    "     in an existing scope.  If not `True`, and the existing scope already has\n",
    "     the given variables, an error is raised.\n",
    "    kernel_initializer: (optional) The initializer to use for the weight and\n",
    "    projection matrices.\n",
    "    bias_initializer: (optional) The initializer to use for the bias.\n",
    "  \"\"\"\n",
    "  def __init__(self,\n",
    "               num_units,\n",
    "               num_inputs,\n",
    "               tau_m=5.0,\n",
    "               v_theta=1.0,\n",
    "               v_reset=0.0,\n",
    "               tau_s=5.0,\n",
    "               tau_refract=3.0,\n",
    "               dt=1.0,\n",
    "               activation=None,\n",
    "               reuse=None,\n",
    "               kernel_initializer=None,\n",
    "               bias_initializer=None,\n",
    "               state_is_tuple=False,\n",
    "               output_is_tuple=False):\n",
    "        \n",
    "    super(SNNCell, self).__init__(_reuse=reuse)\n",
    "    self._num_units = num_units\n",
    "    self._num_inputs = num_inputs\n",
    "    self.tau_m=tau_m\n",
    "    self.v_theta=v_theta\n",
    "    self.v_reset=v_reset\n",
    "    self.tau_s=tau_s\n",
    "    self.tau_refract=tau_refract\n",
    "    self.dt=dt\n",
    "    self._activation = activation or math_ops.tanh\n",
    "    self._kernel_initializer = kernel_initializer\n",
    "    self._bias_initializer = bias_initializer\n",
    "    self._state_is_tuple= state_is_tuple\n",
    "    self._output_is_tuple= output_is_tuple\n",
    "    self._calculate_crossing= _calcualte_crossings\n",
    "    self._tensor_expand_dim = _tensor_expand_dim\n",
    "    self._tensor_linear_rec_in = _tensor_linear_rec_in\n",
    "\n",
    "\n",
    "  @property\n",
    "  def state_size(self):\n",
    "    return (SNNStateTuple(self._num_units,\n",
    "                          self._num_units,\n",
    "                          self._num_units,\n",
    "                          np.array([self._num_units,self._num_units]),\n",
    "                          np.array([self._num_units,self._num_inputs])) if self._state_is_tuple else self._num_units)\n",
    "                          \n",
    "\n",
    "#  @property\n",
    "#  def output_size(self):\n",
    "#    return self._num_units\n",
    "  @property\n",
    "  def output_size(self):\n",
    "    return (SNNOutputTuple(self._num_units,\n",
    "                          self._num_units,\n",
    "                          self._num_units,\n",
    "                          self._num_units,\n",
    "                          np.array([self._num_units,self._num_units]),\n",
    "                          np.array([self._num_units,self._num_inputs])) if self._state_is_tuple else self._num_units)\n",
    "    \n",
    "\n",
    "  def call(self, inputs, state):\n",
    "    \"\"\"Spiking Neuron Cell (SNN).\n",
    "    Args:\n",
    "      inputs: `2-D` tensor with shape `[batch_size x input_size]`.\n",
    "      state: An `SNNStateTuple` of state tensors, shaped as following \n",
    "              `[batch_size x self.state_size]`\n",
    "              `[batch_size x self.state_size]`\n",
    "              `[batch_size x self.state_size]`\n",
    "              `[batch_size x self.state_size x self.state_size]`\n",
    "              `[batch_size x self.state_size x self.state_size]`\n",
    "        `[batch_size x self.state_size]`.\n",
    "    Returns:\n",
    "      A pair containing the new output, and the new state as SNNStateTuple\n",
    "    \"\"\"\n",
    "    if self._state_is_tuple:\n",
    "        v_mem,spike, t_reset, S_rec, S_in = state\n",
    "    else: \n",
    "        logging.warn(\"%s: Please use float \", self)\n",
    "    # initialize crossing function \n",
    "    #if self._calculate_crossing is None:\n",
    "    #    self._calculate_crossing = _calcualte_crossings(v_mem,self.v_theta)\n",
    "        \n",
    "    #if self._tensor_expand_dim is None:\n",
    "    #    self._tensor_expand_dim = _tensor_expand_dim(spike,inputs,self._num_units)\n",
    "    \n",
    "    \n",
    "    # compoutation \n",
    "    # 1-get spikes \n",
    "    spike=self._calculate_crossing(v_mem,self._num_units)\n",
    "    v_reseting=tf.scalar_mul(self.v_theta,spike)\n",
    "    v_update=tf.subtract(v_mem,v_reseting)\n",
    "    ## get conductance \n",
    "        # expand input spikes and recurent spikes \n",
    "    spike_expand,inputs_expand=self._tensor_expand_dim(spike,inputs,self._num_units)\n",
    "    \n",
    "        # decay previous values and threshold them \n",
    "        # recurrent \n",
    "    #if True:\n",
    "    #    logging.warn(\"%s: Please use state tuple \", tf.shape(S_in))  \n",
    "        \n",
    "    S_rec_subtract=S_rec-tf.divide(S_rec,self.tau_s)\n",
    "        # remove values below zero \n",
    "    S_rec_update=tf.clip_by_value(S_rec_subtract,0.0,200)\n",
    "        # input \n",
    "    S_in_subtract=S_in-tf.divide(S_in,self.tau_s)\n",
    "        # remove values below zero \n",
    "    S_in_update=tf.clip_by_value(S_in_subtract,0.0,200)\n",
    "    \n",
    "    # update S_rec and S_in\n",
    "    S_rec_new=tf.add(S_rec_update,spike_expand)\n",
    "    S_in_new=tf.add(S_in_update,inputs_expand)\n",
    "    \n",
    "    # calculate new G = W*S \n",
    "    G_neuron=self._tensor_linear_rec_in(S_rec_new,S_in_new,self._num_units)\n",
    "    \n",
    "    ## compute I_input \n",
    "    I_input=tf.multiply(G_neuron,v_mem)+2\n",
    "    \n",
    "    ## update membrane potential \n",
    "        # update refractory period \n",
    "    t_subtract= tf.subtract(t_reset,1.0)\n",
    "    t_update=tf.clip_by_value(t_subtract,0.0,100)\n",
    "    t_reset_new=tf.add(tf.multiply(spike,self.tau_refract),t_reset)\n",
    "        # get eligible neurons for update \n",
    "    eligilible_update=tf.cast(tf.equal(t_reset_new,0.0),tf.float32)\n",
    "        # update voltage\n",
    "    dV_op=tf.multiply(eligilible_update,tf.divide(tf.subtract(I_input,v_update),self.tau_m))\n",
    "    v_mem_new=tf.add(v_update,tf.multiply(dV_op,self.dt))\n",
    "    \n",
    "    ## return variables \n",
    "    \n",
    "    if self._state_is_tuple:\n",
    "        new_state = SNNStateTuple( v_mem_new, spike,t_reset_new, S_rec_new, S_in_new )\n",
    "    if self._output_is_tuple:\n",
    "        new_output = SNN(v_mem_new, spike,G_neuron ,t_reset_new, S_rec_new, S_in_new )\n",
    "    else:\n",
    "        new_output = spike\n",
    "    return new_output, new_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__new__() takes 6 positional arguments but 7 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-182-5b05773d8c39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtf_input_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_spikes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mSNN_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSNNCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_units\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate_is_tuple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_is_tuple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtau_s\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamic_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSNN_cell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf_input_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mcell_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/KeRNL/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36mdynamic_rnn\u001b[0;34m(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0mswap_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswap_memory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m         dtype=dtype)\n\u001b[0m\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m     \u001b[0;31m# Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/KeRNL/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36m_dynamic_rnn_loop\u001b[0;34m(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)\u001b[0m\n\u001b[1;32m    826\u001b[0m       \u001b[0mparallel_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m       \u001b[0mmaximum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       swap_memory=swap_memory)\n\u001b[0m\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m   \u001b[0;31m# Unpack final output if not using output tuples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/KeRNL/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m   3230\u001b[0m       \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWHILE_CONTEXT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3231\u001b[0m     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants,\n\u001b[0;32m-> 3232\u001b[0;31m                                     return_same_structure)\n\u001b[0m\u001b[1;32m   3233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmaximum_iterations\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3234\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/KeRNL/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mBuildLoop\u001b[0;34m(self, pred, body, loop_vars, shape_invariants, return_same_structure)\u001b[0m\n\u001b[1;32m   2950\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mutation_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2951\u001b[0m         original_body_result, exit_vars = self._BuildLoop(\n\u001b[0;32m-> 2952\u001b[0;31m             pred, body, original_loop_vars, loop_vars, shape_invariants)\n\u001b[0m\u001b[1;32m   2953\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2954\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/KeRNL/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36m_BuildLoop\u001b[0;34m(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\u001b[0m\n\u001b[1;32m   2885\u001b[0m         flat_sequence=vars_for_body_with_tensor_arrays)\n\u001b[1;32m   2886\u001b[0m     \u001b[0mpre_summaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2887\u001b[0;31m     \u001b[0mbody_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpacked_vars_for_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2888\u001b[0m     \u001b[0mpost_summaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2889\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/KeRNL/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(i, lv)\u001b[0m\n\u001b[1;32m   3199\u001b[0m         cond = lambda i, lv: (  # pylint: disable=g-long-lambda\n\u001b[1;32m   3200\u001b[0m             math_ops.logical_and(i < maximum_iterations, orig_cond(*lv)))\n\u001b[0;32m-> 3201\u001b[0;31m         \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3203\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/KeRNL/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36m_time_step\u001b[0;34m(time, output_ta_t, state)\u001b[0m\n\u001b[1;32m    797\u001b[0m           skip_conditionals=True)\n\u001b[1;32m    798\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m       \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    800\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m     \u001b[0;31m# Pack state if using state tuples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/KeRNL/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m     \u001b[0minput_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_sequence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 785\u001b[0;31m     \u001b[0mcall_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msequence_length\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/KeRNL/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, state, scope)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope_attrname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNNCell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_rnn_get_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/KeRNL/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m       \u001b[0;31m# Actually call layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/KeRNL/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0min_deferred_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m           raise ValueError('A layer\\'s `call` method should return a Tensor '\n",
      "\u001b[0;32m<ipython-input-181-3ed4ee08b1cd>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, state)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSNNStateTuple\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mv_mem_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspike\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt_reset_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS_rec_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS_in_new\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_is_tuple\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0mnew_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSNNStateTuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_mem_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspike\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mG_neuron\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mt_reset_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS_rec_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS_in_new\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mnew_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspike\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __new__() takes 6 positional arguments but 7 were given"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "sequence_length = 20\n",
    "num_inputs = 1\n",
    "num_units=1\n",
    "#shape=(2, 5, 2)\n",
    "#input_spikes=np.random.randint(2,size=[2,10,1])\n",
    "input_spikes=np.ones([batch_size,sequence_length,num_inputs])\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "tf_input_values = tf.constant(input_spikes, dtype=tf.float32)\n",
    "SNN_cell = SNNCell(num_units=num_units,state_is_tuple=True,output_is_tuple=True,num_inputs=num_inputs,tau_s=100.0)\n",
    "outputs, state = tf.nn.dynamic_rnn(cell=SNN_cell, dtype=tf.float32, inputs=tf_input_values)\n",
    "\n",
    "cell_outputs=[]\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    output_run , state_run = sess.run([outputs, state])\n",
    "\n",
    "    variables_names =[v.name for v in tf.global_variables()]\n",
    "    values = sess.run(variables_names)\n",
    "    for k,v in zip(variables_names, values):\n",
    "        print(k, v) \n",
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(output_run.t_reset.flatten())\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(output_run.spike.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values231 = np.array([\n",
    "    [[1], [2], [3]],\n",
    "    [[2], [3], [4]]\n",
    "])\n",
    "\n",
    "# Batch size = 3, sequence length = 5, number input = 2, shape=(3, 5, 2)\n",
    "values352 = np.array([\n",
    "    [[1, 4], [2, 5], [3, 6], [4, 7], [5, 8]],\n",
    "    [[2, 5], [3, 6], [4, 7], [5, 8], [6, 9]],\n",
    "    [[3, 6], [4, 7], [5, 8], [6, 9], [7, 10]]\n",
    "])\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "tf_values231 = tf.constant(values352, dtype=tf.float32)\n",
    "lstm_cell = SNNCell(num_units=100,state_is_tuple=True,num_inputs=2)\n",
    "outputs, state = tf.nn.dynamic_rnn(cell=lstm_cell, dtype=tf.float32, inputs=tf_values231)\n",
    "\n",
    "print(outputs)\n",
    "# tf.Tensor 'rnn_3/transpose:0' shape=(2, 3, 100) dtype=float32\n",
    "#print(state.c)\n",
    "# tf.Tensor 'rnn_3/while/Exit_2:0' shape=(2, 100) dtype=float32\n",
    "#print(state.h)\n",
    "# tf.Tensor 'rnn_3/while/Exit_3:0' shape=(2, 100) dtype=float32\n",
    "cell_outputs=[]\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    output_run , state_run = sess.run([outputs, state])\n",
    "    cell_outputs.append(output_run)\n",
    "    \n",
    "output_run"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(KeRNL)",
   "language": "python",
   "name": "kernl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
