{
 "cells": [
  {
   "cell_type": "code",

   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import collections\n",
    "import hashlib\n",
    "import numbers\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "from tensorflow.python.eager import context\n",
    "from tensorflow.python.framework import constant_op\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.framework import tensor_util\n",
    "from tensorflow.python.layers import base as base_layer\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import clip_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.ops import partitioned_variables\n",
    "from tensorflow.python.ops import random_ops\n",
    "from tensorflow.python.ops import tensor_array_ops\n",
    "from tensorflow.python.ops import variable_scope as vs\n",
    "from tensorflow.python.ops import variables as tf_variables\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "from tensorflow.python.util import nest\n",
    "from tensorflow.contrib.rnn.python.ops.core_rnn_cell import _Linear\n",
    "from tensorflow.contrib import slim\n",
    "import spiking_cell_bare\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Total number of batches: 250\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 8c348af6d6d6ed103e42439ad0a8bbd90ee1815f
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> a271c87361162ca84407c8cfb6cf1a96da57da4b
   "source": [
    "# uplading mnist data \n",
    "\n",
    "old_v = tf.logging.get_verbosity()\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "train_data = mnist.train.images  # Returns np.array\n",
    "train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "eval_data = mnist.test.images  # Returns np.array\n",
    "eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)\n",
    "\n",
    "tf.logging.set_verbosity(old_v)\n",
    "\n",
    "# Training Parameters\n",
    "weight_learning_rate = 1e-3\n",
<<<<<<< HEAD
<<<<<<< HEAD
    "training_steps = 1000\n",
=======
    "training_steps = 5000\n",
>>>>>>> 8c348af6d6d6ed103e42439ad0a8bbd90ee1815f
=======
    "training_steps = 5000\n",
>>>>>>> a271c87361162ca84407c8cfb6cf1a96da57da4b
    "batch_size = 220\n",
    "display_step = 25\n",
    "test_len=128\n",
    "grad_clip=200\n",
    "# Network Parameters\n",
    "num_input = 1 # MNIST data input (img shape: 28*28)\n",
    "num_context_input=1\n",
    "MNIST_timesteps = 28*28 # timesteps\n",
    "context_timesteps=54\n",
    "timesteps=MNIST_timesteps+context_timesteps\n",
    "# \n",
    "num_unit_input_layer=80 # input layer neurons\n",
    "num_context_unit=1\n",
    "\n",
    "num_hidden = 200 # hidden layer num of features\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "total_batch = int(mnist.train.num_examples / batch_size)\n",
    "print(\"Total number of batches:\", total_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## spiking cell "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": null,
>>>>>>> 8c348af6d6d6ed103e42439ad0a8bbd90ee1815f
=======
   "execution_count": null,
>>>>>>> a271c87361162ca84407c8cfb6cf1a96da57da4b
   "metadata": {},
   "outputs": [],
   "source": [
    "def SNN_all_states(x,context):\n",
    "    \n",
    "    with tf.variable_scope('input_context') as scope:\n",
    "        context_input_layer_cell=spiking_cell_bare.context_input_spike_cell(num_units=1,context_switch=MNIST_timesteps)\n",
    "        context_initial_state = context_input_layer_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "        output_context, states_context = tf.nn.dynamic_rnn(context_input_layer_cell, dtype=tf.float32, inputs=context,initial_state=context_initial_state)\n",
    "    with tf.variable_scope('input_layer') as scope: \n",
    "        input_layer_cell=spiking_cell_bare.input_spike_cell(num_units=num_unit_input_layer)\n",
    "        input_initial_state = input_layer_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "        output_l1, states_l1 = tf.nn.dynamic_rnn(input_layer_cell, dtype=tf.float32, inputs=x,initial_state=input_initial_state)\n",
    "    with tf.variable_scope('hidden_layer') as scope: \n",
    "        hidden_layer_cell=spiking_cell_bare.conductance_spike_Cell(num_units=num_hidden,output_is_tuple=False,tau_refract=5.0)\n",
    "        hidden_initial_state = hidden_layer_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "        output_hidden, states_hidden = tf.nn.dynamic_rnn(hidden_layer_cell, dtype=tf.float32, inputs=tf.concat([output_l1,output_context],-1),initial_state=hidden_initial_state)\n",
    "    with tf.variable_scope('output_layer') as scope : \n",
    "        output_layer_cell=spiking_cell_bare.output_spike_cell(num_units=num_classes)\n",
    "        output_voltage, voltage_states=tf.nn.dynamic_rnn(output_layer_cell,dtype=tf.float32,inputs=output_hidden)\n",
    "\n",
    "    return output_voltage,output_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## computation graph "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:(220, 80): Please use float \n",
      "WARNING:tensorflow:(220, 80): Please use float \n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 8c348af6d6d6ed103e42439ad0a8bbd90ee1815f
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> a271c87361162ca84407c8cfb6cf1a96da57da4b
   "source": [
    "tf.reset_default_graph()\n",
    "graph=tf.Graph()\n",
    "with graph.as_default():\n",
    "    # define inputs and context to the network\n",
    "    X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "    Context=tf.placeholder('float',shape=[batch_size,timesteps,num_context_input])\n",
    "    Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "    \n",
    "    # define a function for extraction of variable names\n",
    "    SNN_output,SNN_hidden_states=SNN_all_states(X,Context)\n",
    "    SNN_logits=tf.reduce_mean(SNN_output[:,-1-context_timesteps:,:],axis=1)\n",
    "    trainables=tf.trainable_variables()\n",
    "    variable_names=[v.name for v in tf.trainable_variables()]\n",
    "    # \n",
    "    find_joing_index = lambda x, name_1,name_2 : [a and b for a,b in zip([np.unicode_.find(k.name, name_1)>-1 for k in x] ,[np.unicode_.find(k.name, name_2)>-1 for k in x])].index(True)\n",
    "    # find trainable parameters for keRNL \n",
    "    with tf.name_scope('SNN_Trainables') as scope:\n",
    "        SNN_hidden_kernel_index= find_joing_index(trainables,'hidden_layer','kernel')\n",
<<<<<<< HEAD
<<<<<<< HEAD
    "        SNN_output_kernel_index= find_joing_index(trainables,'output_layer','kernel')\n",
    "    # \n",
    "        SSN_weight_training_indices=np.asarray([SNN_hidden_kernel_index,SNN_output_kernel_index],dtype=np.int)\n",
=======
=======
>>>>>>> a271c87361162ca84407c8cfb6cf1a96da57da4b
    "        SNN_hidden_bias_index= find_joing_index(trainables,'hidden_layer','bias')\n",
    "        SNN_output_kernel_index= find_joing_index(trainables,'output_layer','kernel')\n",
    "        SNN_output_bias_index= find_joing_index(trainables,'output_layer','bias')\n",
    "    # \n",
    "        SSN_weight_training_indices=np.asarray([SNN_hidden_kernel_index,SNN_hidden_bias_index,SNN_output_kernel_index,SNN_output_bias_index],dtype=np.int)\n",
<<<<<<< HEAD
>>>>>>> 8c348af6d6d6ed103e42439ad0a8bbd90ee1815f
=======
>>>>>>> a271c87361162ca84407c8cfb6cf1a96da57da4b
    "        SNN_weight_trainables= [trainables[k] for k in SSN_weight_training_indices]\n",
    "    #\n",
    "    #define loss functions  \n",
    "    ##################\n",
    "    # SNN train ####\n",
    "    ##################\n",
    "    with tf.name_scope(\"SNN_train\") as scope:\n",
    "        # outputs \n",
    "        SNN_loss_output_prediction=tf.losses.softmax_cross_entropy(onehot_labels=Y,logits=SNN_logits)\n",
    "        SNN_prediction = tf.nn.softmax(SNN_logits)\n",
    "        SNN_correct_pred = tf.equal(tf.argmax(SNN_prediction, 1), tf.argmax(Y, 1))\n",
    "        SNN_accuracy = tf.reduce_mean(tf.cast(SNN_correct_pred, tf.float32))\n",
    "       # define optimizer \n",
    "        SNN_weight_optimizer = tf.train.AdamOptimizer(learning_rate=weight_learning_rate)\n",
    "        \n",
    "       \n",
    "        with tf.name_scope('SNN_train_weights') as scope: \n",
    "            SNN_grad_cost_trainables=tf.gradients(SNN_loss_output_prediction,SNN_weight_trainables)\n",
    "            \n",
    "            # crop the gradients  \n",
    "            SNN_weight_grads_and_vars=list(zip(SNN_grad_cost_trainables,SNN_weight_trainables))\n",
    "            SNN_cropped_weight_grads_and_vars=[(tf.clip_by_norm(grad, grad_clip),var) if  np.unicode_.find(var.name,'output')==-1 else (grad,var) for grad,var in SNN_weight_grads_and_vars]\n",
    "            # apply gradients \n",
    "            SNN_weight_train_op = SNN_weight_optimizer.apply_gradients(SNN_cropped_weight_grads_and_vars)\n",
    "    \n",
    "    ##################\n",
    "    # SUMMARIES ######\n",
    "    ##################\n",
    "    with tf.name_scope(\"SNN_summaries\") as scope:\n",
    "        # SNN logits\n",
    "        tf.summary.histogram('SNN_logits',SNN_logits)\n",
    "        # SNN kernel\n",
    "        tf.summary.histogram('SNN_hidden_kernel_grad',SNN_grad_cost_trainables[0]+1e-10)\n",
    "        tf.summary.histogram('SNN_hidden_kernel', SNN_weight_trainables[0]+1e-10)\n",
    "        # SNN output weight\n",
<<<<<<< HEAD
<<<<<<< HEAD
    "        tf.summary.histogram('SNN_output_kernel_grad',SNN_grad_cost_trainables[1]+1e-10)\n",
    "        tf.summary.histogram('SNN_output_kernel', SNN_weight_trainables[1]+1e-10)\n",
=======
=======
>>>>>>> a271c87361162ca84407c8cfb6cf1a96da57da4b
    "        tf.summary.histogram('SNN_output_kernel_grad',SNN_grad_cost_trainables[2]+1e-10)\n",
    "        tf.summary.histogram('SNN_output_kernel', SNN_weight_trainables[2]+1e-10)\n",
    "        # SNN output bias\n",
    "        tf.summary.histogram('SNN_hidden_bias_grad',SNN_grad_cost_trainables[1])\n",
    "        tf.summary.histogram('SNN_hidden_bias', SNN_weight_trainables[1]+1e-10)\n",
    "        \n",
    "        tf.summary.histogram('SNN_output_bias_grad',SNN_grad_cost_trainables[3])\n",
    "        tf.summary.histogram('SNN_output_bias', SNN_weight_trainables[3]+1e-10)\n",
<<<<<<< HEAD
>>>>>>> 8c348af6d6d6ed103e42439ad0a8bbd90ee1815f
=======
>>>>>>> a271c87361162ca84407c8cfb6cf1a96da57da4b
    "        # SNN loss and accuracy \n",
    "        tf.summary.scalar('SNN_loss_output_prediction',SNN_loss_output_prediction)\n",
    "        tf.summary.scalar('SNN_accuracy',SNN_accuracy)\n",
    "        \n",
    "        # SNN kernel and ouput matrix \n",
    "        tf.summary.image('SNN_hidden_kernel_matrix',tf.expand_dims(tf.expand_dims(SNN_weight_trainables[0],axis=0),axis=-1))\n",
<<<<<<< HEAD
<<<<<<< HEAD
    "        tf.summary.image('SNN_output_kernel_matrix',tf.expand_dims(tf.expand_dims(SNN_weight_trainables[1],axis=0),axis=-1))\n",
    "        # SNN gradients \n",
    "        tf.summary.image('SNN_hidden_kernel_grad',tf.expand_dims(tf.expand_dims(SNN_grad_cost_trainables[0],axis=0),axis=-1))\n",
    "        tf.summary.image('SNN_output_kernel_grad',tf.expand_dims(tf.expand_dims(SNN_grad_cost_trainables[1],axis=0),axis=-1))\n",
=======
=======
>>>>>>> a271c87361162ca84407c8cfb6cf1a96da57da4b
    "        tf.summary.image('SNN_output_kernel_matrix',tf.expand_dims(tf.expand_dims(SNN_weight_trainables[2],axis=0),axis=-1))\n",
    "        # SNN gradients \n",
    "        tf.summary.image('SNN_hidden_kernel_grad',tf.expand_dims(tf.expand_dims(SNN_grad_cost_trainables[0],axis=0),axis=-1))\n",
    "        tf.summary.image('SNN_output_kernel_grad',tf.expand_dims(tf.expand_dims(SNN_grad_cost_trainables[2],axis=0),axis=-1))\n",
<<<<<<< HEAD
>>>>>>> 8c348af6d6d6ed103e42439ad0a8bbd90ee1815f
=======
>>>>>>> a271c87361162ca84407c8cfb6cf1a96da57da4b
    "        SNN_merged_summary_op=tf.summary.merge_all(scope=\"SNN_summaries\")    \n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['variable: ', 'hidden_layer/rnn/conductance_spike__cell/kernel:0']\n",
      "['variable: ', -1]\n",
      "['shape: ', (281, 200)]\n",
      "['variable: ', 'output_layer/rnn/output_spike_cell/kernel:0']\n",
      "['variable: ', 0]\n",
      "['shape: ', (200, 10)]\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 8c348af6d6d6ed103e42439ad0a8bbd90ee1815f
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> a271c87361162ca84407c8cfb6cf1a96da57da4b
   "source": [
    "with tf.Session(graph=graph,) as sess : \n",
    "    sess.run(init)\n",
    "    values,trainable_vars = sess.run([variable_names,trainables])\n",
    "    for k, v in zip(variable_names,values):\n",
    "        print([\"variable: \" , k])\n",
    "        #print([\"value: \" , v])\n",
    "        print([\"variable: \" , np.unicode_.find(k,'output')]) \n",
    "        print([\"shape: \" , v.shape])\n",
    "        #print(v) \n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": null,
>>>>>>> 8c348af6d6d6ed103e42439ad0a8bbd90ee1815f
=======
   "execution_count": null,
>>>>>>> a271c87361162ca84407c8cfb6cf1a96da57da4b
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/SNN_MNIST_BPTT/weight_learn_%.1e_tr_step_%.1e_batch_%.1e_hidd_%.1e_gc_%.1e_run_%s\" %(weight_learning_rate,training_steps,batch_size,num_hidden,grad_clip, datetime.now().strftime(\"%Y%m%d_%H%M\"))\n",
    "Path(log_dir).mkdir(exist_ok=True, parents=True)\n",
    "filelist = [ f for f in os.listdir(log_dir) if f.endswith(\".local\") ]\n",
    "for f in filelist:\n",
    "    os.remove(os.path.join(log_dir, f))\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2,SNN train Loss: 3.438,SNN accu: 0.073\n",
      "Step: 26,SNN train Loss: 2.495,SNN accu: 0.095\n",
      "Step: 51,SNN train Loss: 2.470,SNN accu: 0.095\n",
      "Step: 76,SNN train Loss: 2.406,SNN accu: 0.118\n",
      "Step: 101,SNN train Loss: 2.392,SNN accu: 0.118\n",
      "Step: 126,SNN train Loss: 2.387,SNN accu: 0.091\n",
      "Step: 151,SNN train Loss: 2.389,SNN accu: 0.123\n",
      "Step: 176,SNN train Loss: 2.362,SNN accu: 0.136\n",
      "Step: 201,SNN train Loss: 2.404,SNN accu: 0.100\n",
      "Step: 226,SNN train Loss: 2.355,SNN accu: 0.095\n",
      "Step: 251,SNN train Loss: 2.307,SNN accu: 0.127\n",
      "Step: 276,SNN train Loss: 2.377,SNN accu: 0.118\n",
      "Step: 301,SNN train Loss: 2.309,SNN accu: 0.155\n",
      "Step: 326,SNN train Loss: 2.376,SNN accu: 0.123\n",
      "Step: 351,SNN train Loss: 2.352,SNN accu: 0.109\n",
      "Step: 376,SNN train Loss: 2.335,SNN accu: 0.150\n",
      "Step: 401,SNN train Loss: 2.343,SNN accu: 0.100\n",
      "Step: 426,SNN train Loss: 2.297,SNN accu: 0.150\n",
      "Step: 451,SNN train Loss: 2.344,SNN accu: 0.145\n",
      "Step: 476,SNN train Loss: 2.287,SNN accu: 0.155\n",
      "Step: 501,SNN train Loss: 2.316,SNN accu: 0.136\n",
      "Step: 526,SNN train Loss: 2.353,SNN accu: 0.123\n",
      "Step: 551,SNN train Loss: 2.329,SNN accu: 0.114\n",
      "Step: 576,SNN train Loss: 2.300,SNN accu: 0.136\n",
      "Step: 601,SNN train Loss: 2.347,SNN accu: 0.145\n",
      "Step: 626,SNN train Loss: 2.331,SNN accu: 0.095\n",
      "Step: 651,SNN train Loss: 2.288,SNN accu: 0.145\n",
      "Step: 676,SNN train Loss: 2.293,SNN accu: 0.132\n",
      "Step: 701,SNN train Loss: 2.322,SNN accu: 0.114\n",
      "Step: 726,SNN train Loss: 2.349,SNN accu: 0.109\n",
      "Step: 751,SNN train Loss: 2.269,SNN accu: 0.123\n",
      "Step: 776,SNN train Loss: 2.272,SNN accu: 0.123\n",
      "Step: 801,SNN train Loss: 2.310,SNN accu: 0.114\n",
      "Step: 826,SNN train Loss: 2.319,SNN accu: 0.168\n",
      "Step: 851,SNN train Loss: 2.345,SNN accu: 0.127\n",
      "Step: 876,SNN train Loss: 2.260,SNN accu: 0.118\n",
      "Step: 901,SNN train Loss: 2.329,SNN accu: 0.145\n",
      "Step: 926,SNN train Loss: 2.311,SNN accu: 0.091\n",
      "Step: 951,SNN train Loss: 2.298,SNN accu: 0.173\n",
      "Step: 976,SNN train Loss: 2.278,SNN accu: 0.132\n",
      "Optimization Finished!\n",
      "Model saved in path: logs/SNN_MNIST_BPTT/weight_learn_1.0e-03_tr_step_1.0e+03_batch_2.2e+02_hidd_2.0e+02_gc_2.0e+02_run_20190213_1106/model.ckpt-999\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 8c348af6d6d6ed103e42439ad0a8bbd90ee1815f
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> a271c87361162ca84407c8cfb6cf1a96da57da4b
   "source": [
    "# write graph into tensorboard \n",
    "tb_writer = tf.summary.FileWriter(log_dir,graph)\n",
    "# run a training session \n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(init)\n",
    "    for step in range(1,1000):#range(1,training_steps+1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        batch_x=batch_x.reshape((batch_size,MNIST_timesteps,num_input))\n",
    "        batch_x_full=np.concatenate([batch_x,np.zeros((batch_size,timesteps-MNIST_timesteps,num_input))],axis=1)\n",
    "        context_input=np.ones((batch_size,timesteps,num_context_input))\n",
    "        # IRNN train \n",
    "        SNN_train, SNN_loss,SNN_accu=sess.run([SNN_weight_train_op,SNN_loss_output_prediction,SNN_accuracy],feed_dict={X:batch_x_full,Context:context_input, Y:batch_y})\n",
    "        \n",
    "        # run summaries \n",
    "        SNN_merged_summary=sess.run(SNN_merged_summary_op,feed_dict={X:batch_x_full,Context:context_input, Y:batch_y})\n",
    "        \n",
    "        tb_writer.add_summary(SNN_merged_summary, global_step=step)\n",
    "        # \n",
    "        if step % display_step==0 or step==1 : \n",
    "            # get batch loss and accuracy \n",
    "            print('Step: {},SNN train Loss: {:.3f},SNN accu: {:.3f}'.format(step + 1, SNN_loss,SNN_accu))\n",
    "\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    #test_data = mnist.test.images[:test_len].reshape((-1, timesteps, num_input))\n",
    "    #test_label = mnist.test.labels[:test_len]\n",
    "    #print(\"Testing Accuracy:\", \n",
    "    #    sess.run(loss_output_prediction, feed_dict={X: test_data, Y: test_label}))\n",
    "    save_path = saver.save(sess, log_dir+\"/model.ckpt\", global_step=step,write_meta_graph=True)\n",
    "    print(\"Model saved in path: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
