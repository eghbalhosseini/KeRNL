{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code for creating a Kernel based relu-RNN learning for sequential MNIST\n",
    "adapted from : Roth, Christopher, Ingmar Kanitscheider, and Ila Fiete. 2018. “Kernel RNN Learning (KeRNL),” September. https://openreview.net/forum?id=ryGfnoC5KQ. version 2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python libraries\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import collections\n",
    "import hashlib\n",
    "import numbers\n",
    "import matplotlib.cm as cm\n",
    "from sys import getsizeof\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import os\n",
    "from pandas import DataFrame\n",
    "from IPython.display import HTML\n",
    "\n",
    "# tensorflow and its dependencies \n",
    "import tensorflow as tf\n",
    "from tensorflow.python.eager import context\n",
    "from tensorflow.python.framework import constant_op\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.framework import tensor_util\n",
    "from tensorflow.python.layers import base as base_layer\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import clip_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.ops import partitioned_variables\n",
    "from tensorflow.python.ops import random_ops\n",
    "from tensorflow.python.ops import tensor_array_ops\n",
    "from tensorflow.python.ops import variable_scope as vs\n",
    "from tensorflow.python.ops import variables as tf_variables\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "from tensorflow.python.util import nest\n",
    "from tensorflow.contrib.rnn.python.ops.core_rnn_cell import _Linear\n",
    "from tensorflow.contrib import slim\n",
    "\n",
    "## user defined modules \n",
    "# kernel rnn cell \n",
    "import keRNL_cell "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first we download mnist data for training and testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uplading mnist data \n",
    "old_v = tf.logging.get_verbosity()\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "train_data = mnist.train.images  # Returns np.array\n",
    "train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "eval_data = mnist.test.images  # Returns np.array\n",
    "eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)\n",
    "tf.logging.set_verbosity(old_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we define training parameters and network parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9c3f170f1eb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# uplading mnist data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mold_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_verbosity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_verbosity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtutorials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmnist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_data_sets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MNIST_data/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# uplading mnist data \n",
    "old_v = tf.logging.get_verbosity()\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "train_data = mnist.train.images  # Returns np.array\n",
    "train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "eval_data = mnist.test.images  # Returns np.array\n",
    "eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)\n",
    "tf.logging.set_verbosity(old_v)\n",
    "\n",
    "# Training Parameters\n",
    "weight_learning_rate = 1e-6 # learning rate for weights in the network \n",
    "tensor_learning_rate = 1e-5 # learning rate for sensitivity tensor and temporal filter tensor \n",
    "training_steps = 5000\n",
    "batch_size = 50\n",
    "display_step = 10\n",
    "test_len=128\n",
    "grad_clip=100\n",
    "# Network Parameters\n",
    "num_input = 1 # MNIST data input (img shape: 28*28)\n",
    "timesteps = 28*28 # timesteps\n",
    "num_hidden = 100 # hidden layer num of features\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "perturbation_std=1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next we define a KeRNL unit and an output layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_RNN(x, weights, biases):\n",
    "    with tf.variable_scope('recurrent',initializer=tf.initializers.identity()) as scope: \n",
    "        # Define a lstm cell with tensorflow\n",
    "        keRNL = keRNL_cell.KeRNLCell(num_units=num_hidden,\n",
    "                                     num_inputs=num_input,\n",
    "                                     time_steps=timesteps,\n",
    "                                     noise_std=perturbation_std,\n",
    "                                     sensitivity_initializer=tf.initializers.identity)\n",
    "        # Get lstm cell output\n",
    "        kernel_outputs, kernel_states = tf.nn.dynamic_rnn(keRNL , x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(kernel_outputs[:,-1,:], weights['out']) + biases['out'], kernel_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next step is defining a graph for training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "graph=tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # define weights and inputs to the network\n",
    "    weights = {'out': tf.Variable(tf.random_normal([num_hidden, num_classes]),name='output_weight')}\n",
    "    biases = {'out': tf.Variable(tf.random_normal([num_classes]),name='output_addition')}\n",
    "    X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "    Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "    \n",
    "    # define network output and trainiables \n",
    "    logits,states = kernel_RNN(X, weights, biases)\n",
    "    variable_names=[v.name for v in tf.trainable_variables()]\n",
    "    trainables=tf.trainable_variables()\n",
    "    \n",
    "    # get the index of trainable variables \n",
    "    temporal_filter_index=[np.unicode_.find(k.name,'temporal_filter_coeff')>-1 for k in trainables].index(True)\n",
    "    sensitivity_tensor_index=[np.unicode_.find(k.name,'sensitivity_tensor')>-1 for k in trainables].index(True)    \n",
    "    kernel_index=[np.unicode_.find(k.name,'kernel')>-1 for k in trainables].index(True)\n",
    "    bias_index=[np.unicode_.find(k.name,'bias')>-1 for k in trainables].index(True)\n",
    "    output_weight_index=[np.unicode_.find(k.name,'output_weight')>-1 for k in trainables].index(True)\n",
    "    output_addition_index=[np.unicode_.find(k.name,'output_addition')>-1 for k in trainables].index(True)\n",
    "    \n",
    "    # trainables for tensors \n",
    "    tensor_training_indices=np.asarray([sensitivity_tensor_index,\n",
    "                                        temporal_filter_index],dtype=np.int)\n",
    "    tensor_trainables= [trainables[k] for k in tensor_training_indices]\n",
    "    \n",
    "    # trainables for weights \n",
    "    weight_training_indices=np.asarray([kernel_index,\n",
    "                                        output_weight_index,\n",
    "                                        output_addition_index],dtype=np.int)\n",
    "    weight_trainables= [trainables[k] for k in weight_training_indices]\n",
    "\n",
    "    ## compute lossses \n",
    "    # compute loss for predictions. \n",
    "    loss_output_prediction = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "    logits=logits, labels=Y))\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "    # compute loss for estimating sensitivity tensor and temporal_filter_coeff, \n",
    "    loss_state_prediction=tf.losses.mean_squared_error(tf.subtract(states.h_hat, states.h),\n",
    "                                                       tf.matmul(states.Gamma,trainables[sensitivity_tensor_index]))\n",
    "\n",
    "    ## define optimizers \n",
    "    # define optimizers learning the weights\n",
    "    weight_optimizer = tf.train.RMSPropOptimizer(learning_rate=weight_learning_rate)\n",
    "    \n",
    "    # define optimizer for learning the sensitivity tensor and temporal filter \n",
    "    tensor_optimizer = tf.train.AdamOptimizer(learning_rate=tensor_learning_rate)\n",
    "    \n",
    "    ## get gradients and apply them \n",
    "    ## optimize for temporal_filter and sensitivity_tensor\n",
    "    # calculate gradients\n",
    "    sensitivity_tensor_update=tf.gradients(xs=trainables[sensitivity_tensor_index],ys=loss_state_prediction)\n",
    "    temporal_filter_update=tf.gradients(xs=trainables[temporal_filter_index],ys=loss_state_prediction)\n",
    "    tensor_grads_and_vars=list(zip([sensitivity_tensor_update[-1],temporal_filter_update[-1]],tensor_trainables))\n",
    "    # apply gradients \n",
    "    tensor_train_op = tensor_optimizer.apply_gradients(tensor_grads_and_vars)\n",
    "\n",
    "    ## optimize for recurrent weights and output weights \n",
    "    # 1- gradient for the recurrent weights \n",
    "    grad_cost_to_output=tf.gradients(loss_output_prediction,logits, name= 'grad_cost_to_y')\n",
    "    error_in_hidden_state=tf.expand_dims(tf.reduce_mean(tf.matmul(grad_cost_to_output[-1],\n",
    "                                                                  tf.transpose(trainables[output_weight_index])),axis=0),axis=0)\n",
    "    weight_update_aux=tf.matmul(trainables[kernel_index],tf.transpose(error_in_hidden_state))\n",
    "    total_trace=tf.concat([states.input_trace,states.recurrent_trace],axis=2)\n",
    "    weight_update=tf.transpose(tf.reduce_mean(tf.multiply(total_trace,tf.transpose(tf.tile(weight_update_aux,[1,num_hidden]))),axis=0))\n",
    "    \n",
    "    #2- gradient for output weight\n",
    "    grad_cost_to_output_weight=tf.gradients(loss_output_prediction,trainables[output_weight_index], name= 'grad_cost_to_output_weight')\n",
    "    grad_cost_to_output_bias=tf.gradients(loss_output_prediction,trainables[output_addition_index], name= 'grad_cost_to_output_bias')\n",
    "    # zip gradients and vars \n",
    "    weight_grads_and_vars=list(zip([weight_update,grad_cost_to_output_weight[-1],grad_cost_to_output_bias[-1]],weight_trainables))\n",
    "    # Apply gradient Clipping to recurrent weights \n",
    "    cropped_weight_grads_and_vars=[(tf.clip_by_norm(grad, grad_clip),var) if  np.unicode_.find(var.name,'output')==-1 else \n",
    "                            (grad,var) for grad,var in weight_grads_and_vars]\n",
    "    # apply gradients \n",
    "    weight_train_op = weight_optimizer.apply_gradients(cropped_weight_grads_and_vars)\n",
    "    \n",
    "    ## Evaluate model (with test logits, for dropout to be disabled)\n",
    "    correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "    # Initialize the variables (i.e. assign their default value)\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    ## get variables to save to tensorboard \n",
    "    # network output \n",
    "    tf.summary.histogram('prediction',prediction+1e-8)\n",
    "    tf.summary.histogram('logits',logits+1e-8)\n",
    "    \n",
    "    # tensor training parameters \n",
    "    tf.summary.histogram('sensitivity_updates',sensitivity_tensor_update[-1]+1e-10)\n",
    "    tf.summary.histogram('temporal_filter_updates',temporal_filter_update[-1]+1e-10)\n",
    "    tf.summary.histogram('sensitivity_tensor',trainables[sensitivity_tensor_index]+1e-10)\n",
    "    tf.summary.histogram('temporal_filter',trainables[temporal_filter_index]+1e-10)\n",
    "    tf.summary.scalar('loss_state_prediction',loss_state_prediction)\n",
    "    \n",
    "    # weight training parameters \n",
    "    tf.summary.histogram('weight_updates',weight_update+1e-10)\n",
    "    tf.summary.histogram('output_weight_updates',grad_cost_to_output_weight[-1]+1e-10)\n",
    "    tf.summary.histogram('output_bias_updates',grad_cost_to_output_bias[-1]+1e-10)\n",
    "    tf.summary.histogram('weights', trainables[kernel_index]+1e-10)\n",
    "    tf.summary.histogram('output_weights', trainables[output_weight_index]+1e-10)\n",
    "    tf.summary.histogram('output_addition', trainables[output_addition_index]+1e-10)\n",
    "    tf.summary.scalar('loss_output_prediction',loss_output_prediction)\n",
    "\n",
    "    # merge and save all \n",
    "    merged_summary_op=tf.summary.merge_all()\n",
    "    \n",
    "    # save training \n",
    "    saver = tf.train.Saver()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next we test the graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify initialization \n",
    "with tf.Session(graph=graph) as sess : \n",
    "    sess.run(init)\n",
    "    values,trainable_vars = sess.run([variable_names,trainables])\n",
    "    for k, v in zip(variable_names,values):\n",
    "        print([\"variable: \" , k])\n",
    "        #print([\"value: \" , v])\n",
    "        print([\"variable: \" , np.unicode_.find(k,'output')]) \n",
    "        print([\"shape: \" , v.shape])\n",
    "        #print(v) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we define a saving folder for the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/kernel_rnn/two_optimizaer/MNIST_gc_%d_eta_m_%d_eta_%d_batch_%d_run_%s\" %(grad_clip,tensor_learning_rate,weight_learning_rate,batch_size, datetime.now().strftime(\"%Y%m%d_%H%M\"))\n",
    "Path(log_dir).mkdir(exist_ok=True, parents=True)\n",
    "filelist = [ f for f in os.listdir(log_dir) if f.endswith(\".local\") ]\n",
    "for f in filelist:\n",
    "    os.remove(os.path.join(log_dir, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training loop for sensitivity and temporal filter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write graph into tensorboard \n",
    "tb_writer = tf.summary.FileWriter(log_dir,graph)\n",
    "# run a training session \n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(init)\n",
    "    for step in range(1,training_steps+1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        batch_x=batch_x.reshape((batch_size,timesteps,num_input))\n",
    "        \n",
    "        # run optimizer \n",
    "        tensor_opt, tensor_loss=sess.run([tensor_train_op,loss_state_prediction],\n",
    "                                         feed_dict={X:batch_x, Y:batch_y})\n",
    "        \n",
    "        # run summaries \n",
    "        merged_summary=sess.run(merged_summary_op,feed_dict={X:batch_x, Y:batch_y})\n",
    "        tb_writer.add_summary(merged_summary, global_step=step)\n",
    "\n",
    "        if step % display_step==0 or step==1 : \n",
    "            # get batch loss and accuracy \n",
    "            print('Step: {}, state Loss: {:.3f}'.format(step + 1, tensor_loss))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    test_data = mnist.test.images[:test_len].reshape((-1, timesteps, num_input))\n",
    "    test_label = mnist.test.labels[:test_len]\n",
    "    print(\"Testing Accuracy:\", \n",
    "        sess.run(loss_state_prediction, feed_dict={X: test_data, Y: test_label}))\n",
    "    save_path = saver.save(sess, log_dir+\"/model.ckpt\", global_step=step,write_meta_graph=True)\n",
    "    print(\"Model saved in path: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training loop for network weights and loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write graph into tensorboard \n",
    "tb_writer = tf.summary.FileWriter(log_dir,graph)\n",
    "# run a training session \n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(init)\n",
    "    for step in range(1,100):#range(1,training_steps+1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        batch_x=batch_x.reshape((batch_size,timesteps,num_input))\n",
    "        \n",
    "        # run optimizer \n",
    "        weight_opt, weight_loss,acc_train=sess.run([weight_train_op,loss_output_prediction,accuracy],\n",
    "                                         feed_dict={X:batch_x, Y:batch_y})\n",
    "        \n",
    "        # run summaries \n",
    "        merged_summary=sess.run(merged_summary_op,feed_dict={X:batch_x, Y:batch_y})\n",
    "        tb_writer.add_summary(merged_summary, global_step=step)\n",
    "\n",
    "        if step % display_step==0 or step==1 : \n",
    "            # get batch loss and accuracy \n",
    "            print('Step: {}, Train Loss: {:.3f}, Train Acc: {:.3f}'.format(\n",
    "            step + 1, weight_loss, acc_train))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    test_data = mnist.test.images[:test_len].reshape((-1, timesteps, num_input))\n",
    "    test_label = mnist.test.labels[:test_len]\n",
    "    print(\"Testing Accuracy:\", \n",
    "        sess.run(loss_output_prediction, feed_dict={X: test_data, Y: test_label}))\n",
    "    save_path = saver.save(sess, log_dir+\"/model.ckpt\", global_step=step,write_meta_graph=True)\n",
    "    print(\"Model saved in path: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(KeRNL)",
   "language": "python",
   "name": "kernl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
