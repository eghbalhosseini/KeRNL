{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "tf.enable_eager_execution()\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters \n",
    "learning_rate=0.001\n",
    "training_steps=500\n",
    "batch_size=32\n",
    "display_step=20\n",
    "\n",
    "#network parameters \n",
    "num_input_units=80\n",
    "timesteps= 28*28\n",
    "num_classes=10\n",
    "num_hidden_units =200\n",
    "num_output_units =10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state=tf.random_uniform(shape=[num_hidden_units,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_in=tf.where(tf.greater(tf.random_uniform(shape=[num_input_units,1]),1.0),\n",
    "                  tf.constant(1.0,shape=[num_input_units,1],dtype=tf.float32),\n",
    "                              tf.constant(0.0,shape=[num_input_units,1],dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=tf.constant(1.0,dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " [spike,state]=LIFLayer(num_hidden_units,num_input_units,dt,spike_in,state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OutputLayer(k,j,spike_in,dt,state):\n",
    "    neurons=k\n",
    "    inputs=j\n",
    "    tau_m = 5.0\n",
    "    # tensor variables \n",
    "    V_out = tf.contrib.eager.Variable(tf.constant(0.0,shape=[k,1],dtype=tf.float32),name='V_out')\n",
    "    W_out = tf.contrib.eager.Variable(tf.random_normal(shape=[k,j],mean=0.0,stddev=0.1,dtype=tf.float32),name='W_out')\n",
    "    G_out= tf.contrib.eager.Variable(tf.ones(shape=[k,1],dtype=tf.float32),name='G_out')\n",
    "    \n",
    "    ## update membrane dynamics \n",
    "    dv_out=tf.divide(state,tau_m)\n",
    "    V_temp=tf.subtract(state,dv_out*dt)\n",
    "    V_update=tf.clip_by_value(V_temp,tf.constant(0.0,shape=[k,1]),tf.constant(100.0,shape=[k,1]))\n",
    "    spike_in_float=tf.cast(spike_in,tf.float32)\n",
    "    spike_temp=tf.transpose(tf.tile(spike_in_float,[1,k]))\n",
    "    weight_update=tf.multiply(W_out,spike_temp)\n",
    "    G_update=tf.reduce_sum(weight_update, 1, keepdims=True)\n",
    "    # not sure about this step\n",
    "    v_membrane = tf.add(V_update,G_update)\n",
    "    return v_membrane\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_rec=tf.where(tf.greater(tf.random_uniform(shape=[num_hidden_units,1]),0.5),\n",
    "                  tf.constant(1.0,shape=[num_hidden_units,1],dtype=tf.float32),\n",
    "                              tf.constant(0.0,shape=[num_hidden_units,1],dtype=tf.float32))\n",
    "state_out=tf.random_uniform(shape=[num_output_units,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vout = OutputLayer(num_output_units,num_hidden_units,spike_rec,dt,state_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as t:\n",
    "    t.watch(spike_rec)\n",
    "    Vout = OutputLayer(num_output_units,num_hidden_units,spike_rec,dt,state_out)\n",
    "\n",
    "dz_dx = t.gradient(Vout, spike_rec) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_rec=tf.where(tf.greater(tf.random_uniform(shape=[num_hidden_units,1]),0.5),\n",
    "                  tf.constant(1.0,shape=[num_hidden_units,1],dtype=tf.float32),\n",
    "                              tf.constant(0.0,shape=[num_hidden_units,1],dtype=tf.float32))\n",
    "x = tf.random_uniform(shape=[num_output_units,1])\n",
    "W_o = tf.contrib.eager.Variable(tf.random_normal(shape=[num_output_units,num_hidden_units],mean=0.0,stddev=0.1,dtype=tf.float32))\n",
    "\n",
    "with tf.GradientTape() as t:\n",
    "  t.watch(x)\n",
    "  y=tf.divide(x,5.0)\n",
    "  z=tf.subtract(x,y*dt)\n",
    "  z1=tf.clip_by_value(z,tf.constant(0.0,shape=[num_output_units,1]),tf.constant(0.5,shape=[num_output_units,1]))\n",
    "  z2=tf.cast(z1,tf.float64)\n",
    "  sp=tf.cast(spike_rec,tf.float32)\n",
    "  sp1=tf.transpose(tf.tile(sp,[1,num_output_units]))\n",
    "  w1=tf.multiply(W_o,sp1)\n",
    "  G_up=tf.reduce_sum(w1, 1, keepdims=True) \n",
    "  v_mem = tf.divide(z1,G_up)\n",
    "# Derivative of z with respect to the original input tensor x\n",
    "dz_dx = t.gradient(z2, x)\n",
    "dz_dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state=tf.clip_by_value(tf.add(tf.random_uniform(shape=[num_hidden_units,1]),tf.constant(.5,shape=[num_hidden_units,1])),0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "[spike,state]=LIFLayer(num_hidden_units,num_input_units,dt,spike_in,state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as t:\n",
    "    t.watch(spike_rec)\n",
    "    v_update=tf.where(tf.cast(spike_rec,tf.bool),tf.constant(0.0,shape=[num_hidden_units,1],dtype=tf.float32),state)\n",
    "    v_update1=tf.subtract(state,tf.multiply(spike_rec,tf.constant(1.0,shape=[num_hidden_units,1],dtype=tf.float32)))\n",
    "    Spike_temp=tf.transpose(tf.tile(spike_rec,[1,num_hidden_units]))\n",
    "dz_dx = t.gradient(Spike_temp, spike_rec) \n",
    "#plt.figure()\n",
    "#plt.subplot(3,1,1)\n",
    "#plt.plot(v_update1.numpy())\n",
    "#plt.subplot(3,1,2)\n",
    "#plt.plot(v_update.numpy())\n",
    "#plt.subplot(3,1,3)\n",
    "#plt.plot(state.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LIFLayer(n,m,dt,Spike_in,state):\n",
    "    neurons=n\n",
    "    inputs=m\n",
    "    tau_m = 5.0\n",
    "    v_theta = 1.0\n",
    "    v_reset = 0.0\n",
    "    tau_s = 5.0\n",
    "    tau_refract = 3.0\n",
    "    # tensor variables \n",
    "    V = tf.contrib.eager.Variable(tf.constant(0.0,shape=[n,1],dtype=tf.float32),name='V')\n",
    "    Spike = tf.contrib.eager.Variable(tf.constant(v_reset,shape=[n,1],dtype=tf.float32),name='Spike')\n",
    "    W_rec = tf.contrib.eager.Variable(tf.random_normal(shape=[n,n],mean=1.0,stddev=0.1,dtype=tf.float32),name='W_rec')\n",
    "    W_in = tf.contrib.eager.Variable(tf.random_normal(shape=[n,m],mean=1.0,stddev=0.1,dtype=tf.float32),name='W_in')\n",
    "    G=tf.contrib.eager.Variable(tf.ones(shape=[n,1],dtype=tf.float32),name='G')\n",
    "    S=tf.contrib.eager.Variable(tf.zeros(shape=[n,n],dtype=tf.float32),name='S')\n",
    "    G_in=tf.contrib.eager.Variable(tf.ones(shape=[n,1],dtype=tf.float32),name='G_in')\n",
    "    S_in=tf.contrib.eager.Variable(tf.zeros(shape=[n,m],dtype=tf.float32),name='S_in')\n",
    "    I_syn=tf.contrib.eager.Variable(tf.ones(shape=[n,1],dtype=tf.float32),name='I_syn')\n",
    "    t_reset=tf.contrib.eager.Variable(tf.ones(shape=[n,1],dtype=tf.float32),name='t_reset')\n",
    "    \n",
    "    # subfunctions \n",
    "    @tf.custom_gradient\n",
    "    def calculate_crossing_op(x):\n",
    "        x_norm=tf.divide(tf.subtract(x,tf.constant(v_theta,shape=[n,1])),\n",
    "                         tf.constant(v_theta,shape=[n,1]))\n",
    "        temp=tf.greater_equal(x,tf.constant(v_theta,shape=[n,1],dtype=tf.float32))\n",
    "        def grad(dy):            \n",
    "            return tf.maximum(tf.constant(0.0,dtype=tf.float32),tf.subtract(tf.constant(1.0,dtype=tf.float32),tf.abs(x_norm)))  \n",
    "        return tf.cast(temp,tf.float32), grad\n",
    "\n",
    "    ## spiking neuron dynamics \n",
    "    Spike=calculate_crossing_op(state)\n",
    "    v_update=tf.subtract(state,tf.multiply(tf.cast(Spike,tf.float32),tf.constant(v_theta,shape=[n,1],dtype=tf.float32)))\n",
    "    ## update conductance for recurrent spikes \n",
    "    dS_op=tf.divide(S,tau_s)\n",
    "    dS_in_op=tf.divide(S_in,tau_s)\n",
    "    S_temp=tf.subtract(S,dS_op*dt)\n",
    "    S_in_temp=tf.subtract(S_in,dS_in_op*dt)\n",
    "    S_op=tf.clip_by_value(S_temp,tf.constant(0.0,shape=[n,n]),tf.constant(100.0,shape=[n,n]))\n",
    "    S_in_op=tf.clip_by_value(S_in_temp,tf.constant(0.0,shape=[n,m]),tf.constant(100.0,shape=[n,m]))\n",
    "    Spike_op_float=tf.cast(Spike,tf.float32)\n",
    "    Spike_in_op_float=tf.cast(Spike_in,tf.float32)\n",
    "    Spike_ax=tf.clip_by_value(tf.subtract(tf.transpose(tf.tile(Spike_op_float,[1,n])),\n",
    "                                            tf.eye(n,dtype=tf.float32)),0.0,100.0)\n",
    "\n",
    "    Spike_in_temp=tf.transpose(tf.tile(Spike_in_op_float,[1,n]))\n",
    "    Spike_in_ax=Spike_in_temp\n",
    "    S_update= tf.add(S,Spike_ax)\n",
    "    S_in_update=tf.add(S_in,Spike_in_ax)\n",
    "    G_op=tf.reduce_sum(tf.multiply(W_rec,S_update), 1, keepdims=True)\n",
    "    G_in_op=tf.reduce_sum(tf.multiply(W_in,S_in_update), 1, keepdims=True)\n",
    "    I_input=tf.add(tf.multiply(G_op,v_update),tf.multiply(G_in_op,v_update))\n",
    "    ## update voltages\n",
    "    # find neurons in refractory \n",
    "    t_subtract= tf.subtract(t_reset,tf.constant(1.0,shape=[n,1]))\n",
    "    t_margin=tf.clip_by_value(t_subtract,0.0,100)\n",
    "    t_reset_update=tf.add(t_margin,tf.multiply(Spike,tf.constant(v_theta,shape=[n,1],dtype=tf.float32)))\n",
    "    eligilible_update=tf.cast(tf.equal(t_reset_update,tf.constant(0.0,shape=[n,1])),tf.float32)\n",
    "    # update voltage\n",
    "    dV_op=tf.add(tf.constant(v_reset,shape=[n,1],dtype=tf.float32),tf.multiply(eligilible_update,\n",
    "                                                                               tf.divide(tf.subtract(I_input,v_update),tau_m)))\n",
    "    V_out=tf.add(v_update,tf.multiply(dV_op,dt))\n",
    "    return Spike, v_update\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[spike,state]=LIFLayer(num_hidden_units,num_input_units,dt,spike_in,state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gradient(x,n):\n",
    "    @tf.custom_gradient\n",
    "    def calculate_crossing_op(x):\n",
    "        x_norm=tf.divide(tf.subtract(x,tf.constant(1.0,shape=[n,1])),tf.constant(1.0,shape=[n,1]))\n",
    "        temp=tf.greater_equal(x,tf.constant(1.0,shape=[n,1],dtype=tf.float32))\n",
    "        def grad(dy):            \n",
    "            return tf.maximum(tf.constant(0.0,dtype=tf.float32),tf.subtract(tf.constant(1.0,dtype=tf.float32),tf.abs(x_norm)))  \n",
    "        return tf.cast(temp,tf.float32), grad\n",
    "    z = calculate_crossing_op(x)\n",
    "    return z \n",
    "\n",
    "x = tf.reshape(tf.linspace(-10.0, 10.0, 300, name=\"linspace\"),shape=[300,1])\n",
    "with tf.GradientTape() as t:\n",
    "    t.watch(x)\n",
    "    z = test_gradient(x,300)\n",
    "\n",
    "dz_dx = t.gradient(z, x) \n",
    "plt.figure(figsize=[15,10])\n",
    "plt.plot(x.numpy().flatten(),z.numpy().flatten())\n",
    "plt.plot(x.numpy(),dz_dx.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gradient(x,Spike_in,n,m):\n",
    "    tau_m = 5.0\n",
    "    v_theta = 1.0\n",
    "    v_reset = 0.0\n",
    "    tau_s = 5.0\n",
    "    tau_refract = 3.0\n",
    "    S=tf.contrib.eager.Variable(tf.zeros(shape=[n,n],dtype=tf.float32),name='S')\n",
    "    S_in=tf.contrib.eager.Variable(tf.zeros(shape=[n,m],dtype=tf.float32),name='S_in')\n",
    "    W_rec = tf.contrib.eager.Variable(tf.random_normal(shape=[n,n],mean=0.0,stddev=0.1,dtype=tf.float32),name='W_rec')\n",
    "    W_in = tf.contrib.eager.Variable(tf.random_normal(shape=[n,m],mean=1.0,stddev=0.1,dtype=tf.float32),name='W_in')\n",
    "    t_reset=tf.contrib.eager.Variable(tf.ones(shape=[n,1],dtype=tf.float32),name='t_reset')\n",
    "    @tf.custom_gradient\n",
    "    def calculate_crossing_op(x):\n",
    "        x_norm=tf.divide(tf.subtract(x,tf.constant(1.0,shape=[n,1])),tf.constant(1.0,shape=[n,1]))\n",
    "        temp=tf.greater_equal(x,tf.constant(1.0,shape=[n,1],dtype=tf.float32))\n",
    "        def grad(dy):            \n",
    "            return tf.maximum(tf.constant(0.0,dtype=tf.float32),tf.subtract(tf.constant(1.0,dtype=tf.float32),tf.abs(x_norm)))  \n",
    "        return tf.cast(temp,tf.float32), grad\n",
    "    Spike=calculate_crossing_op(x)\n",
    "    v_update=tf.subtract(x,tf.multiply(tf.cast(Spike,tf.float32),tf.constant(1.0,shape=[n,1],dtype=tf.float32)))\n",
    "    ## update conductance for recurrent spikes \n",
    "    dS_op=tf.divide(S,3.0)\n",
    "    dS_in_op=tf.divide(S_in,tau_s)\n",
    "    S_temp=tf.subtract(S,dS_op*dt)\n",
    "    S_in_temp=tf.subtract(S_in,dS_in_op*dt)\n",
    "    S_op=tf.clip_by_value(S_temp,tf.constant(0.0,shape=[n,n]),tf.constant(100.0,shape=[n,n]))\n",
    "    S_in_op=tf.clip_by_value(S_in_temp,tf.constant(0.0,shape=[n,m]),tf.constant(100.0,shape=[n,m]))\n",
    "    Spike_op_float=tf.cast(Spike,tf.float32)\n",
    "    Spike_in_op_float=tf.cast(Spike_in,tf.float32)\n",
    "    Spike_ax=tf.clip_by_value(tf.subtract(tf.transpose(tf.tile(Spike_op_float,[1,n])),\n",
    "                                            tf.eye(n,dtype=tf.float32)),0.0,100.0)\n",
    "\n",
    "    Spike_in_temp=tf.transpose(tf.tile(Spike_in_op_float,[1,n]))\n",
    "    Spike_in_ax=Spike_in_temp\n",
    "    S_update= tf.add(S,Spike_ax)\n",
    "    S_in_update=tf.add(S_in,Spike_in_ax)\n",
    "    G_op=tf.reduce_sum(tf.multiply(W_rec,S_update), 1, keepdims=True)\n",
    "    G_in_op=tf.reduce_sum(tf.multiply(W_in,S_in_update), 1, keepdims=True)\n",
    "    I_input=tf.add(tf.multiply(G_op,v_update),tf.multiply(G_in_op,v_update))\n",
    "\n",
    "    ## update voltages\n",
    "    # find neurons in refractory \n",
    "    t_subtract= tf.subtract(t_reset,tf.constant(1.0,shape=[n,1]))\n",
    "    \n",
    "    t_margin=tf.clip_by_value(t_subtract,0.0,100)\n",
    "    t_reset_update=tf.add(t_margin,tf.multiply(Spike,tf.constant(v_theta,shape=[n,1],dtype=tf.float32)))\n",
    "\n",
    "    eligilible_update=tf.cast(tf.equal(t_reset_update,tf.constant(0.0,shape=[n,1])),tf.float32)\n",
    "#     # update voltage\n",
    "    dV_op=tf.add(tf.constant(v_reset,shape=[n,1],dtype=tf.float32),tf.multiply(eligilible_update,\n",
    "                                                                                tf.divide(tf.subtract(I_input,v_update),tau_m)))\n",
    "\n",
    "    V_out=tf.add(v_update,tf.multiply(dV_op,dt))\n",
    "    return V_out\n",
    "\n",
    "x = tf.reshape(tf.linspace(-10.0, 10.0, 200, name=\"linspace\"),shape=[200,1])\n",
    "with tf.GradientTape() as t:\n",
    "    t.watch(x)\n",
    "    z = test_gradient(x,spike_in,200,num_input_units)\n",
    "\n",
    "dz_dx = t.gradient(z, x) \n",
    "#plt.figure(figsize=[15,10])\n",
    "#plt.plot(x.numpy().flatten(),z.numpy().flatten())\n",
    "#plt.plot(x.numpy(),dz_dx.numpy())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_test=tf.get_variable('W_test',shape=[10,10],dtype=tf.float32,initializer=tf.random_normal_initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calcualte_crossings(x,threshold,output_size):\n",
    "    \"\"\"input :x : a 2D tensor with batch x n \n",
    "    outputs a tensor with the same size as x \n",
    "    and values of 0 or 1 depending on comparison between \n",
    "    x and threshold\"\"\" \n",
    "    dtype=x.dtype\n",
    "    shape=x.get_shape()\n",
    "    total_x_size=shape[1].value\n",
    "    thresholds=tf.constant(threshold,shape=[total_x_size,output_size],dtype=dtype)\n",
    "    # if it has one row \n",
    "    res=tf.greater_equal(x,thresholds,dtype=tf.float32)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpikingRnnCell(tf.contrib.rnn.RNNCell):\n",
    "    # input and output of the network are spikes\n",
    "    # states of the network are membrane voltage and synaptic input S \n",
    "    def __init__(self,num_units,tau_m=5.0,v_theta=1.0,v_reset=0.0,tau_s=5.0,tau_refract=3.0,reuse=None):\n",
    "        super(SpikingRnnCell,self).__init__(_reuse=reuse)\n",
    "        self.num_units=num_units\n",
    "        self.tau_m=tau_m\n",
    "        self.v_theta=v_theta\n",
    "        self.v_reset=v_reset\n",
    "        self.tau_s=tau_s\n",
    "        self.tau_refract=tau_refract\n",
    "        self._weight_linear=None\n",
    "        \n",
    "        # variables \n",
    "        \n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self.num_units\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return 1\n",
    "    \n",
    "        \n",
    "    #call routine is used by tensorflow to compute the output and next state of the network,\n",
    "    def __call__(self,inputs,state):\n",
    "        \n",
    "        #output is some funtion of states\n",
    "        # first slice up the state into three vectors, \n",
    "        v_mem=tf.slice(state,[0,0],[self.num_units,1])\n",
    "        g_mem=tf.slice(state,[self.num_units,0],[self.num_units,1])\n",
    "        t_mem=tf.slice(state,[2*self.num_units,0],[self.num_units,1])\n",
    "        #\n",
    "        spike=tf.cast(tf.greater_equal(v_mem,tf.constant(self.v_theta,shape=[self.state_size,1],dtype=tf.float32)),tf.float32)\n",
    "        v_update=tf.subtract(v_mem,tf.multiply(tf.cast(spike,tf.float32),\n",
    "                                               tf.constant(self.v_theta,shape=[self.num_units,1],dtype=tf.float32)))\n",
    "        #\n",
    "        #spike_rec=tf.clip_by_value(tf.subtract(tf.transpose(tf.tile(spike,[1,self.num_units])),\n",
    "        #                                    tf.eye(self.num_units,dtype=tf.float32)),0.0,100.0)\n",
    "        #spike_in=tf.transpose(tf.tile(tf.cast(inputs,tf.float32),[1,self.num_units]))\n",
    "        if self._weight_linear is None:\n",
    "            self._weight_linear=_Linear([inputs,spike],self.num_units,False)\n",
    "            \n",
    "        g_update=self._weight_linear([spike,inputs])\n",
    "        \n",
    "        \n",
    "        #g_update=tf.add(tf.reduce_sum(tf.multiply(self.W_rec,spike_rec), 1, keepdims=True),\n",
    "        #                tf.reduce_sum(tf.multiply(self.W_in,spike_in), 1, keepdims=True))\n",
    "        #\n",
    "        dg_mem=tf.subtract(g_mem,tf.divide(g_mem,self.tau_s))\n",
    "        #g_mem_new=tf.add(g_update,tf.subtract(g_mem,dg_mem))\n",
    "        g_mem_new=g_update\n",
    "        \n",
    "        #\n",
    "        if  np.not_equal(g_update.get_shape()[0],self.state_size) :\n",
    "            raise NotImplementedError(\"Abstract method\")\n",
    "        \n",
    "        I_input=tf.multiply(tf.transpose(g_update),v_update)\n",
    "        #                \n",
    "        t_subtract=tf.subtract(t_mem,tf.constant(1.0,shape=[self.num_units,1]))\n",
    "        t_margin=tf.clip_by_value(t_subtract,0.0,100.0)\n",
    "        t_mem_new=tf.add(t_margin,tf.multiply(spike,tf.constant(self.tau_refract,shape=[self.num_units,1],dtype=tf.float32)))\n",
    "        update_trace=tf.cast(tf.equal(t_mem_new,tf.constant(0.0,shape=[self.num_units,1])),tf.float32)\n",
    "        #\n",
    "        dv_mem=tf.add(tf.constant(self.v_reset,shape=[self.num_units,1],dtype=tf.float32),\n",
    "                      tf.multiply(update_trace,tf.divide(tf.subtract(I_input,v_update),self.tau_m)))\n",
    "        v_mem_new=tf.add(v_update,dv_mem)\n",
    "\n",
    "        return spike, tf.concat([v_mem_new,g_mem_new,t_mem_new],0)\n",
    "    \n",
    "    ## crossing fucntion \n",
    "    \n",
    "    #def calculate_crossing_op(self,x):\n",
    "    #    x_norm=tf.divide(tf.subtract(x,tf.constant(self.v_theta,shape=[self.state_size,1])),\n",
    "    #                     tf.constant(self.v_theta,shape=[self.state_size,1]))\n",
    "    #    temp=tf.greater_equal(x,tf.constant(self.v_theta,shape=[self.state_size,1],dtype=tf.float32))\n",
    "    #    def grad(dy):            \n",
    "    #        return tf.maximum(tf.constant(0.0,dtype=tf.float32),tf.subtract(tf.constant(1.0,dtype=tf.float32),tf.abs(x_norm)))  \n",
    "    #    return temp, grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calcualte_crossings(x,threshold):\n",
    "    \"\"\"input :x : a 2D tensor with batch x n \n",
    "    outputs a tensor with the same size as x \n",
    "    and values of 0 or 1 depending on comparison between \n",
    "    x and threshold\"\"\" \n",
    "    @tf.custom_gradient\n",
    "    def crossings(x):\n",
    "        dtype=x.dtype\n",
    "        shape=x.get_shape()\n",
    "        thresholds=tf.constant(threshold,shape=[shape[0].value,shape[1].value],dtype=dtype)\n",
    "        # if it has one row \n",
    "        res=tf.greater_equal(x,thresholds)\n",
    "        def grad(dy):\n",
    "            # calculate 1-|x|\n",
    "            temp=1-tf.abs(x)\n",
    "            dyres=tf.maximum(temp,0.0)\n",
    "            return dyres\n",
    "        return tf.cast(res,dtype=dtype), grad\n",
    "    z=crossings(x)\n",
    "    return z "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.random_normal(mean=1,stddev=.5,shape=[1,100])\n",
    "x = tf.reshape(tf.linspace(-10.0, 10.0, 300, name=\"linspace\"),shape=[1,300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = _calcualte_crossings(x,1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.scalar_mul(5.0,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import variable_scope as vs\n",
    "def _tensor_linear(x,output_size,w_type):\n",
    "    \"\"\"input - x : a 3D tensor with batch x n x m \n",
    "    w_type is a string indicating with weight is being process : W_in or W_rec\n",
    "    y is a 2D with size batch x m\n",
    "    outputs a tensor  with size batch x output_size\n",
    "    \"\"\" \n",
    "    shape_x=x.get_shape()\n",
    "    # \n",
    "    scope=vs.get_variable_scope()\n",
    "    with vs.variable_scope(scope) as outer_scope:\n",
    "        if tf.strings.regex_full_match(w_type,_INPUT_WEIGHT_NAME):\n",
    "            weight=tf.get_variable(_INPUT_WEIGHT_NAME,[shape_x[1],shape_x[2]]) # [ n x m]\n",
    "        elif tf.strings.regex_full_match(w_type,_RECURRENT_WEIGHT_NAME):\n",
    "            weight=tf.get_variable(_RECURRENT_WEIGHT_NAME,[shape_x[1],shape_x[2]]) # [n x n]\n",
    "        else:\n",
    "            raise ValueError(\"expecting W_rec or W_in as weight input\")\n",
    "        #\n",
    "        # expand x to be a 2D tensor with (batch x n) x m dimension\n",
    "        x_aux=tf.reshape(x,[-1,shape_x[2]])\n",
    "        # apply weights \n",
    "        res_aux=tf.matmul(x_aux,weight,transpose_b=True)\n",
    "        # sum along the last dimension\n",
    "        res_long=tf.reduce_sum(res_aux,tf.rank(res_aux)-1)\n",
    "        # reshape to match output size \n",
    "        res=tf.reshape(res_long,[-1,output_size])\n",
    "        return res\n",
    "\n",
    "_INPUT_WEIGHT_NAME = \"W_in\"\n",
    "_RECURRENT_WEIGHT_NAME = \"W_rec\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(np.arange(1, 13, dtype=np.float32),\n",
    "                shape=[2, 3, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = tf.constant(np.arange(1.0, 13.0, dtype=np.int32),\n",
    "                shape=[2, 3, 2],dtype=tf.float32)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_rec=tf.contrib.eager.Variable(tf.constant(1.0,shape=[3,2]),name='W_rec')\n",
    "W_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=tf.constant([[1,0,1],[0,1,0],[1,1,0]],dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=tf.matmul(b,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=tf.reshape(h,[-1,3,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf.matrix_set_diag(a, tf.zeros(a.shape[0:-1]), name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = tf.reshape(embed, [-1, m])\n",
    "h = tf.matmul(embed, U)\n",
    "h = tf.reshape(h, [-1, n, c])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tensor_linear_rec_in(x,y,output_size):\n",
    "    \"\"\"input - x : a 3D tensor with batch x n x n \n",
    "    y is a 2D with size batch x m\n",
    "    outputs a tensor  with size batch x output_size\n",
    "    \"\"\" \n",
    "    shape_x=x.get_shape()\n",
    "    shape_y=y.get_shape()\n",
    "    # \n",
    "    scope=vs.get_variable_scope()\n",
    "    with vs.variable_scope(scope) as outer_scope:\n",
    "#        weight_rec=tf.get_variable(_RECURRENT_WEIGHT_NAME,[shape_x[1],shape_x[2]],\n",
    "#                                   initializer=tf.constant(1.0,shape=[shape_x[1],shape_x[2]])) # [n x n]\n",
    "#        weight_in=tf.get_variable(_INPUT_WEIGHT_NAME,[shape_y[1],shape_y[2]],\n",
    "#                                  initializer=tf.constant(0.0,shape=[shape_y[1],shape_y[2]])) # [n x m]\n",
    "        weight_rec=tf.get_variable(_RECURRENT_WEIGHT_NAME,\n",
    "                                   initializer=tf.constant(0.0,shape=[shape_x[1],shape_x[2]])) # [n x n]\n",
    "        weight_in=tf.get_variable(_INPUT_WEIGHT_NAME,\n",
    "                                  initializer=tf.constant(.5,shape=[shape_y[1],shape_y[2]])) # [n x m]\n",
    "\n",
    "        #\n",
    "        # apply_weights \n",
    "        #recurrent\n",
    "        res_rec_aux=tf.multiply(x,weight_rec)\n",
    "        res_rec_final=tf.reduce_sum(res_rec_aux,tf.rank(res_rec_aux)-1)\n",
    "        #input\n",
    "        res_in_aux=tf.multiply(y,weight_in)\n",
    "        res_in_final=tf.reduce_sum(res_in_aux,tf.rank(res_in_aux)-1)\n",
    "        # sum both \n",
    "        res=tf.add(res_in_final,res_rec_final)\n",
    "\n",
    "        return res\n",
    "\n",
    "_INPUT_WEIGHT_NAME = \"W_in\"\n",
    "_RECURRENT_WEIGHT_NAME = \"W_rec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(np.arange(1.0, 33.0, dtype=np.int32),\n",
    "                shape=[2, 4, 4],dtype=tf.float32)\n",
    "b = tf.constant(np.arange(1.0, 25.0, dtype=np.int32),\n",
    "                shape=[2, 4, 3],dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_sum(b,tf.rank(b)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tensor_linear_rec_in(a,b,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo():\n",
    "  with tf.variable_scope(\"foo\", reuse=tf.AUTO_REUSE):\n",
    "    v = tf.get_variable(\"v\", [1])\n",
    "  return v\n",
    "\n",
    "v1 = foo()  # Creates v.\n",
    "v2 = foo()  # Gets the same, existing v.\n",
    "assert v1 == v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=tf.constant([[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0],[.5,.5,.5,.5],[.5,.5,.5,.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1=tf.constant([[1.0,1.0,1.0],[1.0,1.0,1.0],[.5,.5,.5],[.5,.5,.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=tf.multiply(a,w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tensor_expand_dim(x,y,output_size):\n",
    "    \"\"\"input - x : a 2D tensor with batch x n \n",
    "    y is a 2D with size batch x m\n",
    "    outputs is 3D tensor with size batch x n x n and batch x n x m \n",
    "    \"\"\" \n",
    "    shape_x=x.get_shape()\n",
    "    shape_y=y.get_shape()\n",
    "    # define a matrix for removing the diagonal in recurrent spikes \n",
    "    x_diag_fixer=tf.get_variable('x_diag_fixer',\n",
    "                                 initializer=tf.subtract(tf.constant(1.0,shape=[shape_x[1],shape_x[1]]),\n",
    "                                                    tf.eye(output_size))) # [n x n]\n",
    "    # expand x  \n",
    "    x_temp=tf.reshape(tf.tile(x,[1,output_size]),[-1,output_size,shape_x[1]])\n",
    "    # remove diagonal \n",
    "    x_expand=tf.multiply(x_temp,x_diag_fixer)\n",
    "    # expand y  \n",
    "    y_expand=tf.reshape(tf.tile(y,[1,output_size]),[-1,output_size,shape_y[1]])\n",
    "\n",
    "\n",
    "    return x_expand, y_expand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.constant([[1,0,0],[0,1,1],[0,0,1],[1,1,0]],dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=tf.constant([[1,0],[0,1],[0,0],[1,1]],dtype=tf.float32)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_expand,b_expand=_tensor_expand_dim(a,b,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_diag_fixer=tf.get_variable('x_diag_fixer',initializer=tf.subtract(tf.constant(1.0,shape=[3,3]),\n",
    "                                                    tf.eye(3))) # [n x n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=x_diag_fixer.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.greater_equal(a,1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_spikes=np.random.randint(2,size=[2,10,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_spikes=np.random.randint(2,size=[2,10,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_spikes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_input_values = tf.constant(input_spikes, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=tf.random_uniform([batch_size,num_of_inputs])\n",
    "batch_size=10\n",
    "num_of_inputs=1\n",
    "inputs=tf.random_uniform([batch_size,num_of_inputs])\n",
    "shape_x=np.array([batch_size,num_of_inputs])\n",
    "threshold_size=5\n",
    "weight=tf.concat([tf.constant(1.0,shape=[shape_x[0],threshold_size]),\n",
    "                                                              tf.constant(-1.0,shape=[shape_x[0],threshold_size])],\n",
    "                                                                axis=1\n",
    "                                                               )\n",
    "threshold=tf.constant([.2,.4,.6,.8,1],shape=[1,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=tf.constant([.2,.4,.6,.8,1],shape=[1,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_expand=tf.matmul(tf.constant(1.0,shape=[shape_x[0],1]),threshold)\n",
    "threshold_expand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "do positive crossing first "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_expand=tf.matmul(inputs,tf.constant(1.0,shape=[shape_x[1],threshold_size]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_outputs=tf.cast(tf.greater(inputs_expand-threshold_expand,0),tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_outputs=tf.cast(tf.greater(threshold_expand-inputs_expand,0),tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.concat([positive_outputs,negative_outputs],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.divide(1,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.linspace(np.divide(1,40),1,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.constant(tf.linspace(1.0,2,2),shape=[1,40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.random_uniform(shape=[10,1],dtype=tf.float32)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calcualte_crossings(x,threshold):\n",
    "    \"\"\"input :x : a 2D tensor with batch x n \n",
    "    outputs a tensor with the same size as x \n",
    "    and values of 0 or 1 depending on comparison between \n",
    "    x and threshold\"\"\" \n",
    "    @tf.custom_gradient\n",
    "    def crossings(x):\n",
    "        dtype=x.dtype\n",
    "        shape=x.get_shape()\n",
    "        thresholds=tf.constant(threshold,shape=[shape[0].value,shape[1].value],dtype=dtype)\n",
    "        # if it has one row \n",
    "        res=tf.greater_equal(x,thresholds)\n",
    "        def grad(dy):\n",
    "            # calculate 1-|x|\n",
    "            temp=1-tf.abs(x)\n",
    "            dyres=tf.maximum(temp,0.0)\n",
    "            return dyres\n",
    "        return tf.cast(res,dtype=dtype), grad\n",
    "    z=crossings(x)\n",
    "    return z \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(10)\n",
    "a=tf.random_uniform(shape=[5,5],dtype=tf.float32)\n",
    "threshold=tf.constant([.5,0,.1,.2,0],shape=[5,1])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.negative(tf.divide(tf.sign(threshold-a)-1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "plt.figure(figsize=[15,10])\n",
    "colors_map=cm.viridis(np.linspace(0,1,z.shape[0]))\n",
    "for t in range(z.shape[1]):\n",
    "    cross=np.argwhere(z[:,t])\n",
    "    plt.scatter(cross*0+t,cross,color=colors_map[cross.flatten(),:],s=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([-1,1],dtype=tf.float32)\n",
    "b = tf.nn.relu(a)\n",
    "g = tf.gradients(b, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer()\n",
    "    c=sess.run(g)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=tf.constant(np.ones(4,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.arange(12).reshape(4,3)\n",
    "b=np.arange(8).reshape(4,-1)\n",
    "c=np.arange(24).reshape(4,3,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.constant(np.arange(10.0),shape=[2,5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=tf.constant(np.arange(6.0), shape=[2,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tf.einsum(\"un,uv->unv\", a, b)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=2\n",
    "n=3\n",
    "m=4\n",
    "a=tf.constant(np.arange(6.0),shape=[batch,n])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=tf.constant(np.arange(24.0),shape=[batch,n,m])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.multiply(b,tf.expand_dims(a,axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=tf.expand_dims(tf.expand_dims(b,axis=0),axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use itertools and dictionary to create a loop over differet parameters \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_train_step={\"A\":10,\"B\":100,\"C\":1000}\n",
    "dict_batch_size={\"A\":50,\"B\":150,\"C\":250}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=list(itertools.permutations('ABC', 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_train_step[a[0][1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.constant()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=tf.constant(np.arange(24.0),shape=[batch,n,m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=2\n",
    "n=3\n",
    "m=4\n",
    "a=tf.constant(np.arange(6.0),shape=[batch,n])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.transpose(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.constant(np.arange(18.0),shape=[3,3,2],dtype=tf.float32)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=tf.roll(a,shift=-1, axis=1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[:,-0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=tf.expand_dims(tf.concat([tf.ones([2,2],tf.float32),tf.zeros([1,2],dtype=tf.float32)],axis=0),axis=0)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.multiply(a,c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=103, shape=(4, 3), dtype=float32, numpy=\n",
       "array([[0., 0., 0.],\n",
       "       [1., 0., 1.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 1., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=tf.constant(np.array([1,3,1]),dtype=tf.int32)\n",
    "batch_size=2\n",
    "tf.transpose(tf.one_hot(b,depth=max(b)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=tf.tile(tf.expand_dims(tf.transpose(tf.one_hot(b,depth=max(b)+1)),axis=0),[batch_size,1,1])\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.constant([[1,1,1],[0,0,1]],tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=tf.multiply(tf.expand_dims(a,axis=-2),c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S=tf.roll(d,shift=-1,axis=1)\n",
    "S[:,0,:]\n",
    "tf.constant(0,shape=[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.diag(tf.concat([tf.ones(3,tf.float32),tf.constant([0],dtype=tf.float32)],axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=4, shape=(2,), dtype=int32, numpy=array([2, 2], dtype=int32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_hidden=2\n",
    "num_batch=3\n",
    "delay_vect=tf.random_uniform(minval=1,maxval=3,shape=[num_hidden],dtype=tf.int32)\n",
    "delay_vect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike=tf.cast(tf.random_uniform(minval=0,maxval=2,shape=[num_batch,num_hidden],dtype=tf.int32),tf.float32)\n",
    "spike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delay_mat=tf.transpose(tf.one_hot(delay_vect,depth=max(delay_vect)+1))\n",
    "delay_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delay_tensor=tf.tile(tf.expand_dims(delay_mat,axis=0),[num_batch,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_spike=tf.einsum('uvn,vn->uvn',delay_tensor,spike)\n",
    "output_spike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf.multiply(tf.expand_dims(spike,axis=-2),delay_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elems = tf.constant([1, 2, 3, 4, 5, 6],dtype=tf.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=326, shape=(2, 2), dtype=int32, numpy=\n",
       "array([[0, 1],\n",
       "       [1, 0]], dtype=int32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spike=tf.random_uniform(minval=0,maxval=2,shape=[batch_size,num_hidden],dtype=tf.int32)\n",
    "spike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=235, shape=(2, 2, 3), dtype=float32, numpy=\n",
       "array([[[0., 0., 0.],\n",
       "        [0., 1., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.multiply(tf.expand_dims(tf.cast(spike,tf.float32),axis=-1),tf.one_hot(spike,depth=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "synaptic_delay=tf.constant([3,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=340, shape=(2, 2), dtype=int32, numpy=\n",
       "array([[0, 2],\n",
       "       [3, 0]], dtype=int32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.multiply(synaptic_delay,spike)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=442, shape=(2, 4, 2), dtype=float32, numpy=\n",
       "array([[[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [1., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.multiply(tf.expand_dims(tf.cast(spike,tf.float32),axis=-2),tf.one_hot(tf.multiply(synaptic_delay,spike),depth=4,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "synaptic_shape=np.array([1,2,3])\n",
    "correction_mat=tf.expand_dims(tf.concat([tf.ones([synaptic_shape[1]-1,synaptic_shape[2]],tf.float32),tf.zeros([1,synaptic_shape[2]],dtype=tf.float32)],axis=0),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=525, shape=(1, 2, 3), dtype=float32, numpy=\n",
       "array([[[1., 1., 1.],\n",
       "        [0., 0., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correction_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=2\n",
    "delay=4\n",
    "n=3\n",
    "m=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "S=np.zeros((batch,delay,n,m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_2D_input_spike(S,spike,delay_tensor,depth):\n",
    "    \"\"\"input - \n",
    "    S : a 4D tensor with batch x delay x n x m ,\n",
    "    spike: a 2D tensor batch x n, \n",
    "    delay_tensor : a 2D tensor n x m\n",
    "    depth : scalar representing maximum delay \n",
    "    \n",
    "    outputs a tensor with size batch x output_size, where outputsize is twice the size of thresholds_size\n",
    "    \"\"\"\n",
    "    # roll S and get new input,\n",
    "    S_update=tf.roll(S,shift=-1,axis=1)\n",
    "    new_input=S_update[:,0,:]\n",
    "    synaptic_shape=tf.shape(S)\n",
    "    # add new spikes to S\n",
    "    correction_mat=tf.expand_dims(tf.concat([tf.ones([synaptic_shape[1]-1,synaptic_shape[2],synaptic_shape[3]],tf.float32),tf.zeros([1,synaptic_shape[2],synaptic_shape[3]],dtype=tf.float32)],axis=0),axis=0)\n",
    "\n",
    "    S_cut=tf.multiply(S,correction_mat)\n",
    "    # add spikes to S_cut\n",
    "    spike_temp=tf.expand_dims(tf.expand_dims(tf.cast(spike,tf.float32),axis=-2),axis=-2)\n",
    "    delay_tensor_temp=tf.one_hot(tf.cast(tf.multiply(delay_tensor,spike),tf.int32),depth=depth,axis=1)\n",
    "    spike_tensor=tf.multiply(spike_temp,delay_tensor_temp)\n",
    "    # update S and return output\n",
    "    S_out=tf.add(S_cut,spike_tensor)\n",
    "    return S_out, new_input\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "synaptic_shape=S.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1464, shape=(2,), dtype=int32, numpy=array([2, 2], dtype=int32)>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=tf.constant((np.array([[1,1],[1,0]])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [3, 2],\n",
       "       [1, 4]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delay=np.array([[1,0],[3,2],[1,4]])\n",
    "delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1594, shape=(2, 3, 2), dtype=int64, numpy=\n",
       "array([[[1, 0],\n",
       "        [3, 2],\n",
       "        [1, 4]],\n",
       "\n",
       "       [[1, 0],\n",
       "        [3, 0],\n",
       "        [1, 0]]])>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delay_mat=tf.einsum('uv,nv->nuv',tf.constant(delay),x)\n",
    "delay_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1622, shape=(2, 4, 3, 2), dtype=float32, numpy=\n",
       "array([[[[0., 1.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [0., 0.],\n",
       "         [1., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 1.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [1., 0.],\n",
       "         [0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [0., 0.],\n",
       "         [1., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [1., 0.],\n",
       "         [0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.one_hot(delay_mat,depth=4,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=948, shape=(1, 1, 1, 2), dtype=float32, numpy=array([[[[1., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_temp=tf.expand_dims(tf.expand_dims(tf.cast(x,tf.float32),axis=-2),axis=-2)\n",
    "x_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=974, shape=(1, 4, 3, 2), dtype=float32, numpy=\n",
       "array([[[[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [0., 0.],\n",
       "         [1., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [1., 0.],\n",
       "         [0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_new=tf.multiply(x_temp,tf.one_hot(delay_mat,depth=4,axis=1))\n",
    "S_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=998, shape=(1, 4, 3, 2), dtype=float32, numpy=\n",
       "array([[[[1., 0.],\n",
       "         [0., 0.],\n",
       "         [1., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [1., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_update=tf.roll(S_new,shift=-1,axis=1)\n",
    "S_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1023, shape=(1, 3, 2), dtype=float32, numpy=\n",
       "array([[[1., 0.],\n",
       "        [0., 0.],\n",
       "        [1., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_input=S_update[:,0,:]\n",
    "new_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1045, shape=(4,), dtype=int32, numpy=array([1, 4, 3, 2], dtype=int32)>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synaptic_shape=tf.shape(S_new)\n",
    "synaptic_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=732, shape=(1, 4, 3, 2), dtype=float32, numpy=\n",
       "array([[[[1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correction_mat=tf.expand_dims(tf.concat([tf.ones([synaptic_shape[1]-1,synaptic_shape[2],synaptic_shape[3]],tf.float32),tf.zeros([1,synaptic_shape[2],synaptic_shape[3]],dtype=tf.float32)],axis=0),axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=787, shape=(1, 4, 3, 2), dtype=float32, numpy=\n",
       "array([[[[1., 0.],\n",
       "         [0., 0.],\n",
       "         [1., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [1., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_cut=tf.multiply(S_update,correction_mat)\n",
    "S_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.util import nest\n",
    "from tensorflow.python.ops import variable_scope as vs\n",
    "_TENSOR_VARIABLE_NAME='kernel_2D'\n",
    "def _tensor_linear(args,\n",
    "            output_size,\n",
    "            bias=False,\n",
    "            bias_initializer=None,\n",
    "            kernel_initializer=None):\n",
    "  \"\"\"Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\n",
    "  Args:\n",
    "    args: a 2D Tensor or a list of 2D, batch, n, Tensors.\n",
    "    output_size: int, second dimension of W[i].\n",
    "    bias: boolean, whether to add a bias term or not.\n",
    "    bias_initializer: starting value to initialize the bias\n",
    "      (default is all zeros).\n",
    "    kernel_initializer: starting value to initialize the weight.\n",
    "  Returns:\n",
    "    A 2D Tensor with shape `[batch, output_size]` equal to\n",
    "    sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n",
    "  Raises:\n",
    "    ValueError: if some of the arguments has unspecified or wrong shape.\n",
    "  \"\"\"\n",
    "  if args is None or (nest.is_sequence(args) and not args):\n",
    "    raise ValueError(\"`args` must be specified\")\n",
    "  if not nest.is_sequence(args):\n",
    "    args = [args]\n",
    "\n",
    "  # Calculate the total size of arguments on dimension 1.\n",
    "  total_arg_size = 0\n",
    "  shapes = [a.get_shape() for a in args]\n",
    "  for shape in shapes:\n",
    "    if shape.ndims != 3:\n",
    "      raise ValueError(\"tensor linear is expecting 3D arguments: %s\" % shapes)\n",
    "    if shape.dims[-1].value is None:\n",
    "      raise ValueError(\"linear expects shape[2] to be provided for shape %s, \"\n",
    "                       \"but saw %s\" % (shape, shape[-1]))\n",
    "    else:\n",
    "      total_arg_size += shape.dims[-1].value\n",
    "\n",
    "  dtype = [a.dtype for a in args][0]\n",
    "\n",
    "  # Now the computation.\n",
    "  scope = vs.get_variable_scope()\n",
    "  with vs.variable_scope(scope) as outer_scope:\n",
    "    weights = vs.get_variable(\n",
    "        _TENSOR_VARIABLE_NAME, [output_size, total_arg_size],\n",
    "        dtype=dtype,\n",
    "        initializer=kernel_initializer)\n",
    "    if len(args) == 1:\n",
    "      res = tf.reduce_sum(tf.multiply(weights,args[0]),axis=-1)\n",
    "    else:\n",
    "      res = tf.reduce_sum(tf.multiply(weights,array_ops.concat(args, -1)),axis=-1)\n",
    "    if not bias:\n",
    "      return res\n",
    "    with vs.variable_scope(outer_scope) as inner_scope:\n",
    "      inner_scope.set_partitioner(None)\n",
    "      if bias_initializer is None:\n",
    "        bias_initializer = init_ops.constant_initializer(0.0, dtype=dtype)\n",
    "      biases = vs.get_variable(\n",
    "          _BIAS_VARIABLE_NAME, [output_size],\n",
    "          dtype=dtype,\n",
    "          initializer=bias_initializer)\n",
    "    return nn_ops.bias_add(res, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=744, shape=(1, 3), dtype=float32, numpy=array([[0.41732073, 0.        , 0.73945963]], dtype=float32)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_tensor_linear(new_input,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=81, shape=(1, 3, 2), dtype=float32, numpy=\n",
       "array([[[1., 0.],\n",
       "        [0., 0.],\n",
       "        [1., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kernel_2D' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-45e08cee1eaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkernel_2D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'kernel_2D' is not defined"
     ]
    }
   ],
   "source": [
    "kernel_2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    " weights = vs.get_variable(\n",
    "        _TENSOR_VARIABLE_NAME, [3, 2],\n",
    "        dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'kernel_2D:0' shape=(3, 2) dtype=float32, numpy=\n",
       "array([[ 0.39672112,  0.76382565],\n",
       "       [-0.79841745,  0.8517103 ],\n",
       "       [-0.8448924 ,  1.011239  ]], dtype=float32)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=528, shape=(1, 3, 2), dtype=float32, numpy=\n",
       "array([[[-0.8342153 , -0.        ],\n",
       "        [ 0.        , -0.        ],\n",
       "        [-0.12256187, -0.        ]]], dtype=float32)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.multiply(weights,new_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
