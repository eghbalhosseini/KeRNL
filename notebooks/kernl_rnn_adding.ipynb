{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python libraries\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import collections\n",
    "import hashlib\n",
    "import numbers\n",
    "import matplotlib.cm as cm\n",
    "from sys import getsizeof\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "from IPython.display import HTML\n",
    "import re\n",
    "\n",
    "# tensorflow and its dependencies \n",
    "import tensorflow as tf\n",
    "from tensorflow.python.eager import context\n",
    "from tensorflow.python.framework import constant_op\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.framework import tensor_util\n",
    "from tensorflow.python.layers import base as base_layer\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import clip_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.ops import partitioned_variables\n",
    "from tensorflow.python.ops import random_ops\n",
    "from tensorflow.python.ops import tensor_array_ops\n",
    "from tensorflow.python.ops import variable_scope as vs\n",
    "from tensorflow.python.ops import variables as tf_variables\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "from tensorflow.python.util import nest\n",
    "from tensorflow.contrib.rnn.python.ops.core_rnn_cell import _Linear\n",
    "from tensorflow.contrib import slim\n",
    "\n",
    "## user defined modules \n",
    "# kernel rnn cell \n",
    "import kernl_rnn_cell\n",
    "import adding_problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/eghbal/MyData/KeRNL/logs/kernl_rnn_addition/add_tanh_eta_tensor_1e-05_eta_weight_1e-03_batch_2e+01_hum_hidd_1e+02_gc_1e+02_steps_5e+04_run_20190227_1630'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Training Parameters\n",
    "tensor_learning_rate = 1e-5\n",
    "weight_learning_rate = 1e-3\n",
    "training_steps = 50000\n",
    "batch_size = 25\n",
    "test_size=10000\n",
    "display_step = 200\n",
    "grad_clip=100\n",
    "# Network Parameters\n",
    "num_input = 2 # adding problem data input (first input are the random digits , second input is the mask)\n",
    "time_steps = 100\n",
    "num_hidden = 100 # hidden layer num of features\n",
    "num_output = 1 # value of the addition estimation \n",
    "# \n",
    "\n",
    "# Noise Parameters\n",
    "perturbation_std=1e-4\n",
    "\n",
    "\n",
    "\n",
    "# Noise Parameters\n",
    "perturbation_std=1e-2\n",
    "log_dir = os.environ['HOME']+\"/MyData/KeRNL/logs/kernl_rnn_addition/add_tanh_eta_tensor_%1.0e_eta_weight_%1.0e_batch_%1.0e_hum_hidd_%1.0e_gc_%1.0e_steps_%1.0e_run_%s\" %(tensor_learning_rate,weight_learning_rate,batch_size,num_hidden,grad_clip,training_steps, datetime.now().strftime(\"%Y%m%d_%H%M\"))\n",
    "log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define KeRNL unit\n",
    "def kernl_rnn(x,kernel_weights,kernel_bias):\n",
    "    # Define a KeRNL cell, the initialization is done inside the cell with default initializers\n",
    "    with tf.variable_scope('kernl') as scope: \n",
    "        kernl_rnn_unit = kernl_rnn_cell.kernl_rnn_cell(num_units=num_hidden,\n",
    "                                                      num_inputs=num_input,\n",
    "                                                      time_steps=time_steps,\n",
    "                                                      noise_param=perturbation_std,\n",
    "                                                      sensitivity_initializer=tf.initializers.identity\n",
    "                                                      ,activation='tanh')\n",
    "        # Get KeRNL cell output\n",
    "        kernel_outputs, kernel_states = tf.nn.dynamic_rnn(kernl_rnn_unit, inputs=x, dtype=tf.float32,time_major=False)\n",
    "        kernl_rnn_output=tf.matmul(kernel_outputs[:,-1,:], kernel_weights) + kernel_bias     \n",
    "    return kernl_rnn_output, kernel_states  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "graph=tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.variable_scope('kernl_output',initializer=tf.contrib.layers.xavier_initializer()) as scope:\n",
    "        kernl_weights = tf.get_variable(shape=[num_hidden, num_output],name='output_weight')\n",
    "        kernl_biases = tf.get_variable(shape=[num_output],name='output_addition')\n",
    "\n",
    "    # define weights and inputs to the network\n",
    "    X = tf.placeholder(\"float\", [None, time_steps, num_input])\n",
    "    Y = tf.placeholder(\"float\", [None, num_output])\n",
    "    # define a function for extraction of variable names\n",
    "    kernl_output,kernl_states=kernl_rnn(X,kernl_weights,kernl_biases)\n",
    "    trainables=tf.trainable_variables()\n",
    "    variable_names=[v.name for v in tf.trainable_variables()]\n",
    "    # \n",
    "    find_joing_index = lambda x, name_1,name_2 : [a and b for a,b in zip([np.unicode_.find(k.name, name_1)>-1 for k in x] ,[np.unicode_.find(k.name, name_2)>-1 for k in x])].index(True)\n",
    "    # find trainable parameters for kernl \n",
    "    with tf.name_scope('kernl_Trainables') as scope:\n",
    "        kernl_output_weight_index= find_joing_index(trainables,'kernl','output_weight')\n",
    "        kernl_output_addition_index= find_joing_index(trainables,'kernl','output_addition')\n",
    "        kernl_temporal_filter_index= find_joing_index(trainables,'kernl','temporal_filter')\n",
    "        kernl_sensitivity_tensor_index= find_joing_index(trainables,'kernl','sensitivity_tensor')\n",
    "        kernl_kernel_index= find_joing_index(trainables,'kernl','kernel')\n",
    "        kernl_bias_index= find_joing_index(trainables,'kernl','bias')\n",
    "    # \n",
    "        kernl_tensor_training_indices=np.asarray([kernl_sensitivity_tensor_index,kernl_temporal_filter_index],dtype=np.int)\n",
    "        kernl_tensor_trainables= [trainables[k] for k in kernl_tensor_training_indices]\n",
    "    #\n",
    "        kernl_weight_training_indices=np.asarray([kernl_kernel_index,kernl_bias_index,kernl_output_weight_index,kernl_output_addition_index],dtype=np.int)\n",
    "        kernl_weight_trainables= [trainables[k] for k in kernl_weight_training_indices]\n",
    "     \n",
    "    # define loss functions  \n",
    "    ##################\n",
    "    # kernl train ####\n",
    "    ##################\n",
    "    with tf.name_scope(\"kernl_train\") as scope:\n",
    "        # outputs \n",
    "        kernl_loss_output_prediction=tf.losses.mean_squared_error(Y,kernl_output)\n",
    "        \n",
    "        # states \n",
    "        kernl_loss_state_prediction=tf.losses.mean_squared_error(tf.subtract(kernl_states.h_hat, kernl_states.h),tf.matmul(kernl_states.Theta,trainables[kernl_sensitivity_tensor_index]))\n",
    "        # define optimizers \n",
    "        kernl_weight_optimizer = tf.train.RMSPropOptimizer(learning_rate=weight_learning_rate)\n",
    "        kernl_tensor_optimizer = tf.train.RMSPropOptimizer(learning_rate=tensor_learning_rate)\n",
    "        \n",
    "        with tf.name_scope('kernl_train_tensors') as scope:\n",
    "            \n",
    "            kernl_delta_sensitivity=tf.subtract(tf.matmul(kernl_states.Theta,tf.transpose(trainables[kernl_sensitivity_tensor_index])),tf.subtract(kernl_states.h_hat,kernl_states.h))\n",
    "            kernl_sensitivity_tensor_update= tf.reduce_mean(tf.einsum(\"un,uv->unv\",kernl_delta_sensitivity,kernl_states.Theta),axis=0)\n",
    "            kernl_temporal_filter_update= tf.reduce_mean(tf.multiply(tf.matmul(kernl_delta_sensitivity,trainables[kernl_sensitivity_tensor_index]),kernl_states.Gamma),axis=0)\n",
    "            kernl_tensor_grads_and_vars=list(zip([kernl_sensitivity_tensor_update,kernl_temporal_filter_update],kernl_tensor_trainables))\n",
    "            kernl_cropped_tensor_grads_and_vars=[(tf.clip_by_norm(grad, grad_clip),var) if  np.unicode_.find(var.name,'output')==-1 else (grad,var) for grad,var in kernl_tensor_grads_and_vars]\n",
    "            kernl_tensor_train_op = kernl_tensor_optimizer.apply_gradients(kernl_cropped_tensor_grads_and_vars)\n",
    "        \n",
    "        with tf.name_scope('kernl_train_weights') as scope: \n",
    "            \n",
    "            kernl_grad_cost_to_output=tf.gradients(kernl_loss_output_prediction,kernl_output, name= 'kernl_grad_cost_to_y')\n",
    "            kernl_error_in_hidden_state=tf.matmul(kernl_grad_cost_to_output[-1],tf.transpose(trainables[kernl_output_weight_index]))\n",
    "            kernl_delta_weight=tf.matmul(kernl_error_in_hidden_state,trainables[kernl_sensitivity_tensor_index]) \n",
    "            kernl_weight_update=tf.transpose(tf.reduce_mean(tf.einsum(\"un,unv->unv\",kernl_delta_weight,kernl_states.eligibility_trace),axis=0))\n",
    "            kernl_bias_update=tf.reduce_mean(tf.multiply(kernl_delta_weight,kernl_states.bias_trace),axis=0)\n",
    "            # output layer \n",
    "            kernl_grad_cost_to_output_layer=tf.gradients(kernl_loss_output_prediction,[trainables[kernl_output_weight_index],trainables[kernl_output_addition_index]], name= 'kernl_grad_cost_to_output_layer')\n",
    "            # crop the gradients  \n",
    "            kernl_weight_grads_and_vars=list(zip([kernl_weight_update,kernl_bias_update,kernl_grad_cost_to_output_layer[0],kernl_grad_cost_to_output_layer[1]],kernl_weight_trainables))\n",
    "            #kernl_cropped_weight_grads_and_vars=[(tf.clip_by_norm(grad, grad_clip),var) if  np.unicode_.find(var.name,'output')==-1 else (grad,var) for grad,var in kernl_weight_grads_and_vars]\n",
    "            kernl_cropped_weight_grads_and_vars=[(tf.clip_by_norm(grad, grad_clip),var) for grad,var in kernl_weight_grads_and_vars]\n",
    "            # apply gradients \n",
    "            kernl_weight_train_op = kernl_weight_optimizer.apply_gradients(kernl_cropped_weight_grads_and_vars)\n",
    "    with tf.name_scope(\"kernl_evaluate\") as scope: \n",
    "        kernl_loss_cross_validiation=tf.losses.mean_squared_error(Y,kernl_output)\n",
    "        \n",
    "    ##################\n",
    "    # SUMMARIES ######\n",
    "    ##################\n",
    "    with tf.name_scope('cross_validation_summary') as scope: \n",
    "        tf.summary.scalar('cross_validation_summary',kernl_loss_cross_validiation+1e-10)\n",
    "        kernl_evaluate_summary_op=tf.summary.merge_all(scope=\"cross_validation_summary\")       \n",
    "\n",
    "    with tf.name_scope(\"kernl_tensor_summaries\") as scope: \n",
    "        # kernl sensitivity tensor \n",
    "        tf.summary.histogram('kernl_sensitivity_tensor_grad',kernl_sensitivity_tensor_update+1e-10)\n",
    "        tf.summary.histogram('kernl_sensitivity_tensor',trainables[kernl_sensitivity_tensor_index]+1e-10)\n",
    "        # kernl temporal filter \n",
    "        tf.summary.histogram('kernl_temporal_filter_grad',kernl_temporal_filter_update+1e-10)\n",
    "        tf.summary.histogram('kernl_temporal_filter',trainables[kernl_temporal_filter_index]+1e-10)\n",
    "        # kernl loss \n",
    "        tf.summary.scalar('kernl_loss_state_prediction',kernl_loss_state_prediction+1e-10)\n",
    "        # kernl senstivity tensor and temporal filter \n",
    "        tf.summary.image('kernl_sensitivity_tensor',tf.expand_dims(tf.expand_dims(trainables[kernl_sensitivity_tensor_index],axis=0),axis=-1))\n",
    "        tf.summary.image('kernl_sensitivity_tensor_grad',tf.expand_dims(tf.expand_dims(kernl_sensitivity_tensor_update,axis=0),axis=-1))\n",
    "        tf.summary.image('kernl_temporal_filter',tf.expand_dims(tf.expand_dims(tf.expand_dims(trainables[kernl_temporal_filter_index],axis=0),axis=-1),axis=-1))\n",
    "        tf.summary.image('kernl_temporal_filter_grad',tf.expand_dims(tf.expand_dims(tf.expand_dims(kernl_temporal_filter_update,axis=0),axis=-1),axis=-1))\n",
    "        kernl_tensor_merged_summary_op=tf.summary.merge_all(scope=\"kernl_tensor_summaries\")\n",
    "        \n",
    "    with tf.name_scope(\"kernl_weight_summaries\") as scope: \n",
    "        # kernl kernel\n",
    "        tf.summary.histogram('kernl_kernel_grad',kernl_weight_update+1e-10)\n",
    "        tf.summary.histogram('kernl_kernel',trainables[kernl_kernel_index]+1e-10)\n",
    "        # kernl bias \n",
    "        tf.summary.histogram('kernl_bias_grad',kernl_bias_update+1e-10)\n",
    "        tf.summary.histogram('kernl_bias',trainables[kernl_bias_index]+1e-10)\n",
    "        # kernl output weight\n",
    "        tf.summary.histogram('kernl_output_weight_grad',kernl_grad_cost_to_output_layer[0]+1e-10)\n",
    "        tf.summary.histogram('kernl_output_weights', trainables[kernl_output_weight_index]+1e-10)\n",
    "        # kernl output bias\n",
    "        tf.summary.histogram('kernl_output_addition_grad',kernl_grad_cost_to_output_layer[1]+1e-10)\n",
    "        tf.summary.histogram('kernl_output_addition', trainables[kernl_output_addition_index]+1e-10)\n",
    "        # kernl loss \n",
    "        tf.summary.scalar('kernl_loss_output_prediction',kernl_loss_output_prediction+1e-10)\n",
    "        # kernl kernel and output weight \n",
    "        tf.summary.image('kernl_kernel',tf.expand_dims(tf.expand_dims(trainables[kernl_kernel_index],axis=0),axis=-1))\n",
    "        tf.summary.image('kernl_kernel_grad',tf.expand_dims(tf.expand_dims(kernl_weight_update,axis=0),axis=-1))\n",
    "        tf.summary.image('kernl_output_weight',tf.expand_dims(tf.expand_dims(trainables[kernl_output_weight_index],axis=0),axis=-1))\n",
    "        tf.summary.image('kernl_output_weight_grad',tf.expand_dims(tf.expand_dims(kernl_grad_cost_to_output_layer[0],axis=0),axis=-1))\n",
    "        kernl_weight_merged_summary_op=tf.summary.merge_all(scope=\"kernl_weight_summaries\")\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['variable: ', 'kernl_output/output_weight:0']\n",
      "['variable: ', 6]\n",
      "['shape: ', (100, 1)]\n",
      "['variable: ', 'kernl_output/output_addition:0']\n",
      "['variable: ', 6]\n",
      "['shape: ', (1,)]\n",
      "['variable: ', 'kernl/rnn/kernl_rnn_cell/temporal_filter:0']\n",
      "['variable: ', -1]\n",
      "['shape: ', (100,)]\n",
      "['variable: ', 'kernl/rnn/kernl_rnn_cell/sensitivity_tensor:0']\n",
      "['variable: ', -1]\n",
      "['shape: ', (100, 100)]\n",
      "['variable: ', 'kernl/rnn/kernl_rnn_cell/kernel:0']\n",
      "['variable: ', -1]\n",
      "['shape: ', (102, 100)]\n",
      "['variable: ', 'kernl/rnn/kernl_rnn_cell/bias:0']\n",
      "['variable: ', -1]\n",
      "['shape: ', (100,)]\n"
     ]
    }
   ],
   "source": [
    "# verify initialization\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "with tf.Session(graph=graph,) as sess : \n",
    "    sess.run(init)\n",
    "    values,trainable_vars = sess.run([variable_names,trainables])\n",
    "    for k, v in zip(variable_names,values):\n",
    "        print([\"variable: \" , k])\n",
    "        #print([\"value: \" , v])\n",
    "        print([\"variable: \" , np.unicode_.find(k,'output')]) \n",
    "        print([\"shape: \" , v.shape])\n",
    "        #print(v) training_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(log_dir).mkdir(exist_ok=True, parents=True)\n",
    "filelist = [ f for f in os.listdir(log_dir) if f.endswith(\".local\") ]\n",
    "for f in filelist:\n",
    "    os.remove(os.path.join(log_dir, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2,kernl train Loss: 3.504, state loss 0.008, cross validation loss :3.059\n",
      "Step: 201,kernl train Loss: 0.193, state loss 0.006, cross validation loss :0.203\n",
      "Step: 401,kernl train Loss: 0.178, state loss 0.006, cross validation loss :0.191\n",
      "Step: 601,kernl train Loss: 0.146, state loss 0.005, cross validation loss :0.188\n",
      "Step: 801,kernl train Loss: 0.180, state loss 0.004, cross validation loss :0.191\n",
      "Step: 1001,kernl train Loss: 0.208, state loss 0.004, cross validation loss :0.238\n",
      "Step: 1201,kernl train Loss: 0.183, state loss 0.003, cross validation loss :0.178\n",
      "Step: 1401,kernl train Loss: 0.397, state loss 0.005, cross validation loss :0.223\n",
      "Step: 1601,kernl train Loss: 0.156, state loss 0.003, cross validation loss :0.172\n",
      "Step: 1801,kernl train Loss: 0.157, state loss 0.002, cross validation loss :0.177\n",
      "Step: 2001,kernl train Loss: 0.132, state loss 0.002, cross validation loss :0.166\n",
      "Step: 2201,kernl train Loss: 0.181, state loss 0.002, cross validation loss :0.165\n",
      "Step: 2401,kernl train Loss: 0.160, state loss 0.002, cross validation loss :0.168\n",
      "Step: 2601,kernl train Loss: 0.148, state loss 0.002, cross validation loss :0.170\n",
      "Step: 2801,kernl train Loss: 0.162, state loss 0.030, cross validation loss :0.173\n",
      "Step: 3001,kernl train Loss: 0.151, state loss 0.009, cross validation loss :0.168\n",
      "Step: 3201,kernl train Loss: 0.136, state loss 0.027, cross validation loss :0.167\n",
      "Step: 3401,kernl train Loss: 0.139, state loss 0.015, cross validation loss :0.168\n",
      "Step: 3601,kernl train Loss: 0.097, state loss 0.025, cross validation loss :0.184\n",
      "Step: 3801,kernl train Loss: 0.189, state loss 0.063, cross validation loss :0.176\n",
      "Step: 4001,kernl train Loss: 0.165, state loss 0.018, cross validation loss :0.168\n",
      "Step: 4201,kernl train Loss: 0.196, state loss 0.009, cross validation loss :0.169\n",
      "Step: 4401,kernl train Loss: 0.102, state loss 0.011, cross validation loss :0.173\n",
      "Step: 4601,kernl train Loss: 0.103, state loss 0.010, cross validation loss :0.167\n",
      "Step: 4801,kernl train Loss: 0.163, state loss 0.027, cross validation loss :0.172\n",
      "Step: 5001,kernl train Loss: 0.164, state loss 0.025, cross validation loss :0.165\n",
      "Step: 5201,kernl train Loss: 0.213, state loss 0.008, cross validation loss :0.169\n",
      "Step: 5401,kernl train Loss: 0.130, state loss 0.005, cross validation loss :0.169\n",
      "Step: 5601,kernl train Loss: 0.183, state loss 0.014, cross validation loss :0.171\n",
      "Step: 5801,kernl train Loss: 0.118, state loss 0.009, cross validation loss :0.183\n",
      "Step: 6001,kernl train Loss: 0.181, state loss 0.007, cross validation loss :0.172\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-4ceeff62988c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# run summaries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mkernl_tensor_merged_summary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernl_tensor_merged_summary_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mkernl_weight_merged_summary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernl_weight_merged_summary_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mtb_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernl_tensor_merged_summary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# write graph into tensorboard \n",
    "tb_writer = tf.summary.FileWriter(log_dir,graph)\n",
    "# run a training session \n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(init)\n",
    "    for step in range(1,training_steps+1):\n",
    "        batch_x, batch_y = adding_problem.get_batch(batch_size=batch_size,time_steps=time_steps)\n",
    "\n",
    "        # kernl state  train \n",
    "        kernl_state_train, kernl_tensor_loss=sess.run([kernl_tensor_train_op,kernl_loss_state_prediction],feed_dict={X:batch_x, Y:batch_y})\n",
    "        # kernl weight  train \n",
    "        kernl_weight_train, kernl_loss=sess.run([kernl_weight_train_op,kernl_loss_output_prediction],feed_dict={X:batch_x, Y:batch_y})\n",
    "\n",
    "        # run summaries \n",
    "        kernl_tensor_merged_summary=sess.run(kernl_tensor_merged_summary_op,feed_dict={X:batch_x, Y:batch_y})\n",
    "        kernl_weight_merged_summary=sess.run(kernl_weight_merged_summary_op,feed_dict={X:batch_x, Y:batch_y})\n",
    "        \n",
    "        tb_writer.add_summary(kernl_tensor_merged_summary, global_step=step)\n",
    "        tb_writer.add_summary(kernl_weight_merged_summary, global_step=step)\n",
    "        # \n",
    "\n",
    "        # \n",
    "        if step % display_step==0 or step==1 :\n",
    "            test_batch_x, test_batch_y = adding_problem.get_batch(batch_size=test_size,time_steps=time_steps)\n",
    "            kernl_test_loss, kernl_evaluate_summary=sess.run([kernl_loss_cross_validiation,kernl_evaluate_summary_op],feed_dict={X:test_batch_x, Y:test_batch_y})\n",
    "            # get batch loss and accuracy \n",
    "            tb_writer.add_summary(kernl_evaluate_summary, global_step=step)\n",
    "            print('Step: {},kernl train Loss: {:.3f}, state loss {:.3f}, cross validation loss :{:.3f}'.format(step + 1, kernl_loss,kernl_tensor_loss,kernl_test_loss))\n",
    "\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    save_path = saver.save(sess, log_dir+\"/model.ckpt\", global_step=step,write_meta_graph=True)\n",
    "    print(\"Model saved in path: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
