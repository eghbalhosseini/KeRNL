{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "#import matplotlib.pyplot as plt \n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import collections\n",
    "import hashlib\n",
    "import numbers\n",
    "#import matplotlib.cm as cm\n",
    "#from mpl_toolkits.mplot3d import axes3d\n",
    "#%matplotlib inline\n",
    "import os\n",
    "\n",
    "from tensorflow.python.eager import context\n",
    "from tensorflow.python.framework import constant_op\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.framework import tensor_util\n",
    "from tensorflow.python.layers import base as base_layer\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import clip_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.ops import partitioned_variables\n",
    "from tensorflow.python.ops import random_ops\n",
    "from tensorflow.python.ops import tensor_array_ops\n",
    "from tensorflow.python.ops import variable_scope as vs\n",
    "from tensorflow.python.ops import variables as tf_variables\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "from tensorflow.python.util import nest\n",
    "from tensorflow.contrib.rnn.python.ops.core_rnn_cell import _Linear\n",
    "#from matplotlib.colors import LinearSegmentedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# uplading mnist data \n",
    "\n",
    "old_v = tf.logging.get_verbosity()\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "train_data = mnist.train.images  # Returns np.array\n",
    "train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "eval_data = mnist.test.images  # Returns np.array\n",
    "eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)\n",
    "\n",
    "tf.logging.set_verbosity(old_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 1e-5\n",
    "training_steps = 5000\n",
    "batch_size = 128\n",
    "display_step = 200\n",
    "test_len=128\n",
    "grad_clip=200\n",
    "# Network Parameters\n",
    "num_input = 1 # MNIST data input (img shape: 28*28)\n",
    "timesteps = 28*28 # timesteps\n",
    "num_hidden = 128 # hidden layer num of features\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "    \n",
    "    # using variable scope to initialize to identity \n",
    "    with tf.variable_scope('recurrent',initializer=tf.initializers.identity()) as scope: \n",
    "        # Define a lstm cell with tensorflow\n",
    "        lstm_cell = tf.contrib.rnn.BasicRNNCell(num_hidden,name='irnn')\n",
    "        # Get lstm cell output\n",
    "        outputs, states = tf.nn.dynamic_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[:,-1,:], weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kernel_RNN_v2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-af7af8090268>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# define network output and trainiables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'keRNL_train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel_RNN_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mvariable_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mtrainables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'kernel_RNN_v2' is not defined"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "graph=tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # define weights and inputs to the network\n",
    "    with tf.variable_scope('KeRNL_output_layer', initializer=tf.contrib.layers.xavier_initializer()) as scope:\n",
    "        weights = {'out': tf.get_variable(shape=[num_hidden, num_classes],name='output_weight')}\n",
    "        biases = {'out': tf.get_variable(shape=[num_classes],name='output_addition')}\n",
    "\n",
    "    with tf.variable_scope('IRNN_output_layer', initializer=tf.contrib.layers.xavier_initializer()) as scope:\n",
    "        weights = {'out': tf.get_variable(shape=[num_hidden, num_classes],name='output_weight')}\n",
    "        biases = {'out': tf.get_variable(shape=[num_classes],name='output_addition')}\n",
    "\n",
    "    X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "    Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "    # define a function for extraction of variable names\n",
    "    find_index = lambda x, name : [np.unicode_.find(k.name, name)>-1 for k in x].index(True)\n",
    "    # define network output and trainiables\n",
    "    with tf.name_scope('keRNL_train'):\n",
    "        logits,states = kernel_RNN_v2(X, weights, biases)\n",
    "        variable_names=[v.name for v in tf.trainable_variables()]\n",
    "        trainables=tf.trainable_variables()\n",
    "\n",
    "    # get the index of trainable variables\n",
    "        temporal_filter_index= find_index(trainables,'temporal_filter')\n",
    "        sensitivity_tensor_index= find_index(trainables,'sensitivity_tensor')\n",
    "        kernel_index=find_index(trainables,'kernel')\n",
    "        bias_index=find_index(trainables,'bias')\n",
    "        output_weight_index=find_index(trainables,'output_weight')\n",
    "        output_addition_index=find_index(trainables,'output_addition')\n",
    "\n",
    "        # trainables for tensors\n",
    "        tensor_training_indices=np.asarray([sensitivity_tensor_index,\n",
    "                                        temporal_filter_index],dtype=np.int)\n",
    "        tensor_trainables= [trainables[k] for k in tensor_training_indices]\n",
    "\n",
    "        # trainables for weights\n",
    "        weight_training_indices=np.asarray([kernel_index,\n",
    "                                        bias_index,\n",
    "                                        output_weight_index,\n",
    "                                        output_addition_index],dtype=np.int)\n",
    "        weight_trainables= [trainables[k] for k in weight_training_indices]\n",
    "\n",
    "    ## compute lossses\n",
    "    # compute loss for predictions.\n",
    "        loss_output_prediction=tf.losses.softmax_cross_entropy(onehot_labels=Y,logits=logits)\n",
    "        #loss_output_prediction = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        #logits=logits, labels=Y))\n",
    "        prediction = tf.nn.softmax(logits)\n",
    "\n",
    "        # compute loss for estimating sensitivity tensor and temporal_filter_coeff,\n",
    "        loss_state_prediction=tf.losses.mean_squared_error(tf.subtract(states.h_hat, states.h),\n",
    "                                                       tf.matmul(states.Gamma,trainables[sensitivity_tensor_index]))\n",
    "\n",
    "    ## define optimizers\n",
    "        # define optimizers learning the weights\n",
    "        weight_optimizer = tf.train.RMSPropOptimizer(learning_rate=weight_learning_rate)\n",
    "\n",
    "        # define optimizer for learning the sensitivity tensor and temporal filter\n",
    "        tensor_optimizer = tf.train.RMSPropOptimizer(learning_rate=tensor_learning_rate)\n",
    "\n",
    "## get gradients and apply them\n",
    "## optimize for temporal_filter and sensitivity_tensor\n",
    "    # manually calculate gradients\n",
    "        delta_sensitivity=tf.subtract(tf.matmul(states.Theta,\n",
    "                                            tf.transpose(trainables[sensitivity_tensor_index])),\n",
    "                                  tf.subtract(states.h_hat,states.h))\n",
    "        sensitivity_tensor_update= tf.reduce_mean(tf.einsum(\"un,uv->unv\",delta_sensitivity,states.Theta),axis=0)\n",
    "    #\n",
    "        temporal_filter_update= tf.reduce_mean(tf.multiply(tf.matmul(delta_sensitivity,\n",
    "                                                                 trainables[sensitivity_tensor_index]),\n",
    "                                                      states.Gamma),axis=0)\n",
    "        tensor_grads_and_vars=list(zip([sensitivity_tensor_update,temporal_filter_update],tensor_trainables))\n",
    "        cropped_tensor_grads_and_vars=[(tf.clip_by_norm(grad, grad_clip),var) if  np.unicode_.find(var.name,'output')==-1 else\n",
    "                            (grad,var) for grad,var in tensor_grads_and_vars]\n",
    "    # apply gradients\n",
    "        tensor_train_op = tensor_optimizer.apply_gradients(tensor_grads_and_vars)\n",
    "\n",
    "## optimize for recurrent weights and output weights\n",
    "        # 1- gradient for the recurrent weights\n",
    "        grad_cost_to_output=tf.gradients(loss_output_prediction,logits, name= 'grad_cost_to_y')\n",
    "        error_in_hidden_state=tf.matmul(grad_cost_to_output[-1],tf.transpose(trainables[output_weight_index])) # correct\n",
    "        # logging.warn(\"%s: error_in_hidden_state \", error_in_hidden_state.get_shape()) # correct\n",
    "        delta_weight=tf.matmul(error_in_hidden_state,trainables[sensitivity_tensor_index]) # correct\n",
    "        weight_update_test=tf.einsum(\"un,unv->unv\",delta_weight,states.eligibility_trace) # correct\n",
    "        logging.warn(\"%s: weight_update \", weight_update_test.get_shape()) # correct\n",
    "        weight_update=tf.transpose(tf.reduce_mean(weight_update_test,axis=0))\n",
    "        logging.warn(\"%s: weight_update \", weight_update.get_shape()) # correct\n",
    "        # 2- gradient for recurrent bias\n",
    "        bias_update_test=tf.multiply(delta_weight,states.bias_trace)\n",
    "        bias_update=tf.reduce_mean(bias_update_test,axis=0)\n",
    "        logging.warn(\"%s: bias_update \", bias_update.get_shape())\n",
    "        #3- gradient for output weight and bias\n",
    "        grad_cost_to_output_layer=tf.gradients(loss_output_prediction,[trainables[output_weight_index],trainables[output_addition_index]], name= 'grad_cost_to_output_layer')\n",
    "        # zip gradients and vars\n",
    "        weight_grads_and_vars=list(zip([weight_update,bias_update,grad_cost_to_output_layer[0],grad_cost_to_output_layer[1]],weight_trainables))\n",
    "        logging.warn(\"%s:     weight_grads_and_vars \",     weight_grads_and_vars)\n",
    "    # Apply gradient Clipping to recurrent weights\n",
    "        cropped_weight_grads_and_vars=[(tf.clip_by_norm(grad, grad_clip),var) if  np.unicode_.find(var.name,'output')==-1 else\n",
    "                            (grad,var) for grad,var in weight_grads_and_vars]\n",
    "    # apply gradients\n",
    "        weight_train_op = weight_optimizer.apply_gradients(cropped_weight_grads_and_vars)\n",
    "\n",
    "    # group training\n",
    "    #train_op=tf.group(tensor_train_op,weight_train_op)\n",
    "    ## Evaluate model (with test logits, for dropout to be disabled)\n",
    "        correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # Initialize the variables (i.e. assign their default value)\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "    ## get variables to save to tensorboard\n",
    "    # network output\n",
    "        tf.summary.histogram('prediction',prediction+1e-8)\n",
    "        tf.summary.histogram('logits',logits+1e-8)\n",
    "\n",
    "    # tensor training parameters\n",
    "        tf.summary.histogram('sensitivity_updates',sensitivity_tensor_update[-1]+1e-10)\n",
    "        tf.summary.histogram('temporal_filter_updates',temporal_filter_update[-1]+1e-10)\n",
    "        tf.summary.histogram('sensitivity_tensor',trainables[sensitivity_tensor_index]+1e-10)\n",
    "        tf.summary.histogram('temporal_filter',trainables[temporal_filter_index]+1e-10)\n",
    "        tf.summary.scalar('loss_state_prediction',loss_state_prediction)\n",
    "\n",
    "        # weight training parameters\n",
    "        tf.summary.histogram('weight_updates',weight_update+1e-10)\n",
    "        #tf.summary.histogram('output_weight_updates',grad_cost_to_output_layer[0])\n",
    "        #tf.summary.histogram('output_bias_updates',grad_cost_to_output_layer[1]+1e-10)\n",
    "        tf.summary.histogram('weights', trainables[kernel_index]+1e-10)\n",
    "        tf.summary.histogram('output_weights', trainables[output_weight_index]+1e-10)\n",
    "        tf.summary.histogram('output_addition', trainables[output_addition_index]+1e-10)\n",
    "        tf.summary.histogram('error_in_hidden_state', error_in_hidden_state+1e-10)\n",
    "        tf.summary.scalar('loss_output_prediction',loss_output_prediction)\n",
    "\n",
    "    #\n",
    "        tf.summary.image('kernel_matrix',tf.expand_dims(tf.expand_dims(trainables[kernel_index],axis=0),axis=-1))\n",
    "        tf.summary.image('sensitivity_matrix',tf.expand_dims(tf.expand_dims(trainables[sensitivity_tensor_index],axis=0),axis=-1))\n",
    "\n",
    "        # merge and save all\n",
    "        merged_summary_op=tf.summary.merge_all()\n",
    "\n",
    "        # save training\n",
    "        saver = tf.train.Saver()\n",
    "    with tf.name_scope('IRNN_train'):\n",
    "        logits,states = IRNN(X, weights, biases)\n",
    "        variable_names=[v.name for v in tf.trainable_variables()]\n",
    "        trainables=tf.trainable_variables()\n",
    "\n",
    "        # get the index of trainable variables\n",
    "        kernel_index=find_index(trainables,'kernel')\n",
    "        bias_index=find_index(trainables,'bias')\n",
    "        output_weight_index=find_index(trainables,'output_weight')\n",
    "        output_addition_index=find_index(trainables,'output_addition')\n",
    "\n",
    "        # trainables for weights\n",
    "        weight_training_indices=np.asarray([kernel_index,\n",
    "                                        bias_index,\n",
    "                                        output_weight_index,\n",
    "                                        output_addition_index],dtype=np.int)\n",
    "        weight_trainables= [trainables[k] for k in weight_training_indices]\n",
    "\n",
    "        ## compute lossses\n",
    "        # compute loss for predictions.\n",
    "        loss_output_prediction=tf.losses.softmax_cross_entropy(onehot_labels=Y,logits=logits)\n",
    "        #loss_output_prediction = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        #logits=logits, labels=Y))\n",
    "        prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    ## define optimizer\n",
    "        # define optimizers learning the weights\n",
    "        weight_optimizer = tf.train.RMSPropOptimizer(learning_rate=weight_learning_rate)\n",
    "## get gradients and apply them\n",
    "## optimize for temporal_filter and sensitivity_tensor\n",
    "    # manually calculate gradients\n",
    "## optimize for recurrent weights and output weights\n",
    "        # 1- gradient for the recurrent weights\n",
    "        grads=tf.gradients(loss_output_prediction,weight_trainables)\n",
    "        weight_grads_and_vars=list(zip(grads,weight_trainables))\n",
    "    # Apply gradient Clipping to recurrent weights\n",
    "        cropped_weight_grads_and_vars=[(tf.clip_by_norm(grad, grad_clip),var) if  np.unicode_.find(var.name,'output')==-1 else\n",
    "                            (grad,var) for grad,var in weight_grads_and_vars]\n",
    "    # apply gradients\n",
    "        weight_train_op = weight_optimizer.apply_gradients(cropped_weight_grads_and_vars)\n",
    "\n",
    "    # group training\n",
    "    #train_op=tf.group(tensor_train_op,weight_train_op)\n",
    "    ## Evaluate model (with test logits, for dropout to be disabled)\n",
    "        correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # Initialize the variables (i.e. assign their default value)\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "    ## get variables to save to tensorboard\n",
    "    # network output\n",
    "        tf.summary.histogram('prediction',prediction+1e-8)\n",
    "        tf.summary.histogram('logits',logits+1e-8)\n",
    "\n",
    "    # tensor training parameters\n",
    "        # weight training parameters\n",
    "        tf.summary.histogram('weight_updates',grads[kernel_index]+1e-10)\n",
    "        tf.summary.histogram('output_weight_updates',grads[output_weight_index])\n",
    "        tf.summary.histogram('output_bias_updates',grads[output_addition_index])\n",
    "        tf.summary.histogram('weights', trainables[kernel_index]+1e-10)\n",
    "        tf.summary.histogram('output_weights', trainables[output_weight_index]+1e-10)\n",
    "        tf.summary.histogram('output_addition', trainables[output_addition_index]+1e-10)\n",
    "        tf.summary.scalar('loss_output_prediction',loss_output_prediction)\n",
    "        tf.summary.image('kernel_matrix',tf.expand_dims(tf.expand_dims(trainables[kernel_index],axis=0),axis=-1))\n",
    "        # merge and save all\n",
    "        merged_summary_op=tf.summary.merge_all()\n",
    "        # save training\n",
    "        saver = tf.train.Saver()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/irnn/bptt_custom_grad_gc_%d_eta_%d_batch_%d_run_%s\" %(grad_clip,learning_rate,batch_size, datetime.now().strftime(\"%Y%m%d_%H%M\"))\n",
    "Path(log_dir).mkdir(exist_ok=True, parents=True)\n",
    "filelist = [ f for f in os.listdir(log_dir) if f.endswith(\".local\") ]\n",
    "for f in filelist:\n",
    "    os.remove(os.path.join(log_dir, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2, Train Loss: 2.914, Train Acc: 0.109\n",
      "Step: 201, Train Loss: 2.223, Train Acc: 0.125\n",
      "Step: 401, Train Loss: 2.105, Train Acc: 0.305\n",
      "Step: 601, Train Loss: 2.030, Train Acc: 0.258\n",
      "Step: 801, Train Loss: 1.848, Train Acc: 0.367\n",
      "Step: 1001, Train Loss: 1.796, Train Acc: 0.375\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-892d684e1e1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mbatch_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtimesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# run optimizaer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mloss_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_train\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mmerged_summary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrd_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmerged_summary_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew_grads_and_vars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/KeRNL/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/KeRNL/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/KeRNL/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/KeRNL/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/KeRNL/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/KeRNL/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# write graph into tensorboard \n",
    "tb_writer = tf.summary.FileWriter(log_dir,graph)\n",
    "# run a training session \n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(init)\n",
    "    for step in range(1,training_steps+1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        batch_x=batch_x.reshape((batch_size,timesteps,num_input))\n",
    "        # run optimizaer \n",
    "        sess.run(train_op,feed_dict={X:batch_x, Y:batch_y})\n",
    "        loss_train, acc_train= sess.run([loss_op, accuracy],feed_dict={X:batch_x, Y:batch_y})\n",
    "        merged_summary,grd_vars=sess.run([merged_summary_op,new_grads_and_vars],feed_dict={X:batch_x, Y:batch_y})\n",
    "        tb_writer.add_summary(merged_summary, global_step=step)\n",
    "        #tb_writer.flush()\n",
    "        # show interim performance \n",
    "        if step % display_step==0 or step==1 : \n",
    "            # get batch loss and accuracy \n",
    "            print('Step: {}, Train Loss: {:.3f}, Train Acc: {:.3f}'.format(\n",
    "            step + 1, loss_train, acc_train))\n",
    "            # write summary \n",
    "            #tb_writer.add_summary(acc,global_step=step)\n",
    "            #tb_writer.flush()\n",
    "            # evaluate performance on test data \n",
    "            test_X=mnist.test.images[:test_len].reshape((-1, timesteps, num_input))\n",
    "            test_Y=mnist.test.labels[:test_len]\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    test_data = mnist.test.images[:test_len].reshape((-1, timesteps, num_input))\n",
    "    test_label = mnist.test.labels[:test_len]\n",
    "    print(\"Testing Accuracy:\", \n",
    "        sess.run(accuracy, feed_dict={X: test_data, Y: test_label}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_learning_rate = 1e-8 # learning rate for weights in the network\n",
    "tensor_learning_rate = 1e-5 # learning rate for sensitivity tensor and temporal filter tensor\n",
    "training_steps = 20\n",
    "batch_size = 50\n",
    "display_step = 10\n",
    "test_len=128\n",
    "grad_clip=2\n",
    "# Network Parameters\n",
    "num_input = 1 # MNIST data input (img shape: 28*28)\n",
    "timesteps = 28*28 # timesteps\n",
    "num_hidden = 100 # hidden layer num of features\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "perturbation_std=1e-3\n",
    "\n",
    "log_dir = \"/om/user/ehoseini/MyData/KeRNL/logs/kernel_rnn_v1/two_optimizaer/MNIST_gc_%.1e_eta_m_%.1e_eta_%.1e_batch_%.1e_run_%s\" %(grad_clip,tensor_learning_rate,weight_learning_rate,batch_size, datetime.now().strftime(\"%Y%m%d_%H%M\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/om/user/ehoseini/MyData/KeRNL/logs/kernel_rnn_v1/two_optimizaer/MNIST_gc_2.0e+00_eta_m_1.0e-05_eta_1.0e-08_batch_5.0e+01_run_20190131_1321'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
