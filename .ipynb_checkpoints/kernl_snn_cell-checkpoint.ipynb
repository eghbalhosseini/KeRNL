{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background \n",
    "This code implements a spiking neural net with conductance in input. the following equations govern the dynamic of the network. \n",
    "### transmembrane voltage dynamics\n",
    "first we model the transmembrane voltage as \n",
    "$$\\tau_m \\frac{dV_i}{dt}= - V_i(t)+ R_m \\times I^{syn}_i(t) $$ \n",
    "$$ {\\tau_a}_i \\frac{dB_i(t)}{dt} = b_i^0 -B_i(t)$$ \n",
    "where, $R_m$ is membrane resistance, $\\tau_m$ is membrane time constant, and ${\\tau_a}_i$ is adaptation time constant  .\n",
    "the synaptic current relates to synaptic activations in the following way\n",
    "$$I^{syn}_i(t)= \\sum_j W^{in}_{ij} \\times X(t) + \\sum_j W^{rec}_{ij} \\times S_j(t) $$ \n",
    "\n",
    "### neuron firing dynamics \n",
    "The firing dynamics of the neuron is model as a simple reseting. More specifically, \n",
    "$$V_i \\rightarrow V_{reset} \\ \\ \\  if \\ \\ \\ V_i>=B_{i} $$\n",
    "\n",
    "$ V_{\\Theta}$ represent the threshold voltage and $V_{reset}$ is the reset voltage of the neuron.\n",
    "\n",
    "### Input dynamics \n",
    "Input synapes are the the site of learning in the spiking network. Below a conductance based formulation is presented. \n",
    "First, the time-dependent input conductance to membrane is calculated as follows \n",
    "$$ g_i(t) = \\sum_j W_{ij} S_{j}(t) $$\n",
    "\n",
    "in the current version $S_{j}(t)$ is equal to spike at timestep $t$ without any decay dynamics. \n",
    "-  TODO the term $j$ reperesent all the neurons that have a synapse onto the neuron $i$. the time dependence of conductance is due to $S(t)$ which represent the spiking activity for neurons connected to neuron $i$ . The spiking activity has the following governing equations \n",
    "$$ S_{j} \\rightarrow S_{j}+1 \\quad if \\ neuron\\ j\\ fires$$\n",
    "$$ \\frac{dS_{j}(t)}{dt} = \\frac{-S_{j}(t)}{\\tau_s}$$ \n",
    "\n",
    "### Spike Adaptation dynamics \n",
    "The threshold for spiking increases with every spike emited from a neuron with the following dynamics \n",
    "$$ B_{i}(t) \\rightarrow B_{i}(t)+\\frac{\\beta}{{\\tau_a}_i} \\quad if \\ neuron\\ i\\ fires$$\n",
    "\n",
    "### References \n",
    "-  Fiete, Ila R., Walter Senn, Claude Z. H. Wang, and Richard H. R. Hahnloser. 2010. “Spike-Time-Dependent Plasticity and Heterosynaptic Competition Organize Networks to Produce Long Scale-Free Sequences of Neural Activity.” Neuron 65 (4): 563–76. \n",
    "\n",
    "-  Bellec, Guillaume, Darjan Salaj, Anand Subramoney, Robert Legenstein, and Wolfgang Maass. 2018. “Long Short-Term Memory and Learning-to-Learn in Networks of Spiking Neurons.” arXiv [cs.NE]. arXiv. http://arxiv.org/abs/1803.09574.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### notes about the implementation \n",
    "-  both input and recurrent weight are combined into 1 kernel matrix \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import collections\n",
    "import hashlib\n",
    "import numbers\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "from tensorflow.python.eager import context\n",
    "from tensorflow.python.framework import constant_op\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.framework import tensor_util\n",
    "from tensorflow.python.layers import base as base_layer\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import clip_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.ops import partitioned_variables\n",
    "from tensorflow.python.ops import random_ops\n",
    "from tensorflow.python.ops import tensor_array_ops\n",
    "from tensorflow.python.ops import variable_scope as vs\n",
    "from tensorflow.python.ops import variables as tf_variables\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "from tensorflow.python.util import nest\n",
    "from tensorflow.contrib.rnn.python.ops.core_rnn_cell import _Linear\n",
    "from tensorflow.contrib import slim\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "import kernl_spiking_cell_v4 as kernl_spiking_cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first testing the code on a constant input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "sequence_length = 50\n",
    "num_inputs=1\n",
    "num_units=2\n",
    "#input_spikes=np.random.randint(2,size=[batch_size,sequence_length,num_inputs])\n",
    "input_spikes=1.0*np.ones([batch_size,sequence_length,num_inputs])\n",
    "tf.reset_default_graph()\n",
    "\n",
    "tf_input_values = tf.constant(input_spikes, dtype=tf.float32)\n",
    "kernl_snn = kernl_spiking_cell.kernl_spike_Cell(num_units=num_units,\n",
    "                                                num_inputs=num_inputs,\n",
    "                                                state_is_tuple=True,\n",
    "                                                output_is_tuple=True,\n",
    "                                                tau_s=10.0,\n",
    "                                                tau_refract=5.0,\n",
    "                                                tau_m=20.0,\n",
    "                                                noise_param=1.0,\n",
    "                                                time_steps=sequence_length,\n",
    "                                                kernel_initializer=tf.initializers.random_uniform(),\n",
    "                                                bias_initializer=tf.initializers.zeros())\n",
    "outputs, state = tf.nn.dynamic_rnn(cell=kernl_snn, dtype=tf.float32, inputs=tf_input_values)\n",
    "\n",
    "cell_outputs=[]\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    output_run , state_run = sess.run([outputs, state])\n",
    "\n",
    "    variables_names =[v.name for v in tf.global_variables()]\n",
    "    values = sess.run(variables_names)\n",
    "    for k,v in zip(variables_names, values):\n",
    "        print(k, v) \n",
    "plt.figure(figsize=[15,15])\n",
    "ax=plt.subplot(5,1,1)\n",
    "ax.plot(output_run.v_mem.squeeze(axis=0))\n",
    "ax.set_title('v_mem')\n",
    "ax=plt.subplot(5,1,2)\n",
    "ax.plot(output_run.v_mem_hat.squeeze(axis=0),'--')\n",
    "ax.set_title('v_mem_hat')\n",
    "ax=plt.subplot(5,1,3)\n",
    "ax.plot(output_run.psi.squeeze(axis=0))\n",
    "ax.set_title('noise')\n",
    "ax=plt.subplot(5,1,4)\n",
    "ax.plot(output_run.v_mem.squeeze(axis=0)-output_run.v_mem_hat.squeeze(axis=0))\n",
    "ax.set_title('h-h_hat')\n",
    "ax=plt.subplot(5,1,5)\n",
    "ax.plot(np.cumsum(output_run.v_mem.squeeze(axis=0)-output_run.v_mem_hat.squeeze(axis=0),axis=0))\n",
    "ax.set_title('integral of (h-h_hat)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next we create a recurrent version to verify the functionality "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### computation graph for learning $M$ and $K(\\tau)$\n",
    "\n",
    "In the original version of kernl implemented in rnn, the value of h was a continues function in time, and the perturbation could accumulate throughout a trial, which made the calculation of difference between perturbed and non-perturbed h straightforward. However in the spiking network here, there is a complication with the value of v_mem being discontinuous. how can we aleviate this problem. based on the example about, we can see that if we integrate the difference between them, we get a smoother function to calculate gradients from. we first try to optimize based on final difference between the integrals.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "sequence_length = 50\n",
    "num_inputs=1\n",
    "num_units=2\n",
    "#input_spikes=np.random.randint(2,size=[batch_size,sequence_length,num_inputs])\n",
    "input_spikes=1.0*np.ones([batch_size,sequence_length,num_inputs])\n",
    "\n",
    "tf.reset_default_graph()\n",
    "graph=tf.Graph()\n",
    "with graph.as_default():\n",
    "    # define the network \n",
    "    tf_input_values = tf.constant(input_spikes, dtype=tf.float32)\n",
    "    kernl_snn = kernl_spiking_cell.kernl_spike_Cell(num_units=num_units,\n",
    "                                                num_inputs=num_inputs,\n",
    "                                                state_is_tuple=True,\n",
    "                                                output_is_tuple=True,\n",
    "                                                tau_s=10.0,\n",
    "                                                tau_refract=5.0,\n",
    "                                                tau_m=20.0,\n",
    "                                                noise_param=1.0,\n",
    "                                                time_steps=sequence_length,\n",
    "                                                kernel_initializer=tf.initializers.random_uniform(),\n",
    "                                                bias_initializer=tf.initializers.zeros())\n",
    "    kernl_outputs, kernl_state = tf.nn.dynamic_rnn(cell=kernl_snn, dtype=tf.float32, inputs=tf_input_values)\n",
    "    trainables=tf.trainable_variables()\n",
    "    variable_names=[v.name for v in tf.trainable_variables()]\n",
    "    # \n",
    "    find_joing_index = lambda x, name_1,name_2 : [a and b for a,b in zip([np.unicode_.find(k.name, name_1)>-1 for k in x] ,[np.unicode_.find(k.name, name_2)>-1 for k in x])].index(True)\n",
    "    # find trainable parameters for kernl \n",
    "    with tf.name_scope('kernl_Trainables') as scope:\n",
    "        kernl_temporal_filter_index= find_joing_index(trainables,'kernl','temporal_filter')\n",
    "        kernl_sensitivity_tensor_index= find_joing_index(trainables,'kernl','sensitivity_tensor')\n",
    "    # \n",
    "        kernl_tensor_training_indices=np.asarray([kernl_sensitivity_tensor_index,kernl_temporal_filter_index],dtype=np.int)\n",
    "        kernl_tensor_trainables= [trainables[k] for k in kernl_tensor_training_indices]\n",
    "     \n",
    "    with tf.name_scope('kernl_train_tensors') as scope: \n",
    "        state_diff_int=tf.reduce_sum(tf.subtract(kernl_outputs.v_mem_hat[:,:,:], kernl_outputs.v_mem[:,:,:]),axis=1)\n",
    "        #estimate_state_diff_int=tf.matmul(kernl_hidden_states.Theta[:,,:],trainables[kernl_sensitivity_tensor_index])\n",
    "        #kernl_loss_state_prediction=tf.losses.mean_squared_error(tf.subtract(kernl_hidden_states.v_mem_hat[:,-1,:], kernl_hidden_states.v_mem[:,-1,:]),tf.matmul(kernl_hidden_states.Theta[:,-1,:],trainables[kernl_sensitivity_tensor_index]))\n",
    "        #kernl_tensor_optimizer = tf.train.RMSPropOptimizer(learning_rate=tensor_learning_rate)\n",
    "        #kernl_tensor_grads=tf.gradients(ys=kernl_loss_state_prediction,xs=kernl_tensor_trainables)\n",
    "        #kernl_tensor_grad_and_vars=list(zip(kernl_tensor_grads,kernl_tensor_trainables))\n",
    "        #kernl_tensor_train_op=kernl_tensor_optimizer.apply_gradients(kernl_tensor_grad_and_vars)\n",
    "        \n",
    "    \n",
    "    ##################\n",
    "    # SUMMARIES ######\n",
    "    ##################\n",
    "    \n",
    "    #with tf.name_scope(\"kernl_tensor_summaries\") as scope: \n",
    "        # kernl sensitivity tensor \n",
    "     #   tf.summary.histogram('kernl_sensitivity_tensor_grad',kernl_tensor_grads[0]+1e-10)\n",
    "     #   tf.summary.histogram('kernl_sensitivity_tensor',trainables[kernl_sensitivity_tensor_index]+1e-10)\n",
    "        # kernl temporal filter \n",
    "     #   tf.summary.histogram('kernl_temporal_filter_grad',kernl_tensor_grads[1]+1e-10)\n",
    "     #   tf.summary.histogram('kernl_temporal_filter',trainables[kernl_temporal_filter_index]+1e-10)\n",
    "        # kernl loss \n",
    "     #   tf.summary.scalar('kernl_loss_state_prediction',kernl_loss_state_prediction+1e-10)\n",
    "        # kernl senstivity tensor and temporal filter \n",
    "     #   tf.summary.image('kernl_sensitivity_tensor',tf.expand_dims(tf.expand_dims(trainables[kernl_sensitivity_tensor_index],axis=0),axis=-1))\n",
    "     #   tf.summary.image('kernl_sensitivity_tensor_grad',tf.expand_dims(tf.expand_dims(kernl_tensor_grads[0],axis=0),axis=-1))\n",
    "     #   tf.summary.image('kernl_temporal_filter',tf.expand_dims(tf.expand_dims(tf.expand_dims(trainables[kernl_temporal_filter_index],axis=0),axis=-1),axis=-1))\n",
    "     #   tf.summary.image('kernl_temporal_filter_grad',tf.expand_dims(tf.expand_dims(tf.expand_dims(kernl_tensor_grads[1],axis=0),axis=-1),axis=-1))\n",
    "     #   kernl_tensor_merged_summary_op=tf.summary.merge_all(scope=\"kernl_tensor_summaries\")\n",
    "    \n",
    "    # init = tf.global_variables_initializer()\n",
    "    # saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'rnn/transpose_5:0' shape=(1, 50, 2) dtype=float32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernl_outputs.Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape must be rank 2 but is rank 3 for 'MatMul' (op: 'MatMul') with input shapes: [1,50,2], [2,2].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1628\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1629\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Shape must be rank 2 but is rank 3 for 'MatMul' (op: 'MatMul') with input shapes: [1,50,2], [2,2].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-e88e6abc61c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernl_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTheta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkernl_sensitivity_tensor_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   2055\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2056\u001b[0m       return gen_math_ops.mat_mul(\n\u001b[0;32m-> 2057\u001b[0;31m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   2058\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   4558\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m   4559\u001b[0m         \u001b[0;34m\"MatMul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranspose_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranspose_b\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4560\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m   4561\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4562\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m                 instructions)\n\u001b[0;32m--> 488\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[1;32m    490\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3272\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3273\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3274\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3275\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3276\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1790\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1791\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1792\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m     \u001b[0;31m# Initialize self._outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape must be rank 2 but is rank 3 for 'MatMul' (op: 'MatMul') with input shapes: [1,50,2], [2,2]."
     ]
    }
   ],
   "source": [
    "tf.einsum('unv,',kernl_outputs.Theta,trainables[kernl_sensitivity_tensor_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
