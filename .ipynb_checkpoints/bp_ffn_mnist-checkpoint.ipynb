{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf \n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# uplading mnist data \n",
    "\n",
    "old_v = tf.logging.get_verbosity()\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "train_data = mnist.train.images  # Returns np.array\n",
    "train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "eval_data = mnist.test.images  # Returns np.array\n",
    "eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)\n",
    "\n",
    "tf.logging.set_verbosity(old_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/eghbal/MyData/KeRNL/logs/bptt_ffn/mnist_eta_weight_1e-03_batch_2e+01_hum_hidd_1e+03_steps_1e+02_run_20190304_0930'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup the Model Parameters\n",
    "INPUT_SIZE=784\n",
    "HIDDEN_SIZE=1000\n",
    "OUTPUT_SIZE = 10  \n",
    "START_LEARNING_RATE=1e-3\n",
    "BATCH_SIZE=25\n",
    "NUM_TRAINING_STEPS = 50\n",
    "EPOCHS=200\n",
    "TEST_LENGTH=125\n",
    "DISPLAY_STEP=25\n",
    "weight_learning_rate=1e-3\n",
    "\n",
    "log_dir = os.environ['HOME']+\"/MyData/KeRNL/logs/ffn/bp_mnist_eta_weight_%1.0e_batch_%1.0e_hum_hidd_%1.0e_steps_%1.0e_run_%s\" %(weight_learning_rate,BATCH_SIZE,HIDDEN_SIZE,NUM_TRAINING_STEPS, datetime.now().strftime(\"%Y%m%d_%H%M\"))\n",
    "log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drelu(x):\n",
    "    return 1 - tf.maximum(0.0, tf.sign(-x))\n",
    "\n",
    "\n",
    "def dtanh(x):\n",
    "    return(1-tf.mul(tf.nn.tanh(x),tf.nn.tanh(x)))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "graph=tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # define weights and inputs to the network\n",
    "    X = tf.placeholder('float', shape=[None, INPUT_SIZE])  \n",
    "    Y = tf.placeholder('float', shape=[None, OUTPUT_SIZE])\n",
    "    initializer = tf.random_normal_initializer(stddev=0.1)\n",
    "    # define a function for extraction of variable names\n",
    "     # Hidden Layer Variables\n",
    "    W_1 = tf.get_variable(\"Hidden_W\", shape=[INPUT_SIZE, HIDDEN_SIZE], initializer=initializer)\n",
    "    b_1 = tf.get_variable(\"Hidden_b\", shape=[HIDDEN_SIZE], initializer=initializer)\n",
    "  # output layer variables \n",
    "    W_2 = tf.get_variable(\"Output_W\", shape=[HIDDEN_SIZE, OUTPUT_SIZE], initializer=initializer)\n",
    "    b_2 = tf.get_variable(\"Output_b\", shape=[OUTPUT_SIZE], initializer=initializer)\n",
    "    trainables=[W_1,b_1,W_2,b_2]\n",
    "    variable_names=[v.name for v in tf.trainable_variables()]\n",
    "    #\n",
    "    #define transformation from input to output  \n",
    "  # Hidden Layer Transformation\n",
    "    g_hidden=tf.matmul(X, W_1) + b_1\n",
    "    hidden = tf.nn.relu(g_hidden)\n",
    "  # Output Layer Transformation\n",
    "    output = tf.matmul(hidden, W_2) + b_2\n",
    "    \n",
    "\n",
    "            ##################\n",
    "            ## bptt train ####\n",
    "            ##################\n",
    "    with tf.name_scope(\"bptt_train\") as scope:\n",
    "        loss = tf.losses.mean_squared_error(Y, output)\n",
    "        correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(output, 1))\n",
    "        accuracy = 100 * tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        optimizer  = tf.train.AdamOptimizer(learning_rate=weight_learning_rate)\n",
    "  # compute and apply gradiants\n",
    "        dW_2=tf.reduce_mean(tf.transpose(tf.einsum('uv,un->uvn',tf.subtract(output,Y),(hidden))),axis=-1)\n",
    "        db_2=tf.reduce_mean(tf.subtract(output,Y),axis=0)\n",
    "        dg_hidden=drelu(g_hidden)\n",
    "        dg_hidden_diag=tf.linalg.diag(dg_hidden)\n",
    "        dW_1=tf.transpose(tf.reduce_mean(tf.einsum('uv,ug->uvg',tf.einsum('uv,uvg->ug',tf.einsum('un,nv->uv',tf.subtract(output,Y),tf.transpose(W_2)),dg_hidden_diag),X),axis=0))\n",
    "        db_1=tf.transpose(tf.reduce_mean(tf.einsum('uv,ug->ug',tf.einsum('un,nv->uv',tf.subtract(output,Y),tf.transpose(W_2)),dg_hidden),axis=0))                                    \n",
    "        ffn_grads=tf.gradients(loss,[W_1,b_1,W_2,b_2])\n",
    "        new_ffn_grads=list(zip([dW_1,db_1,dW_2,db_2],trainables))\n",
    "        ffn_train_op=optimizer.apply_gradients(new_ffn_grads)\n",
    "        \n",
    "  \n",
    "    with tf.name_scope(\"bptt_evaluate\") as scope: \n",
    "        bptt_loss_cross_validiation=tf.losses.mean_squared_error(Y,output)\n",
    "        bptt_correct_pred_cross_val=tf.equal(tf.argmax(Y, 1), tf.argmax(output, 1))\n",
    "        bptt_accu_cross_validation=100 * tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "    with tf.name_scope('cross_validation_summary') as scope: \n",
    "        tf.summary.scalar('cross_validation_loss',bptt_loss_cross_validiation+1e-10)\n",
    "        tf.summary.scalar('cross_validation_accu',bptt_accu_cross_validation+1e-10)\n",
    "      \n",
    "        bptt_evaluate_summary_op=tf.summary.merge_all(scope=\"cross_validation_summary\") \n",
    "        \n",
    "                ##################\n",
    "                # SUMMARIES ######\n",
    "                ##################\n",
    "                \n",
    "    with tf.name_scope(\"bptt_summaries\") as scope:\n",
    "                    # bptt kernel\n",
    "        tf.summary.histogram('bptt_hidd_W',W_1+1e-10)\n",
    "                    # bptt output weight\n",
    "        tf.summary.histogram('bptt_output_W',W_2+1e-10)\n",
    "                    # bptt output bias\n",
    "                    # bptt loss and accuracy\n",
    "        tf.summary.scalar('bptt_loss_output_prediction',loss+1e-10)\n",
    "        tf.summary.scalar('accuracy',accuracy+1e-10)\n",
    "        bptt_merged_summary_op=tf.summary.merge_all(scope=\"bptt_summaries\")          \n",
    "        \n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['variable: ', 'Hidden_W:0']\n",
      "['variable: ', -1]\n",
      "['shape: ', (784, 1000)]\n",
      "['variable: ', 'Hidden_b:0']\n",
      "['variable: ', -1]\n",
      "['shape: ', (1000,)]\n",
      "['variable: ', 'Output_W:0']\n",
      "['variable: ', -1]\n",
      "['shape: ', (1000, 10)]\n",
      "['variable: ', 'Output_b:0']\n",
      "['variable: ', -1]\n",
      "['shape: ', (10,)]\n"
     ]
    }
   ],
   "source": [
    "# verify initialization\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "with tf.Session(graph=graph,) as sess : \n",
    "    sess.run(init)\n",
    "    values,trainable_vars = sess.run([variable_names,trainables])\n",
    "    for k, v in zip(variable_names,values):\n",
    "        print([\"variable: \" , k])\n",
    "        #print([\"value: \" , v])\n",
    "        print([\"variable: \" , np.unicode_.find(k,'output')]) \n",
    "        print([\"shape: \" , v.shape])\n",
    "        #print(v) training_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(log_dir).mkdir(exist_ok=True, parents=True)\n",
    "filelist = [ f for f in os.listdir(log_dir) if f.endswith(\".local\") ]\n",
    "for f in filelist:\n",
    "    os.remove(os.path.join(log_dir, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 1,bptt train Loss: 3.334, bptt_accuracy : 24.000\n",
      "Epoch: 1, Batch: 26,bptt train Loss: 0.579, bptt_accuracy : 32.000\n",
      "Epoch: 1, Batch: 51,bptt train Loss: 0.347, bptt_accuracy : 48.000\n",
      "Epoch: 1, Batch: 76,bptt train Loss: 0.319, bptt_accuracy : 28.000\n",
      "Epoch: 1, cross validation loss :0.278, cross validation accuracy: 52.000\n",
      "Epoch: 2, Batch: 1,bptt train Loss: 0.312, bptt_accuracy : 44.000\n",
      "Epoch: 2, Batch: 26,bptt train Loss: 0.218, bptt_accuracy : 56.000\n",
      "Epoch: 2, Batch: 51,bptt train Loss: 0.213, bptt_accuracy : 68.000\n",
      "Epoch: 2, Batch: 76,bptt train Loss: 0.168, bptt_accuracy : 52.000\n",
      "Epoch: 2, cross validation loss :0.187, cross validation accuracy: 68.000\n",
      "Epoch: 3, Batch: 1,bptt train Loss: 0.184, bptt_accuracy : 60.000\n",
      "Epoch: 3, Batch: 26,bptt train Loss: 0.199, bptt_accuracy : 56.000\n",
      "Epoch: 3, Batch: 51,bptt train Loss: 0.132, bptt_accuracy : 60.000\n",
      "Epoch: 3, Batch: 76,bptt train Loss: 0.114, bptt_accuracy : 76.000\n",
      "Epoch: 3, cross validation loss :0.138, cross validation accuracy: 68.000\n",
      "Epoch: 4, Batch: 1,bptt train Loss: 0.174, bptt_accuracy : 56.000\n",
      "Epoch: 4, Batch: 26,bptt train Loss: 0.125, bptt_accuracy : 72.000\n",
      "Epoch: 4, Batch: 51,bptt train Loss: 0.109, bptt_accuracy : 76.000\n",
      "Epoch: 4, Batch: 76,bptt train Loss: 0.131, bptt_accuracy : 68.000\n",
      "Epoch: 4, cross validation loss :0.117, cross validation accuracy: 73.600\n",
      "Epoch: 5, Batch: 1,bptt train Loss: 0.115, bptt_accuracy : 80.000\n",
      "Epoch: 5, Batch: 26,bptt train Loss: 0.123, bptt_accuracy : 72.000\n",
      "Epoch: 5, Batch: 51,bptt train Loss: 0.080, bptt_accuracy : 96.000\n",
      "Epoch: 5, Batch: 76,bptt train Loss: 0.100, bptt_accuracy : 80.000\n",
      "Epoch: 5, cross validation loss :0.099, cross validation accuracy: 73.600\n",
      "Epoch: 6, Batch: 1,bptt train Loss: 0.089, bptt_accuracy : 80.000\n",
      "Epoch: 6, Batch: 26,bptt train Loss: 0.098, bptt_accuracy : 72.000\n",
      "Epoch: 6, Batch: 51,bptt train Loss: 0.088, bptt_accuracy : 76.000\n",
      "Epoch: 6, Batch: 76,bptt train Loss: 0.072, bptt_accuracy : 88.000\n",
      "Epoch: 6, cross validation loss :0.084, cross validation accuracy: 79.200\n",
      "Epoch: 7, Batch: 1,bptt train Loss: 0.072, bptt_accuracy : 88.000\n",
      "Epoch: 7, Batch: 26,bptt train Loss: 0.071, bptt_accuracy : 80.000\n",
      "Epoch: 7, Batch: 51,bptt train Loss: 0.075, bptt_accuracy : 76.000\n",
      "Epoch: 7, Batch: 76,bptt train Loss: 0.062, bptt_accuracy : 88.000\n",
      "Epoch: 7, cross validation loss :0.070, cross validation accuracy: 83.200\n",
      "Epoch: 8, Batch: 1,bptt train Loss: 0.089, bptt_accuracy : 76.000\n",
      "Epoch: 8, Batch: 26,bptt train Loss: 0.088, bptt_accuracy : 88.000\n",
      "Epoch: 8, Batch: 51,bptt train Loss: 0.079, bptt_accuracy : 84.000\n",
      "Epoch: 8, Batch: 76,bptt train Loss: 0.070, bptt_accuracy : 76.000\n",
      "Epoch: 8, cross validation loss :0.060, cross validation accuracy: 89.600\n",
      "Epoch: 9, Batch: 1,bptt train Loss: 0.059, bptt_accuracy : 84.000\n",
      "Epoch: 9, Batch: 26,bptt train Loss: 0.055, bptt_accuracy : 92.000\n",
      "Epoch: 9, Batch: 51,bptt train Loss: 0.068, bptt_accuracy : 88.000\n",
      "Epoch: 9, Batch: 76,bptt train Loss: 0.057, bptt_accuracy : 92.000\n",
      "Epoch: 9, cross validation loss :0.052, cross validation accuracy: 92.000\n",
      "Epoch: 10, Batch: 1,bptt train Loss: 0.055, bptt_accuracy : 76.000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-270a0f96e992>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_TRAINING_STEPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbptt_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbptt_accu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbptt_merged_summary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mffn_train_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbptt_merged_summary_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mtb_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbptt_merged_summary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mNUM_TRAINING_STEPS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# write graph into tensorboard \n",
    "tb_writer = tf.summary.FileWriter(log_dir,graph)\n",
    "# run a training session \n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(EPOCHS):\n",
    "        for step in range(NUM_TRAINING_STEPS): \n",
    "            batch_x, batch_y = mnist.train.next_batch(BATCH_SIZE)\n",
    "            _, bptt_loss,bptt_accu,bptt_merged_summary=sess.run([ffn_train_op,loss,accuracy,bptt_merged_summary_op], feed_dict={X: batch_x, Y: batch_y})\n",
    "            tb_writer.add_summary(bptt_merged_summary, global_step=epoch*NUM_TRAINING_STEPS+step+1)\n",
    "\n",
    "            if step % DISPLAY_STEP==0 : \n",
    "                print('Epoch: {}, Batch: {},bptt train Loss: {:.3f}, bptt_accuracy : {:.3f}'.format(epoch+1,step + 1, bptt_loss,bptt_accu))\n",
    "                \n",
    "        # run test at the end of each epoch \n",
    "        test_x=mnist.test.images[:TEST_LENGTH]\n",
    "        test_y=mnist.test.labels[:TEST_LENGTH]  \n",
    "        bptt_test_loss,bptt_test_accu, bptt_evaluate_summary=sess.run([bptt_loss_cross_validiation,bptt_accu_cross_validation,bptt_evaluate_summary_op], feed_dict={X: test_x, Y: test_y})        \n",
    "        tb_writer.add_summary(bptt_evaluate_summary, global_step=epoch*NUM_TRAINING_STEPS+NUM_TRAINING_STEPS+1)\n",
    "        print('Epoch: {}, cross validation loss :{:.3f}, cross validation accuracy: {:.3f}'.format(epoch+1,bptt_test_loss,bptt_test_accu))\n",
    "            \n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    save_path = saver.save(sess, log_dir+\"/model.ckpt\", global_step=step,write_meta_graph=True)\n",
    "    print(\"Model saved in path: %s\" % save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
