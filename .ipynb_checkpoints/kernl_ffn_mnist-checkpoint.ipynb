{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf \n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# uplading mnist data \n",
    "\n",
    "old_v = tf.logging.get_verbosity()\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "train_data = mnist.train.images  # Returns np.array\n",
    "train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "eval_data = mnist.test.images  # Returns np.array\n",
    "eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)\n",
    "\n",
    "tf.logging.set_verbosity(old_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/eghbal/MyData/KeRNL/logs/ffn/kernl_mnist_eta_weight_1e-03_batch_2e+01_hum_hidd_1e+03_steps_5e+01_run_20190304_0953'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup the Model Parameters\n",
    "INPUT_SIZE=784\n",
    "HIDDEN_SIZE=1000\n",
    "OUTPUT_SIZE = 10  \n",
    "START_LEARNING_RATE=1e-3\n",
    "BATCH_SIZE=25\n",
    "NUM_TRAINING_STEPS = 50\n",
    "EPOCHS=200\n",
    "TEST_LENGTH=125\n",
    "DISPLAY_STEP=25\n",
    "weight_learning_rate=1e-3\n",
    "\n",
    "log_dir = os.environ['HOME']+\"/MyData/KeRNL/logs/ffn/kernl_mnist_eta_weight_%1.0e_batch_%1.0e_hum_hidd_%1.0e_steps_%1.0e_run_%s\" %(weight_learning_rate,BATCH_SIZE,HIDDEN_SIZE,NUM_TRAINING_STEPS, datetime.now().strftime(\"%Y%m%d_%H%M\"))\n",
    "log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drelu(x):\n",
    "    return 1 - tf.maximum(0.0, tf.sign(-x))\n",
    "\n",
    "\n",
    "def dtanh(x):\n",
    "    return(1-tf.mul(tf.nn.tanh(x),tf.nn.tanh(x)))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "graph=tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # define weights and inputs to the network\n",
    "    X = tf.placeholder('float', shape=[None, INPUT_SIZE])  \n",
    "    Y = tf.placeholder('float', shape=[None, OUTPUT_SIZE])\n",
    "    initializer = tf.random_normal_initializer(stddev=0.1)\n",
    "    # define a function for extraction of variable names\n",
    "     # Hidden Layer Variables\n",
    "    W_1 = tf.get_variable(\"Hidden_W\", shape=[INPUT_SIZE, HIDDEN_SIZE], initializer=initializer)\n",
    "    b_1 = tf.get_variable(\"Hidden_b\", shape=[HIDDEN_SIZE], initializer=initializer)\n",
    "  # output layer variables \n",
    "    W_2 = tf.get_variable(\"Output_W\", shape=[HIDDEN_SIZE, OUTPUT_SIZE], initializer=initializer)\n",
    "    b_2 = tf.get_variable(\"Output_b\", shape=[OUTPUT_SIZE], initializer=initializer)\n",
    "    # return weight \n",
    "    B=tf.get_variable('B',shape=[OUTPUT_SIZE,HIDDEN_SIZE],initializer=tf.initializers.random_uniform(minval=-0.5,maxval=0.5))\n",
    "    trainables=[W_1,b_1,W_2,b_2,B]\n",
    "    variable_names=[v.name for v in tf.trainable_variables()]\n",
    "    #\n",
    "    #define transformation from input to output  \n",
    "  # Hidden Layer Transformation\n",
    "    g_hidden=tf.matmul(X, W_1) + b_1\n",
    "    hidden = tf.nn.relu(g_hidden)\n",
    "  # Output Layer Transformation\n",
    "    output = tf.matmul(hidden, W_2) + b_2\n",
    "    \n",
    "\n",
    "            ##################\n",
    "            ## kernl train ####\n",
    "            ##################\n",
    "    with tf.name_scope(\"kernl_train\") as scope:\n",
    "        loss = tf.losses.mean_squared_error(Y, output)\n",
    "        correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(output, 1))\n",
    "        accuracy = 100 * tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        optimizer  = tf.train.AdamOptimizer(learning_rate=weight_learning_rate)\n",
    "  # compute and apply gradiants\n",
    "        dW_2=tf.reduce_mean(tf.transpose(tf.einsum('uv,un->uvn',tf.subtract(output,Y),(hidden))),axis=-1)\n",
    "        db_2=tf.reduce_mean(tf.subtract(output,Y),axis=0)\n",
    "        dg_hidden=drelu(g_hidden)\n",
    "        dg_hidden_diag=tf.linalg.diag(dg_hidden)\n",
    "        dW_1=tf.transpose(tf.reduce_mean(tf.einsum('uv,ug->uvg',tf.einsum('uv,uvg->ug',tf.einsum('un,nv->uv',tf.subtract(output,Y),B),dg_hidden_diag),X),axis=0))\n",
    "        db_1=tf.transpose(tf.reduce_mean(tf.einsum('uv,ug->ug',tf.einsum('un,nv->uv',tf.subtract(output,Y),B),dg_hidden),axis=0)) \n",
    "        # gradient for B\n",
    "        dB=tf.negative(tf.reduce_mean(tf.einsum('uv,uz->uvz',output,tf.subtract(hidden,tf.einsum('uv,vz->uz',output,B))),axis=0))\n",
    "        new_ffn_grads=list(zip([dW_1,db_1,dW_2,db_2,dB],trainables))\n",
    "        ffn_train_op=optimizer.apply_gradients(new_ffn_grads)\n",
    "        \n",
    "  \n",
    "    with tf.name_scope(\"evaluate\") as scope: \n",
    "        kernl_loss_cross_validiation=tf.losses.mean_squared_error(Y,output)\n",
    "        kernl_correct_pred_cross_val=tf.equal(tf.argmax(Y, 1), tf.argmax(output, 1))\n",
    "        kernl_accu_cross_validation=100 * tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "    with tf.name_scope('cross_validation_summary') as scope: \n",
    "        tf.summary.scalar('cross_validation_loss',kernl_loss_cross_validiation+1e-10)\n",
    "        tf.summary.scalar('cross_validation_accu',kernl_accu_cross_validation+1e-10)\n",
    "      \n",
    "        kernl_evaluate_summary_op=tf.summary.merge_all(scope=\"cross_validation_summary\") \n",
    "        \n",
    "                ##################\n",
    "                # SUMMARIES ######\n",
    "                ##################\n",
    "                \n",
    "    with tf.name_scope(\"summaries\") as scope:\n",
    "                    # kernl kernel\n",
    "        tf.summary.histogram('kernl_hidd_W',W_1+1e-10)\n",
    "        tf.summary.histogram('return_B',B+1e-10)\n",
    "                    # kernl output weight\n",
    "        tf.summary.histogram('kernl_output_W',W_2+1e-10)\n",
    "                    # kernl output bias\n",
    "                    # kernl loss and accuracy\n",
    "        tf.summary.scalar('loss_output_prediction',loss+1e-10)\n",
    "        tf.summary.scalar('accuracy',accuracy+1e-10)\n",
    "        kernl_merged_summary_op=tf.summary.merge_all(scope=\"summaries\")          \n",
    "        \n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['variable: ', 'Hidden_W:0']\n",
      "['variable: ', -1]\n",
      "['shape: ', (784, 1000)]\n",
      "['variable: ', 'Hidden_b:0']\n",
      "['variable: ', -1]\n",
      "['shape: ', (1000,)]\n",
      "['variable: ', 'Output_W:0']\n",
      "['variable: ', -1]\n",
      "['shape: ', (1000, 10)]\n",
      "['variable: ', 'Output_b:0']\n",
      "['variable: ', -1]\n",
      "['shape: ', (10,)]\n",
      "['variable: ', 'B:0']\n",
      "['variable: ', -1]\n",
      "['shape: ', (10, 1000)]\n"
     ]
    }
   ],
   "source": [
    "# verify initialization\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "with tf.Session(graph=graph,) as sess : \n",
    "    sess.run(init)\n",
    "    values,trainable_vars = sess.run([variable_names,trainables])\n",
    "    for k, v in zip(variable_names,values):\n",
    "        print([\"variable: \" , k])\n",
    "        #print([\"value: \" , v])\n",
    "        print([\"variable: \" , np.unicode_.find(k,'output')]) \n",
    "        print([\"shape: \" , v.shape])\n",
    "        #print(v) training_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(log_dir).mkdir(exist_ok=True, parents=True)\n",
    "filelist = [ f for f in os.listdir(log_dir) if f.endswith(\".local\") ]\n",
    "for f in filelist:\n",
    "    os.remove(os.path.join(log_dir, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 1,kernl train Loss: 7.653, kernl_accuracy : 16.000\n",
      "Epoch: 1, Batch: 26,kernl train Loss: 1.792, kernl_accuracy : 16.000\n",
      "Epoch: 1, cross validation loss :1.307, cross validation accuracy: 15.200\n",
      "Epoch: 2, Batch: 1,kernl train Loss: 1.429, kernl_accuracy : 8.000\n",
      "Epoch: 2, Batch: 26,kernl train Loss: 1.022, kernl_accuracy : 16.000\n",
      "Epoch: 2, cross validation loss :0.859, cross validation accuracy: 29.600\n",
      "Epoch: 3, Batch: 1,kernl train Loss: 0.735, kernl_accuracy : 24.000\n",
      "Epoch: 3, Batch: 26,kernl train Loss: 0.844, kernl_accuracy : 16.000\n",
      "Epoch: 3, cross validation loss :0.661, cross validation accuracy: 30.400\n",
      "Epoch: 4, Batch: 1,kernl train Loss: 0.589, kernl_accuracy : 28.000\n",
      "Epoch: 4, Batch: 26,kernl train Loss: 0.467, kernl_accuracy : 40.000\n",
      "Epoch: 4, cross validation loss :0.557, cross validation accuracy: 25.600\n",
      "Epoch: 5, Batch: 1,kernl train Loss: 0.622, kernl_accuracy : 32.000\n",
      "Epoch: 5, Batch: 26,kernl train Loss: 0.450, kernl_accuracy : 28.000\n",
      "Epoch: 5, cross validation loss :0.420, cross validation accuracy: 33.600\n",
      "Epoch: 6, Batch: 1,kernl train Loss: 0.489, kernl_accuracy : 24.000\n"
     ]
    }
   ],
   "source": [
    "# write graph into tensorboard \n",
    "tb_writer = tf.summary.FileWriter(log_dir,graph)\n",
    "# run a training session \n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(EPOCHS):\n",
    "        for step in range(NUM_TRAINING_STEPS): \n",
    "            batch_x, batch_y = mnist.train.next_batch(BATCH_SIZE)\n",
    "            _, kernl_loss,kernl_accu,kernl_merged_summary=sess.run([ffn_train_op,loss,accuracy,kernl_merged_summary_op], feed_dict={X: batch_x, Y: batch_y})\n",
    "            tb_writer.add_summary(kernl_merged_summary, global_step=epoch*NUM_TRAINING_STEPS+step+1)\n",
    "\n",
    "            if step % DISPLAY_STEP==0 : \n",
    "                print('Epoch: {}, Batch: {},kernl train Loss: {:.3f}, kernl_accuracy : {:.3f}'.format(epoch+1,step + 1, kernl_loss,kernl_accu))\n",
    "                \n",
    "        # run test at the end of each epoch \n",
    "        test_x=mnist.test.images[:TEST_LENGTH]\n",
    "        test_y=mnist.test.labels[:TEST_LENGTH]  \n",
    "        kernl_test_loss,kernl_test_accu, kernl_evaluate_summary=sess.run([kernl_loss_cross_validiation,kernl_accu_cross_validation,kernl_evaluate_summary_op], feed_dict={X: test_x, Y: test_y})        \n",
    "        tb_writer.add_summary(kernl_evaluate_summary, global_step=epoch*NUM_TRAINING_STEPS+NUM_TRAINING_STEPS+1)\n",
    "        print('Epoch: {}, cross validation loss :{:.3f}, cross validation accuracy: {:.3f}'.format(epoch+1,kernl_test_loss,kernl_test_accu))\n",
    "            \n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    save_path = saver.save(sess, log_dir+\"/model.ckpt\", global_step=step,write_meta_graph=True)\n",
    "    print(\"Model saved in path: %s\" % save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
