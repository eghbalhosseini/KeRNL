{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters \n",
    "learning_rate=0.001\n",
    "training_steps=500\n",
    "batch_size=32\n",
    "display_step=20\n",
    "\n",
    "#network parameters \n",
    "num_input_units=80\n",
    "timesteps= 28*28\n",
    "num_classes=10\n",
    "num_hidden_units =200\n",
    "num_output_units =10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LIFLayer(n,m,dt,Spike_in,state):\n",
    "    neurons=n\n",
    "    inputs=m\n",
    "    tau_m = 5.0\n",
    "    v_theta = 1.0\n",
    "    v_reset = 0.0\n",
    "    tau_s = 5.0\n",
    "    tau_refract = 3.0\n",
    "    # tensor variables \n",
    "    V = tf.contrib.eager.Variable(tf.constant(0.0,shape=[n,1],dtype=tf.float32),name='V')\n",
    "    Spike = tf.contrib.eager.Variable(tf.constant(v_reset,shape=[n,1],dtype=tf.float32),name='Spike')\n",
    "    W_rec = tf.contrib.eager.Variable(tf.random_normal(shape=[n,n],mean=1.0,stddev=0.1,dtype=tf.float32),name='W_rec')\n",
    "    W_in = tf.contrib.eager.Variable(tf.random_normal(shape=[n,m],mean=1.0,stddev=0.1,dtype=tf.float32),name='W_in')\n",
    "    G=tf.contrib.eager.Variable(tf.ones(shape=[n,1],dtype=tf.float32),name='G')\n",
    "    S=tf.contrib.eager.Variable(tf.zeros(shape=[n,n],dtype=tf.float32),name='S')\n",
    "    G_in=tf.contrib.eager.Variable(tf.ones(shape=[n,1],dtype=tf.float32),name='G_in')\n",
    "    S_in=tf.contrib.eager.Variable(tf.zeros(shape=[n,m],dtype=tf.float32),name='S_in')\n",
    "    I_syn=tf.contrib.eager.Variable(tf.ones(shape=[n,1],dtype=tf.float32),name='I_syn')\n",
    "    t_reset=tf.contrib.eager.Variable(tf.ones(shape=[n,1],dtype=tf.float32),name='t_reset')\n",
    "    \n",
    "    # subfunctions \n",
    "    @tf.custom_gradient\n",
    "    def calculate_crossing_op(x):\n",
    "        x_norm=tf.divide(tf.subtract(x,tf.constant(v_theta,shape=[n,1])),\n",
    "                         tf.constant(v_theta,shape=[n,1]))\n",
    "        \n",
    "        def grad(dy):            \n",
    "            return tf.maximum(tf.constant(0.0,dtype=tf.float32),tf.subtract(tf.constant(1.0,dtype=tf.float32),tf.abs(x)))  \n",
    "        return tf.greater_equal(x,tf.constant(v_theta,shape=[n,1],dtype=tf.float32)), grad\n",
    "\n",
    "    ## spiking neuron dynamics \n",
    "    Spike=calculate_crossing_op(state)\n",
    "    v_update=tf.where(Spike,tf.constant(v_theta,shape=[n,1],dtype=tf.float32),state)\n",
    "    ## update conductance for recurrent spikes \n",
    "    dS_op=tf.divide(S,tau_s)\n",
    "    dS_in_op=tf.divide(S_in,tau_s)\n",
    "    S_temp=tf.subtract(S,dS_op*dt)\n",
    "    S_in_temp=tf.subtract(S_in,dS_in_op*dt)\n",
    "    S_op=tf.clip_by_value(S_temp,tf.constant(0.0,shape=[n,n]),tf.constant(100.0,shape=[n,n]))\n",
    "    S_in_op=tf.clip_by_value(S_in_temp,tf.constant(0.0,shape=[n,m]),tf.constant(100.0,shape=[n,m]))\n",
    "    Spike_op_float=tf.cast(Spike,tf.float32)\n",
    "    Spike_in_op_float=tf.cast(Spike_in,tf.float32)\n",
    "    Spike_temp=tf.transpose(tf.tile(Spike_op_float,[1,n]))\n",
    "    # not sure about this step\n",
    "    Spike_in_temp=tf.transpose(tf.tile(Spike_in_op_float,[1,n]))\n",
    "    Spike_ax=tf.clip_by_value(tf.subtract(Spike_temp,tf.eye(n,dtype=tf.float32)),0.0,100)\n",
    "    Spike_in_ax=Spike_in_temp\n",
    "    S_update= tf.assign(S,tf.add(S,Spike_ax))\n",
    "    S_in_update= tf.assign(S_in,tf.add(S_in,Spike_in_ax))\n",
    "    G_op=tf.assign(G,tf.reduce_sum(tf.multiply(W_rec,S_update), 1, keepdims=True))\n",
    "    G_in_op=tf.assign(G_in,tf.reduce_sum(tf.multiply(W_in,S_in_update), 1, keepdims=True))\n",
    "    I_input=tf.assign(I_syn,tf.add(tf.multiply(G_op,v_update),tf.multiply(G_in_op,v_update)))\n",
    "\n",
    "    ## update voltages\n",
    "    # find neurons in refractory \n",
    "    t_subtract= tf.assign(t_reset,tf.subtract(t_reset,tf.constant(1.0,shape=[n,1])))\n",
    "    t_margin=tf.assign(t_reset,tf.where(tf.less(t_subtract,0.0),tf.constant(0.0,shape=[n,1]),t_subtract))\n",
    "    t_reset_update=tf.assign(t_reset,tf.where(Spike,tf.constant(tau_refract,shape=[n,1],dtype=tf.float32),t_margin))\n",
    "    eligilible_update=tf.equal(t_reset_update,tf.constant(0.0,shape=[n,1]))\n",
    "    # update voltage     \n",
    "    dV_op=tf.where(eligilible_update,tf.divide(tf.subtract(I_input,v_update),tau_m),\n",
    "                                     tf.constant(v_reset,shape=[n,1],dtype=tf.float32))\n",
    "    V_out=tf.assign(V,tf.minimum(tf.constant(v_theta,shape=[n,1]),\n",
    "                                        tf.add(v_update,tf.multiply(dV_op,dt))))\n",
    "    return Spike, V_out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "state=tf.random_uniform(shape=[num_hidden_units,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_in=tf.where(tf.greater(tf.random_uniform(shape=[num_input_units,1]),1.0),\n",
    "                  tf.constant(1.0,shape=[num_input_units,1],dtype=tf.float32),\n",
    "                              tf.constant(0.0,shape=[num_input_units,1],dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=tf.constant(1.0,dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    " [spike,state]=LIFLayer(num_hidden_units,num_input_units,dt,spike_in,state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OutputLayer(k,j,spike_in,dt,state):\n",
    "    neurons=k\n",
    "    inputs=j\n",
    "    tau_m = 5.0\n",
    "    # tensor variables \n",
    "    V_out = tf.contrib.eager.Variable(tf.constant(0.0,shape=[k,1],dtype=tf.float32),name='V_out')\n",
    "    W_out = tf.contrib.eager.Variable(tf.random_normal(shape=[k,j],mean=0.0,stddev=0.1,dtype=tf.float32),name='W_out')\n",
    "    G_out= tf.contrib.eager.Variable(tf.ones(shape=[k,1],dtype=tf.float32),name='G_out')\n",
    "    \n",
    "    ## update membrane dynamics \n",
    "    dv_out=tf.divide(state,tau_m)\n",
    "    V_temp=tf.subtract(state,dv_out*dt)\n",
    "    V_update=tf.clip_by_value(V_temp,tf.constant(0.0,shape=[k,1]),tf.constant(100.0,shape=[k,1]))\n",
    "    spike_in_float=tf.cast(spike_in,tf.float32)\n",
    "    spike_temp=tf.transpose(tf.tile(spike_in_float,[1,k]))\n",
    "    weight_update=tf.multiply(W_out,spike_temp)\n",
    "    G_update=tf.reduce_sum(weight_update, 1, keepdims=True)\n",
    "    # not sure about this step\n",
    "    v_membrane = tf.add(V_update,G_update)\n",
    "    return v_membrane\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_rec=tf.where(tf.greater(tf.random_uniform(shape=[num_hidden_units,1]),0.5),\n",
    "                  tf.constant(1.0,shape=[num_hidden_units,1],dtype=tf.float32),\n",
    "                              tf.constant(0.0,shape=[num_hidden_units,1],dtype=tf.float32))\n",
    "state_out=tf.random_uniform(shape=[num_output_units,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=38289, shape=(10, 1), dtype=float32, numpy=\n",
       "array([[ 1.3298843 ],\n",
       "       [-0.19542417],\n",
       "       [ 1.5076162 ],\n",
       "       [ 0.9977307 ],\n",
       "       [ 0.4538017 ],\n",
       "       [ 1.7332283 ],\n",
       "       [ 0.92777336],\n",
       "       [ 1.9465357 ],\n",
       "       [-0.4065798 ],\n",
       "       [-0.10051703]], dtype=float32)>"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vout = OutputLayer(num_output_units,num_hidden_units,spike_rec,dt,state_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as t:\n",
    "    t.watch(spike_rec)\n",
    "    Vout = OutputLayer(num_output_units,num_hidden_units,spike_rec,dt,state_out)\n",
    "\n",
    "dz_dx = t.gradient(Vout, spike_rec) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=45005, shape=(10, 1), dtype=float32, numpy=\n",
       "array([[0. ],\n",
       "       [0. ],\n",
       "       [0.8],\n",
       "       [0. ],\n",
       "       [0.8],\n",
       "       [0.8],\n",
       "       [0.8],\n",
       "       [0.8],\n",
       "       [0.8],\n",
       "       [0.8]], dtype=float32)>"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spike_rec=tf.where(tf.greater(tf.random_uniform(shape=[num_hidden_units,1]),0.5),\n",
    "                  tf.constant(1.0,shape=[num_hidden_units,1],dtype=tf.float32),\n",
    "                              tf.constant(0.0,shape=[num_hidden_units,1],dtype=tf.float32))\n",
    "x = tf.random_uniform(shape=[num_output_units,1])\n",
    "W_o = tf.contrib.eager.Variable(tf.random_normal(shape=[num_output_units,num_hidden_units],mean=0.0,stddev=0.1,dtype=tf.float32))\n",
    "\n",
    "with tf.GradientTape() as t:\n",
    "  t.watch(x)\n",
    "  y=tf.divide(x,5.0)\n",
    "  z=tf.subtract(x,y*dt)\n",
    "  z1=tf.clip_by_value(z,tf.constant(0.0,shape=[num_output_units,1]),tf.constant(0.5,shape=[num_output_units,1]))\n",
    "  z2=tf.cast(z1,tf.float64)\n",
    "  sp=tf.cast(spike_rec,tf.float32)\n",
    "  sp1=tf.transpose(tf.tile(sp,[1,num_output_units]))\n",
    "  w1=tf.multiply(W_o,sp1)\n",
    "  G_up=tf.reduce_sum(w1, 1, keepdims=True) \n",
    "  v_mem = tf.divide(z1,G_up)\n",
    "# Derivative of z with respect to the original input tensor x\n",
    "dz_dx = t.gradient(z2, x)\n",
    "dz_dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "state=tf.random_uniform(shape=[num_hidden_units,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "[spike,state]=LIFLayer(num_hidden_units,num_input_units,dt,spike_in,state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=45174, shape=(200, 200), dtype=float32, numpy=\n",
       "array([[1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "with tf.GradientTape() as t:\n",
    "    t.watch(spike_rec)\n",
    "    v_update=tf.where(tf.cast(spike_rec,tf.bool),tf.constant(0.0,shape=[num_hidden_units,1],dtype=tf.float32),state)\n",
    "    v_update1=tf.subtract(state,tf.multiply(spike_rec,tf.constant(1.0,shape=[num_hidden_units,1],dtype=tf.float32)))\n",
    "    Spike_temp=tf.transpose(tf.tile(spike_rec,[1,num_hidden_units]))\n",
    "dz_dx = t.gradient(Spike_temp, spike_rec) \n",
    "#plt.figure()\n",
    "#plt.subplot(3,1,1)\n",
    "#plt.plot(v_update1.numpy())\n",
    "#plt.subplot(3,1,2)\n",
    "#plt.plot(v_update.numpy())\n",
    "#plt.subplot(3,1,3)\n",
    "#plt.plot(state.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LIFLayer(n,m,dt,Spike_in,state):\n",
    "    neurons=n\n",
    "    inputs=m\n",
    "    tau_m = 5.0\n",
    "    v_theta = 1.0\n",
    "    v_reset = 0.0\n",
    "    tau_s = 5.0\n",
    "    tau_refract = 3.0\n",
    "    # tensor variables \n",
    "    V = tf.contrib.eager.Variable(tf.constant(0.0,shape=[n,1],dtype=tf.float32),name='V')\n",
    "    Spike = tf.contrib.eager.Variable(tf.constant(v_reset,shape=[n,1],dtype=tf.float32),name='Spike')\n",
    "    W_rec = tf.contrib.eager.Variable(tf.random_normal(shape=[n,n],mean=1.0,stddev=0.1,dtype=tf.float32),name='W_rec')\n",
    "    W_in = tf.contrib.eager.Variable(tf.random_normal(shape=[n,m],mean=1.0,stddev=0.1,dtype=tf.float32),name='W_in')\n",
    "    G=tf.contrib.eager.Variable(tf.ones(shape=[n,1],dtype=tf.float32),name='G')\n",
    "    S=tf.contrib.eager.Variable(tf.zeros(shape=[n,n],dtype=tf.float32),name='S')\n",
    "    G_in=tf.contrib.eager.Variable(tf.ones(shape=[n,1],dtype=tf.float32),name='G_in')\n",
    "    S_in=tf.contrib.eager.Variable(tf.zeros(shape=[n,m],dtype=tf.float32),name='S_in')\n",
    "    I_syn=tf.contrib.eager.Variable(tf.ones(shape=[n,1],dtype=tf.float32),name='I_syn')\n",
    "    t_reset=tf.contrib.eager.Variable(tf.ones(shape=[n,1],dtype=tf.float32),name='t_reset')\n",
    "    \n",
    "    # subfunctions \n",
    "    @tf.custom_gradient\n",
    "    def calculate_crossing_op(x):\n",
    "        x_norm=tf.divide(tf.subtract(x,tf.constant(v_theta,shape=[n,1])),\n",
    "                         tf.constant(v_theta,shape=[n,1]))\n",
    "        \n",
    "        def grad(dy):            \n",
    "            return tf.maximum(tf.constant(0.0,dtype=tf.float32),tf.subtract(tf.constant(1.0,dtype=tf.float32),tf.abs(x)))  \n",
    "        return tf.greater_equal(x,tf.constant(v_theta,shape=[n,1],dtype=tf.float32)), grad\n",
    "\n",
    "    ## spiking neuron dynamics \n",
    "    Spike=calculate_crossing_op(state)\n",
    "    v_update=tf.subtract(state,tf.multiply(Spike,tf.constant(v_theta,shape=[n,1],dtype=tf.float32)))\n",
    "    ## update conductance for recurrent spikes \n",
    "    dS_op=tf.divide(S,tau_s)\n",
    "    dS_in_op=tf.divide(S_in,tau_s)\n",
    "    S_temp=tf.subtract(S,dS_op*dt)\n",
    "    S_in_temp=tf.subtract(S_in,dS_in_op*dt)\n",
    "    S_op=tf.clip_by_value(S_temp,tf.constant(0.0,shape=[n,n]),tf.constant(100.0,shape=[n,n]))\n",
    "    S_in_op=tf.clip_by_value(S_in_temp,tf.constant(0.0,shape=[n,m]),tf.constant(100.0,shape=[n,m]))\n",
    "    Spike_op_float=tf.cast(Spike,tf.float32)\n",
    "    Spike_in_op_float=tf.cast(Spike_in,tf.float32)\n",
    "    Spike_ax=tf.clip_by_value(tf.subtract(tf.transpose(tf.tile(Spike_op_float,[1,n])),\n",
    "                                            tf.eye(n,dtype=tf.float32)),0.0,100.0)\n",
    "\n",
    "    Spike_in_temp=tf.transpose(tf.tile(Spike_in_op_float,[1,n]))\n",
    "    Spike_in_ax=Spike_in_temp\n",
    "    S_update= tf.add(S,Spike_ax)\n",
    "    S_in_update=tf.add(S_in,Spike_in_ax)\n",
    "    G_op=tf.reduce_sum(tf.multiply(W_rec,S_update), 1, keepdims=True)\n",
    "    G_in_op=tf.reduce_sum(tf.multiply(W_in,S_in_update), 1, keepdims=True)\n",
    "    I_input=tf.add(tf.multiply(G_op,v_update),tf.multiply(G_in_op,v_update))\n",
    "    ## update voltages\n",
    "    # find neurons in refractory \n",
    "    t_subtract= tf.subtract(t_reset,tf.constant(1.0,shape=[n,1]))\n",
    "    t_margin=tf.clip_by_value(t_subtract,0.0,100)\n",
    "    t_reset_update=tf.add(t_margin,tf.multiply(Spike,tf.constant(v_theta,shape=[n,1],dtype=tf.float32)))\n",
    "    eligilible_update=tf.equal(t_reset_update,tf.constant(0.0,shape=[n,1]))\n",
    "    # update voltage\n",
    "    dV_op=tf.add(tf.constant(v_reset,shape=[n,1],dtype=tf.float32),tf.multiply(eligilible_update,\n",
    "                                                                               tf.divide(tf.subtract(I_input,v_update),tau_m)))\n",
    "    V_out=tf.add(v_update,tf.multiply(dV_op,dt))))\n",
    "    return Spike, V_out\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(KeRNL)",
   "language": "python",
   "name": "kernl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
