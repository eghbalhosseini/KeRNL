{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf \n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/eghbal/MyData/KeRNL/logs/ffn/lin_func_approx/dfa_eta_weight_1e-03_batch_2e+01_hum_hidd_1e+03_steps_1e+02_run_20190304_1738'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup the Model Parameters\n",
    "INPUT_SIZE=30\n",
    "HIDDEN_SIZE=1000\n",
    "OUTPUT_SIZE = 10  \n",
    "START_LEARNING_RATE=1e-3\n",
    "BATCH_LENGTH=25\n",
    "NUM_TRAINING_STEPS = 100\n",
    "EPOCHS=15\n",
    "TEST_LENGTH=125\n",
    "DISPLAY_STEP=25\n",
    "weight_learning_rate=1e-3\n",
    "\n",
    "log_dir = os.environ['HOME']+\"/MyData/KeRNL/logs/ffn/lin_func_approx/dfa_eta_weight_%1.0e_batch_%1.0e_hum_hidd_%1.0e_steps_%1.0e_run_%s\" %(weight_learning_rate,BATCH_LENGTH,HIDDEN_SIZE,NUM_TRAINING_STEPS, datetime.now().strftime(\"%Y%m%d_%H%M\"))\n",
    "log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Target=np.random.uniform(low=-1,high=1,size=(30,10))\n",
    "x_train=np.random.uniform(low=0,high=1,size=(BATCH_LENGTH*NUM_TRAINING_STEPS,30))\n",
    "y_train=np.matmul(x_train,Target)\n",
    "#\n",
    "x_test=np.random.uniform(low=0,high=1,size=(EPOCHS*TEST_LENGTH,30))\n",
    "y_test=np.matmul(x_test,Target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "graph=tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    #\n",
    "    BATCH_SIZE=tf.placeholder(tf.int64)\n",
    "    X = tf.placeholder('float', shape=[None, INPUT_SIZE])  \n",
    "    Y = tf.placeholder('float', shape=[None, OUTPUT_SIZE])\n",
    "    # define a dataset\n",
    "    dataset=tf.data.Dataset.from_tensor_slices((X,Y)).batch(BATCH_SIZE).repeat()\n",
    "    dataset = dataset.shuffle(buffer_size=500)\n",
    "    iter = dataset.make_initializable_iterator()\n",
    "    inputs,labels =iter.get_next()\n",
    "    \n",
    "    \n",
    "    #\n",
    "    initializer = tf.random_normal_initializer(stddev=0.1)\n",
    "     # Hidden Layer Variables\n",
    "    W_1 = tf.get_variable(\"Hidden_W\", shape=[INPUT_SIZE, HIDDEN_SIZE], initializer=initializer)\n",
    "    b_1 = tf.get_variable(\"Hidden_b\", shape=[HIDDEN_SIZE], initializer=initializer)\n",
    "  # output layer variables \n",
    "    W_2 = tf.get_variable(\"Output_W\", shape=[HIDDEN_SIZE, OUTPUT_SIZE], initializer=initializer)\n",
    "    b_2 = tf.get_variable(\"Output_b\", shape=[OUTPUT_SIZE], initializer=initializer)\n",
    "    # return weight \n",
    "    B=tf.get_variable('B',shape=[OUTPUT_SIZE,HIDDEN_SIZE],initializer=tf.initializers.random_uniform(minval=-0.5,maxval=0.5))\n",
    "    trainables=[W_1,b_1,W_2,b_2,B]\n",
    "    variable_names=[v.name for v in tf.trainable_variables()]\n",
    "    #\n",
    "    #define transformation from input to output  \n",
    "  # Hidden Layer Transformation\n",
    "    hidden=tf.matmul(inputs, W_1) + b_1\n",
    "  # Output Layer Transformation\n",
    "    output = tf.matmul(hidden, W_2) + b_2\n",
    "    ##################\n",
    "    ## dfa train ####\n",
    "    ##################\n",
    "    with tf.name_scope(\"dfa_train\") as scope:\n",
    "        loss = tf.losses.mean_squared_error(labels, output)\n",
    "        nse=tf.reduce_mean(tf.square(tf.subtract(output,tf.expand_dims(tf.reduce_mean(output,axis=0),axis=0))))\n",
    "        correct_prediction = tf.equal(tf.argmax(labels, 1), tf.argmax(output, 1))\n",
    "        accuracy = 100 * tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        optimizer  = tf.train.AdamOptimizer(learning_rate=weight_learning_rate)\n",
    "        \n",
    "  # compute and apply gradiants\n",
    "        dW_2=tf.reduce_mean(tf.transpose(tf.einsum('uv,un->uvn',tf.subtract(output,labels),(hidden))),axis=-1)\n",
    "        db_2=tf.reduce_mean(tf.subtract(output,labels),axis=0)\n",
    "        dW_1=tf.transpose(tf.reduce_mean(tf.einsum('uv,ug->uvg',tf.einsum('un,nv->uv',tf.subtract(output,labels),B),inputs),axis=0))\n",
    "        db_1=tf.transpose(tf.reduce_mean(tf.einsum('uv,ug->ug',tf.einsum('un,nv->uv',tf.subtract(output,labels),B),hidden*0+1),axis=0)) \n",
    "        # gradient for B\n",
    "        dB=0*tf.negative(tf.reduce_mean(tf.einsum('uv,uz->uvz',output,tf.subtract(hidden,tf.einsum('uv,vz->uz',output,B))),axis=0))\n",
    "        new_ffn_grads=list(zip([dW_1,db_1,dW_2,db_2,dB],trainables))\n",
    "        ffn_train_op=optimizer.apply_gradients(new_ffn_grads)\n",
    "        \n",
    "  \n",
    "    with tf.name_scope(\"evaluate\") as scope: \n",
    "        dfa_loss_cross_validiation=tf.losses.mean_squared_error(labels,output)\n",
    "        dfa_correct_pred_cross_val=tf.equal(tf.argmax(labels, 1), tf.argmax(output, 1))\n",
    "        dfa_accu_cross_validation=100 * tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "    with tf.name_scope('cross_validation_summary') as scope: \n",
    "        tf.summary.scalar('cross_validation_loss',dfa_loss_cross_validiation+1e-10)\n",
    "        tf.summary.scalar('cross_validation_accu',dfa_accu_cross_validation+1e-10)\n",
    "      \n",
    "        dfa_evaluate_summary_op=tf.summary.merge_all(scope=\"cross_validation_summary\") \n",
    "        \n",
    "    ##################\n",
    "    # SUMMARIES ######\n",
    "    ##################\n",
    "                \n",
    "    with tf.name_scope(\"summaries\") as scope:\n",
    "                    # dfa kernel\n",
    "        tf.summary.histogram('dfa_hidd_W',W_1+1e-10)\n",
    "        tf.summary.histogram('return_B',B+1e-10)\n",
    "                    # dfa output weight\n",
    "        tf.summary.histogram('dfa_output_W',W_2+1e-10)\n",
    "                    # dfa output bias\n",
    "                    # dfa loss and accuracy\n",
    "        tf.summary.scalar('loss_output_prediction',loss+1e-10)\n",
    "        tf.summary.scalar('accuracy',accuracy+1e-10)\n",
    "        tf.summary.scalar('NSE',nse+1e-10)\n",
    "        dfa_merged_summary_op=tf.summary.merge_all(scope=\"summaries\")          \n",
    "        \n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['variable: ', 'Hidden_W:0']\n",
      "['variable: ', -1]\n",
      "['shape: ', (30, 1000)]\n",
      "['variable: ', 'Hidden_b:0']\n",
      "['variable: ', -1]\n",
      "['shape: ', (1000,)]\n",
      "['variable: ', 'Output_W:0']\n",
      "['variable: ', -1]\n",
      "['shape: ', (1000, 10)]\n",
      "['variable: ', 'Output_b:0']\n",
      "['variable: ', -1]\n",
      "['shape: ', (10,)]\n",
      "['variable: ', 'B:0']\n",
      "['variable: ', -1]\n",
      "['shape: ', (10, 1000)]\n"
     ]
    }
   ],
   "source": [
    "# verify initialization\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "with tf.Session(graph=graph,) as sess : \n",
    "    sess.run(init)\n",
    "    values,trainable_vars = sess.run([variable_names,trainables])\n",
    "    for k, v in zip(variable_names,values):\n",
    "        print([\"variable: \" , k])\n",
    "        #print([\"value: \" , v])\n",
    "        print([\"variable: \" , np.unicode_.find(k,'output')]) \n",
    "        print([\"shape: \" , v.shape])\n",
    "        #print(v) training_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(log_dir).mkdir(exist_ok=True, parents=True)\n",
    "filelist = [ f for f in os.listdir(log_dir) if f.endswith(\".local\") ]\n",
    "for f in filelist:\n",
    "    os.remove(os.path.join(log_dir, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 1 , total batch 1, total trials: 25,dfa train Loss: 5.247, dfa_accuracy : 4.000\n",
      "Epoch: 1, Batch: 26 , total batch 26, total trials: 650,dfa train Loss: 0.835, dfa_accuracy : 48.000\n",
      "Epoch: 1, Batch: 51 , total batch 51, total trials: 1275,dfa train Loss: 0.414, dfa_accuracy : 56.000\n",
      "Epoch: 1, Batch: 76 , total batch 76, total trials: 1900,dfa train Loss: 0.283, dfa_accuracy : 72.000\n",
      "Epoch: 1, cross validation loss :0.237, cross validation accuracy: 63.200\n",
      "Epoch: 2, Batch: 1 , total batch 101, total trials: 2525,dfa train Loss: 0.203, dfa_accuracy : 56.000\n",
      "Epoch: 2, Batch: 26 , total batch 126, total trials: 3150,dfa train Loss: 0.178, dfa_accuracy : 72.000\n",
      "Epoch: 2, Batch: 51 , total batch 151, total trials: 3775,dfa train Loss: 0.092, dfa_accuracy : 80.000\n",
      "Epoch: 2, Batch: 76 , total batch 176, total trials: 4400,dfa train Loss: 0.074, dfa_accuracy : 84.000\n",
      "Epoch: 2, cross validation loss :0.038, cross validation accuracy: 91.200\n",
      "Epoch: 3, Batch: 1 , total batch 201, total trials: 5025,dfa train Loss: 0.039, dfa_accuracy : 96.000\n",
      "Epoch: 3, Batch: 26 , total batch 226, total trials: 5650,dfa train Loss: 0.023, dfa_accuracy : 88.000\n",
      "Epoch: 3, Batch: 51 , total batch 251, total trials: 6275,dfa train Loss: 0.012, dfa_accuracy : 96.000\n",
      "Epoch: 3, Batch: 76 , total batch 276, total trials: 6900,dfa train Loss: 0.007, dfa_accuracy : 96.000\n",
      "Epoch: 3, cross validation loss :0.004, cross validation accuracy: 98.400\n",
      "Epoch: 4, Batch: 1 , total batch 301, total trials: 7525,dfa train Loss: 0.004, dfa_accuracy : 100.000\n",
      "Epoch: 4, Batch: 26 , total batch 326, total trials: 8150,dfa train Loss: 0.002, dfa_accuracy : 100.000\n",
      "Epoch: 4, Batch: 51 , total batch 351, total trials: 8775,dfa train Loss: 0.001, dfa_accuracy : 100.000\n",
      "Epoch: 4, Batch: 76 , total batch 376, total trials: 9400,dfa train Loss: 0.001, dfa_accuracy : 96.000\n",
      "Epoch: 4, cross validation loss :0.001, cross validation accuracy: 99.200\n",
      "Epoch: 5, Batch: 1 , total batch 401, total trials: 10025,dfa train Loss: 0.001, dfa_accuracy : 100.000\n",
      "Epoch: 5, Batch: 26 , total batch 426, total trials: 10650,dfa train Loss: 0.000, dfa_accuracy : 100.000\n",
      "Epoch: 5, Batch: 51 , total batch 451, total trials: 11275,dfa train Loss: 0.001, dfa_accuracy : 100.000\n",
      "Epoch: 5, Batch: 76 , total batch 476, total trials: 11900,dfa train Loss: 0.000, dfa_accuracy : 100.000\n",
      "Epoch: 5, cross validation loss :0.000, cross validation accuracy: 100.000\n",
      "Epoch: 6, Batch: 1 , total batch 501, total trials: 12525,dfa train Loss: 0.000, dfa_accuracy : 100.000\n",
      "Epoch: 6, Batch: 26 , total batch 526, total trials: 13150,dfa train Loss: 0.000, dfa_accuracy : 100.000\n",
      "Epoch: 6, Batch: 51 , total batch 551, total trials: 13775,dfa train Loss: 0.000, dfa_accuracy : 100.000\n",
      "Epoch: 6, Batch: 76 , total batch 576, total trials: 14400,dfa train Loss: 0.000, dfa_accuracy : 100.000\n",
      "Epoch: 6, cross validation loss :0.000, cross validation accuracy: 100.000\n",
      "Epoch: 7, Batch: 1 , total batch 601, total trials: 15025,dfa train Loss: 0.000, dfa_accuracy : 100.000\n",
      "Epoch: 7, Batch: 26 , total batch 626, total trials: 15650,dfa train Loss: 0.000, dfa_accuracy : 100.000\n",
      "Epoch: 7, Batch: 51 , total batch 651, total trials: 16275,dfa train Loss: 0.000, dfa_accuracy : 100.000\n",
      "Epoch: 7, Batch: 76 , total batch 676, total trials: 16900,dfa train Loss: 0.000, dfa_accuracy : 100.000\n",
      "Epoch: 7, cross validation loss :0.000, cross validation accuracy: 99.200\n",
      "Epoch: 8, Batch: 1 , total batch 701, total trials: 17525,dfa train Loss: 0.000, dfa_accuracy : 100.000\n",
      "Epoch: 8, Batch: 26 , total batch 726, total trials: 18150,dfa train Loss: 0.000, dfa_accuracy : 100.000\n",
      "Epoch: 8, Batch: 51 , total batch 751, total trials: 18775,dfa train Loss: 0.000, dfa_accuracy : 100.000\n",
      "Epoch: 8, Batch: 76 , total batch 776, total trials: 19400,dfa train Loss: 0.000, dfa_accuracy : 100.000\n",
      "Epoch: 8, cross validation loss :0.000, cross validation accuracy: 100.000\n",
      "Epoch: 9, Batch: 1 , total batch 801, total trials: 20025,dfa train Loss: 0.000, dfa_accuracy : 100.000\n",
      "Epoch: 9, Batch: 26 , total batch 826, total trials: 20650,dfa train Loss: 0.000, dfa_accuracy : 100.000\n",
      "Epoch: 9, Batch: 51 , total batch 851, total trials: 21275,dfa train Loss: 0.000, dfa_accuracy : 100.000\n",
      "Epoch: 9, Batch: 76 , total batch 876, total trials: 21900,dfa train Loss: 0.000, dfa_accuracy : 96.000\n",
      "Epoch: 9, cross validation loss :0.000, cross validation accuracy: 99.200\n",
      "Epoch: 10, Batch: 1 , total batch 901, total trials: 22525,dfa train Loss: 0.000, dfa_accuracy : 96.000\n",
      "Epoch: 10, Batch: 26 , total batch 926, total trials: 23150,dfa train Loss: 0.000, dfa_accuracy : 100.000\n",
      "Epoch: 10, Batch: 51 , total batch 951, total trials: 23775,dfa train Loss: 0.000, dfa_accuracy : 100.000\n",
      "Epoch: 10, Batch: 76 , total batch 976, total trials: 24400,dfa train Loss: 0.000, dfa_accuracy : 96.000\n",
      "Epoch: 10, cross validation loss :0.000, cross validation accuracy: 100.000\n",
      "Epoch: 11, Batch: 1 , total batch 1001, total trials: 25025,dfa train Loss: 0.000, dfa_accuracy : 100.000\n",
      "Epoch: 11, Batch: 26 , total batch 1026, total trials: 25650,dfa train Loss: 0.000, dfa_accuracy : 100.000\n",
      "Epoch: 11, Batch: 51 , total batch 1051, total trials: 26275,dfa train Loss: 0.000, dfa_accuracy : 100.000\n",
      "Epoch: 11, Batch: 76 , total batch 1076, total trials: 26900,dfa train Loss: 0.000, dfa_accuracy : 100.000\n",
      "Epoch: 11, cross validation loss :0.000, cross validation accuracy: 100.000\n",
      "Epoch: 12, Batch: 1 , total batch 1101, total trials: 27525,dfa train Loss: 0.000, dfa_accuracy : 100.000\n",
      "Epoch: 12, Batch: 26 , total batch 1126, total trials: 28150,dfa train Loss: 0.000, dfa_accuracy : 100.000\n",
      "Epoch: 12, Batch: 51 , total batch 1151, total trials: 28775,dfa train Loss: 0.000, dfa_accuracy : 100.000\n",
      "Epoch: 12, Batch: 76 , total batch 1176, total trials: 29400,dfa train Loss: 0.000, dfa_accuracy : 100.000\n",
      "Epoch: 12, cross validation loss :0.000, cross validation accuracy: 100.000\n",
      "Epoch: 13, Batch: 1 , total batch 1201, total trials: 30025,dfa train Loss: 0.000, dfa_accuracy : 100.000\n",
      "Epoch: 13, Batch: 26 , total batch 1226, total trials: 30650,dfa train Loss: 0.000, dfa_accuracy : 96.000\n",
      "Epoch: 13, Batch: 51 , total batch 1251, total trials: 31275,dfa train Loss: 0.000, dfa_accuracy : 96.000\n",
      "Epoch: 13, Batch: 76 , total batch 1276, total trials: 31900,dfa train Loss: 0.000, dfa_accuracy : 100.000\n",
      "Epoch: 13, cross validation loss :0.000, cross validation accuracy: 100.000\n",
      "Epoch: 14, Batch: 1 , total batch 1301, total trials: 32525,dfa train Loss: 0.000, dfa_accuracy : 100.000\n",
      "Epoch: 14, Batch: 26 , total batch 1326, total trials: 33150,dfa train Loss: 0.000, dfa_accuracy : 100.000\n",
      "Epoch: 14, Batch: 51 , total batch 1351, total trials: 33775,dfa train Loss: 0.000, dfa_accuracy : 100.000\n",
      "Epoch: 14, Batch: 76 , total batch 1376, total trials: 34400,dfa train Loss: 0.000, dfa_accuracy : 100.000\n",
      "Epoch: 14, cross validation loss :0.000, cross validation accuracy: 100.000\n",
      "Epoch: 15, Batch: 1 , total batch 1401, total trials: 35025,dfa train Loss: 0.000, dfa_accuracy : 100.000\n",
      "Epoch: 15, Batch: 26 , total batch 1426, total trials: 35650,dfa train Loss: 0.000, dfa_accuracy : 100.000\n",
      "Epoch: 15, Batch: 51 , total batch 1451, total trials: 36275,dfa train Loss: 0.000, dfa_accuracy : 100.000\n",
      "Epoch: 15, Batch: 76 , total batch 1476, total trials: 36900,dfa train Loss: 0.000, dfa_accuracy : 100.000\n",
      "Epoch: 15, cross validation loss :0.000, cross validation accuracy: 100.000\n",
      "Optimization Finished!\n",
      "Model saved in path: /home/eghbal/MyData/KeRNL/logs/ffn/lin_func_approx/dfa_eta_weight_1e-03_batch_2e+01_hum_hidd_1e+03_steps_1e+02_run_20190304_1738/model.ckpt-99\n"
     ]
    }
   ],
   "source": [
    "# write graph into tensorboard \n",
    "tb_writer = tf.summary.FileWriter(log_dir,graph)\n",
    "# run a training session \n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(EPOCHS):\n",
    "        sess.run(iter.initializer,feed_dict={X: x_train, Y: y_train , BATCH_SIZE: BATCH_LENGTH})\n",
    "        for step in range(NUM_TRAINING_STEPS): \n",
    "            _, dfa_loss,dfa_accu,dfa_merged_summary=sess.run([ffn_train_op,loss,accuracy,dfa_merged_summary_op])\n",
    "            tb_writer.add_summary(dfa_merged_summary, global_step=epoch*NUM_TRAINING_STEPS+step+1)\n",
    "            if step % DISPLAY_STEP==0 : \n",
    "                print('Epoch: {}, Batch: {} , total batch {}, total trials: {},dfa train Loss: {:.3f}, dfa_accuracy : {:.3f}'.format(epoch+1,step + 1,epoch*NUM_TRAINING_STEPS+step+1,(epoch*NUM_TRAINING_STEPS+step+1)*BATCH_LENGTH, dfa_loss,dfa_accu))\n",
    "        sess.run(iter.initializer, feed_dict={ X: x_test, Y: y_test, BATCH_SIZE: TEST_LENGTH})        \n",
    "        # run test at the end of each epoch  \n",
    "        dfa_test_loss,dfa_test_accu, dfa_evaluate_summary=sess.run([dfa_loss_cross_validiation,dfa_accu_cross_validation,dfa_evaluate_summary_op])        \n",
    "        tb_writer.add_summary(dfa_evaluate_summary, global_step=epoch*NUM_TRAINING_STEPS+NUM_TRAINING_STEPS+1)\n",
    "        print('Epoch: {}, cross validation loss :{:.3f}, cross validation accuracy: {:.3f}'.format(epoch+1,dfa_test_loss,dfa_test_accu))\n",
    "    print(\"Optimization Finished!\")\n",
    "    save_path = saver.save(sess, log_dir+\"/model.ckpt\", global_step=step,write_meta_graph=True)\n",
    "    print(\"Model saved in path: %s\" % save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
