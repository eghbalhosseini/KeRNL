{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    " In the following notebook, I aim to create a model of spiking neural network with conductance dynamics. The dynamics of each neuron can be seperated into membrane dynamics, synpase dynamics and spiking dynamics. In its simplest form the following equations goven the dynamics of each neuron \n",
    "\n",
    "### membrane dynamics \n",
    "membrane dynamics of neuron $i$ during integration has the following dynamics \n",
    "$$ C_m \\frac{dV_i}{dt}= -g_L (V_i - V_L) - g_{Ei} (V_i-V_E) - g_{Ii} (V_i-V_I) $$ \n",
    "\n",
    "In this equation $\\frac{C_m}{g_L}$ is the membrane time constant, $g_{Ei}$ and $g_{Ii}$ are synatic conductances for excitatory and inhibitory inputs, and $V_{E}$ and $V_I$  are reversal potention for the corresponding synapses.  \n",
    "### Firing dynamics\n",
    "The firing dynamics of the neuron is model as a simple reseting. More specifically, \n",
    "$$V_i \\rightarrow V_{reset} \\ \\ \\  if \\ \\ \\ V_i>=V_{\\Theta} $$\n",
    "\n",
    "$ V_{\\Theta}$ represent the threshold voltage and $V_{reset}$ is the reset voltage of the neuron.\n",
    "\n",
    "### Input dynamics \n",
    "Input synapes are the the site of learning in the spiking network. Below a conductance based formulation is presented. \n",
    "First, the time-dependent input conductance to membrane is calculated as follows \n",
    "$$ g_i(t) = \\sum_j W_{ij} S_{ij}(t) $$\n",
    "\n",
    "the term $j$ reperesent all the neurons that have a synapse onto the neuron $i$. the time dependence of conductance is due to $S(t)$ which represent the spiking activity for neurons connected to neuron $i$ . The spiking activity has the following governing equations \n",
    "$$ S_{ij} \\rightarrow S_{ij}+1 \\quad if \\ neuron\\ j\\ fires$$\n",
    "$$ \\frac{dS_{ij}(t)}{dt} = \\frac{-S_{ij}(t)}{\\tau_s}$$ \n",
    "\n",
    "So each spike add a unit to $S_{ij}$ and the input decays with time constant $\\tau_s$\n",
    "\n",
    "# Implementation in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the dynamics presented about, neurons can be defined as object with the following functions:  \n",
    "1. function for calucating the membrane dynamics.\n",
    "2. function for calculating the firing event.\n",
    "3. function for calculating the conductance. \n",
    "\n",
    "We implement the object in within tensorflow and define input as placeholders, and functions stated above are operations in a graph defined over TF variables that represent neurons.\n",
    "\n",
    "the following parameters are defined as global (based on Fiete et. al. 2007)\n",
    "$$ C_m=1 \\quad V_L=-60 \\quad V_E= 0 \\quad V_I= -70 \\quad g_L=0.03 $$\n",
    "$$V_{\\Theta}=-50 \\quad V_{reset}=-55 \\quad \\tau_s = 5 $$\n",
    "\n",
    "in the implementation of the code lower case represent scalars and upper case represent vectors and matrices  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic LIFneuron \n",
    "class LIFNeuron(object):\n",
    "    # initialize the network object\n",
    "    def __init__(self,n_Neur=1,c_m=1, v_m=1.0, v_L=-60, v_E=0, v_I=-70, g_L=0.03, v_Theta=-50.0, v_Reset=-50.0, tau_s=5):\n",
    "        # number of neurons \n",
    "        self.n_Neur=n_Neur \n",
    "        # membrane capacitance  \n",
    "        self.c_m=c_m\n",
    "        # leak potential\n",
    "        self.v_L=v_L\n",
    "        # reversal potentional for excitatory input\n",
    "        self.v_E=v_E\n",
    "        # reversal potential for inhibitory input \n",
    "        self.v_I=v_I\n",
    "        # leak conductance\n",
    "        self.g_L=g_L\n",
    "        # spike threshold potential\n",
    "        self.v_Theta=v_Theta\n",
    "        # Reset potential\n",
    "        self.v_Reset=v_Reset\n",
    "        # synapse time constant \n",
    "        self.tau_s=tau_s\n",
    "        # instantiate a graph for neuron \n",
    "        self.graph=tf.Graph()\n",
    "        #build the graph \n",
    "        with self.graph.as_default():\n",
    "            \n",
    "            # get vars and place holders for the network \n",
    "            self.get_vars_and_ph()\n",
    "            \n",
    "            # operations on the graph to implement the dynamics \n",
    "            self.input=self.get_input_op()\n",
    "            \n",
    "            self.get_conductance_op()\n",
    "            \n",
    "            self.get_integrating_op()\n",
    "            \n",
    "            self.V, self.has_fired =self.get_firing_op()\n",
    "\n",
    "            # TODO : define outputs \n",
    "\n",
    "    # function for variables and placeholders \n",
    "    def get_vars_and_ph(self):\n",
    "        \n",
    "        # membrane potential V,\n",
    "        # vector with size as the number of neurons, initial value v_Reset\n",
    "        self.V=tf.Variable(tf.constant(self.v_Reset,shape=[self.n_Neur,1],dtype=tf.float32),name='V')\n",
    "        \n",
    "        # vector representing which neuron has fired at current time \n",
    "        self.has_fired=tf.Variable(tf.constant(0.0,shape=[self.n_Neur,1],dtype=tf.float32),name='has_fired')\n",
    "        \n",
    "        # \n",
    "        self.V_Reset=tf.Variable(tf.constant(self.v_Reset,shape=[self.n_Neur,1],dtype=tf.float32),name='V_Reset')\n",
    "        \n",
    "        # leak potential \n",
    "        self.V_L=tf.constant(self.v_L,shape=[self.n_Neur,1],dtype=tf.float32)\n",
    "        \n",
    "        # excitatory reversal potential \n",
    "        self.V_E=tf.constant(self.v_E,shape=[self.n_Neur,1],dtype=tf.float32)\n",
    "        \n",
    "        # inhibitory reversal potential \n",
    "        self.V_I=tf.constant(self.v_I,shape=[self.n_Neur,1],dtype=tf.float32)\n",
    "        \n",
    "        # input current from external source, \n",
    "        # vector with size as the number of neurons \n",
    "        self.I_ext=tf.placeholder(dtype=tf.float32,shape=[self.n_Neur,1],name='I_ext')\n",
    "        \n",
    "        # simulation time interval \n",
    "        self.dt=tf.placeholder(dtype=tf.float32, name='dt')\n",
    "        \n",
    "        # weight matrix in the network\n",
    "        self.W=tf.Variable(tf.random.normal(shape=[self.n_Neur,self.n_Neur],mean=0.0,stddev=0.1,dtype=tf.float32),name='W')\n",
    "        \n",
    "        # synaptic conductance \n",
    "        # vector with size as the number of neurons \n",
    "        self.G=tf.Variable(tf.zeros(shape=[self.n_Neur,1],dtype=tf.float32),name='G')\n",
    "        \n",
    "        # leak conductance \n",
    "        self.G_L=tf.constant(self.g_L,shape=[self.n_Neur,1],dtype=tf.float32)\n",
    "        \n",
    "        # synaptic input\n",
    "        # matrix with size as the number of neurons, columns are input, and rows are output\n",
    "        self.S=tf.Variable(tf.zeros(shape=[self.n_Neur,self.n_Neur],dtype=tf.float32),name='S')\n",
    "        \n",
    "    # function for getting input Current \n",
    "    def get_input_op(self):\n",
    "        return self.I_ext\n",
    "\n",
    "    # function for neuron dynamics during integration \n",
    "    def get_integrating_op(self):\n",
    "        # get external current input \n",
    "        I_ext=self.get_input_op()\n",
    "        \n",
    "        # update membrane potential\n",
    "        dV_op=tf.divide(\n",
    "            tf.add_n(\n",
    "                [tf.negative(tf.multiply(self.g_L,tf.subtract(self.V,self.V_L))),\n",
    "                tf.negative(tf.multiply(self.G,tf.subtract(self.V,self.V_E))),\n",
    "                tf.negative(tf.multiply(self.G,tf.subtract(self.V,self.V_I)))]),\n",
    "            self.c_m)\n",
    "        V_op=self.V.assign_add(dV_op*self.dt)\n",
    "    \n",
    "    # neuron dynamics during firing \n",
    "    def get_firing_op(self):\n",
    "        # find out which neurons has crossed the threshold \n",
    "        has_fired_op=tf.greater_equal(self.V, tf.constant(self.v_Theta,shape=[self.n_Neur,1],dtype=tf.float32))\n",
    "        \n",
    "        # reset membrane potential only for units that fired,\n",
    "        V_op=tf.where(has_fired_op,self.V_Reset,self.V)\n",
    "        has_fired_op_float=tf.dtypes.cast(has_fired_op,tf.float32)\n",
    "        \n",
    "        self.has_fired.assign(has_fired_op_float)\n",
    "        self.V.assign(V_op)\n",
    "        return V_op , has_fired_op_float\n",
    "    \n",
    "    # update conductance \n",
    "    def get_conductance_op(self):\n",
    "        # first, update synaptic input \n",
    "        dS_op=tf.divide(tf.negative(self.S),self.c_m)\n",
    "        S_op=self.S.assign_add(dS_op*self.dt)\n",
    "        \n",
    "        # second, update the value of synptic input for neurons \n",
    "        has_fired_ax=tf.tile(self.has_fired,[1,self.n_Neur])\n",
    "        self.S.assign_add(has_fired_ax)\n",
    "        \n",
    "        # get the updated G from multiplying W and S. \n",
    "        self.G.assign(tf.reduce_sum(tf.multiply(self.W,self.S), 1, keepdims=True))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next we simulate 1 neuron to test the functionality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Duration of the simulation in ms\n",
    "T = 200\n",
    "# Duration of each time step in ms\n",
    "dt = 1\n",
    "# Number of iterations = T/dt\n",
    "steps = int(T / dt)\n",
    "# Output variables\n",
    "I = []\n",
    "U = []\n",
    "\n",
    "neuron = LIFNeuron()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape () for Tensor 'I_ext:0', which has shape '(1,)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-21ede6552263>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mfeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0mneuron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mI_ext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mI_ext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneuron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfire\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mneuron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mneuron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_fired\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mI_ext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/KeRNL/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/KeRNL/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m                              \u001b[0;34m'which has shape %r'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[0;32m-> 1128\u001b[0;31m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1129\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape () for Tensor 'I_ext:0', which has shape '(1,)'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Simulation with square input currents\n",
    "\n",
    "# Duration of the simulation in ms\n",
    "T = 200\n",
    "# Duration of each time step in ms\n",
    "dt = 1\n",
    "# Number of iterations = T/dt\n",
    "steps = int(T / dt)\n",
    "# Output variables\n",
    "I = []\n",
    "U = []\n",
    "Fire=[]\n",
    "neuron = LIFNeuron()\n",
    "    \n",
    "with tf.Session(graph=neuron.graph) as sess:\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())    \n",
    "\n",
    "    for step in range(steps):\n",
    "        \n",
    "        t = step * dt\n",
    "        # Set input current in mA\n",
    "        if t > 10 and t < 30:\n",
    "            I_ext = 0.5\n",
    "        elif t > 50 and t < 100:\n",
    "            I_ext = 1.2\n",
    "        elif t > 120 and t < 180:\n",
    "            I_ext = 1.5\n",
    "        else:\n",
    "            I_ext = 0.0\n",
    "\n",
    "        feed = { neuron.I_ext: I_ext, neuron.dt: dt}\n",
    "        \n",
    "        v,fire = sess.run([neuron.V,neuron.has_fired], feed_dict=feed)\n",
    "\n",
    "        I.append(I_ext)\n",
    "        U.append(v)\n",
    "        Fire.append(fire)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] =(12,6)\n",
    "# Draw the input current and the membrane potential\n",
    "plt.figure()\n",
    "plt.plot([i for i in I])\n",
    "plt.title('Square input stimuli')\n",
    "plt.ylabel('Input current (I)')\n",
    "plt.xlabel('Time (msec)')\n",
    "plt.figure()\n",
    "plt.plot([u for u in U])\n",
    "plt.axhline(y=1.0, color='r', linestyle='-')\n",
    "plt.title('LIF response')\n",
    "plt.ylabel('Membrane Potential (mV)')\n",
    "plt.xlabel('Time (msec)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adding random input current "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation with random input currents\n",
    "\n",
    "# Duration of the simulation in ms\n",
    "T = 200\n",
    "# Duration of each time step in ms\n",
    "dt = 1\n",
    "# Number of iterations = T/dt\n",
    "steps = int(T / dt)\n",
    "# Output variables\n",
    "I = []\n",
    "U = []\n",
    "\n",
    "neuron = LIFNeuron()\n",
    "    \n",
    "with tf.Session(graph=neuron.graph) as sess:\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())    \n",
    "\n",
    "    for step in range(steps):\n",
    "        \n",
    "        t = step * dt\n",
    "        if t > 10 and t < 180:\n",
    "            i_app = np.random.normal(1.5, 1.0)\n",
    "        else:\n",
    "            i_app = 0.0\n",
    "\n",
    "        feed = { neuron.i_app: i_app, neuron.dt: dt}\n",
    "        \n",
    "        u = sess.run(neuron.potential, feed_dict=feed)\n",
    "        \n",
    "        I.append(i_app)\n",
    "        U.append(u)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] =(12,6)\n",
    "# Draw the input current and the membrane potential\n",
    "plt.figure()\n",
    "plt.plot([i for i in I])\n",
    "plt.title('Random input stimuli')\n",
    "plt.ylabel('Input current (I)')\n",
    "plt.xlabel('Time (msec)')\n",
    "plt.figure()\n",
    "plt.plot([u for u in U])\n",
    "plt.axhline(y=1.0, color='r', linestyle='-')\n",
    "plt.title('LIF response')\n",
    "plt.ylabel('Membrane Potential (mV)')\n",
    "plt.xlabel('Time (msec)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next is simulating a neuron with synaptic current , assume there are m synapses from input neurons projecting to this neuron, given that the  a memory of previous spikes affect current firing, there should be a history of previous spikes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A new neuron model derived from the LIF neuron\n",
    "# It takes synaptic spikes as input and remember them over a specified time period\n",
    "class LIFSynapticNeuron(LIFNeuron):\n",
    "    \n",
    "    def __init__(self, n_syn, w, max_spikes=50, u_rest=0.0, u_thresh=1.0, tau_rest=4.0, r=1.0, tau=10.0, q=1.5, tau_syn=10.0):\n",
    "      \n",
    "        # Number of synapses\n",
    "        self.n_syn = n_syn\n",
    "        # Maximum number of spikes we remember\n",
    "        self.max_spikes = max_spikes\n",
    "        # The neuron synaptic 'charge'\n",
    "        self.q = q\n",
    "        # The synaptic time constant (ms)\n",
    "        self.tau_syn = tau_syn\n",
    "        # The synaptic efficacy\n",
    "        self.w = w\n",
    "\n",
    "        super(LIFSynapticNeuron, self).__init__(u_rest, u_thresh, tau_rest, r, tau)\n",
    "    \n",
    "    # Update the parent graph variables and placeholders\n",
    "    def get_vars_and_ph(self):\n",
    "        \n",
    "        # Get parent grah variables and placeholders\n",
    "        super(LIFSynapticNeuron, self).get_vars_and_ph()\n",
    "\n",
    "        # Add ours\n",
    "        \n",
    "        # The history of synaptic spike times for the neuron \n",
    "        self.t_spikes = tf.Variable(tf.constant(-1.0, shape=[self.max_spikes, self.n_syn], dtype=tf.float32))\n",
    "        # The last index used to insert spike times\n",
    "        self.t_spikes_idx = tf.Variable(self.max_spikes-1, dtype=tf.int32)\n",
    "        # A placeholder indicating which synapse spiked in the last time step\n",
    "        self.syn_has_spiked = tf.placeholder(shape=[self.n_syn], dtype=tf.bool)\n",
    "\n",
    "    # Operation to update spike times\n",
    "    def update_spike_times(self):\n",
    "        \n",
    "        # Increase the age of older spikes\n",
    "        old_spikes_op = self.t_spikes.assign_add(tf.where(self.t_spikes >=0,\n",
    "                                                          tf.constant(1.0, shape=[self.max_spikes, self.n_syn]) * self.dt,\n",
    "                                                          tf.zeros([self.max_spikes, self.n_syn])))\n",
    "\n",
    "        # Increment last spike index (modulo max_spikes)\n",
    "        new_idx_op = self.t_spikes_idx.assign(tf.mod(self.t_spikes_idx + 1, self.max_spikes))\n",
    "\n",
    "        # Create a list of coordinates to insert the new spikes\n",
    "        idx_op = tf.constant(1, shape=[self.n_syn], dtype=tf.int32) * new_idx_op\n",
    "        coord_op = tf.stack([idx_op, tf.range(self.n_syn)], axis=1)\n",
    "\n",
    "        # Create a vector of new spike times (non-spikes are assigned a negative time)\n",
    "        new_spikes_op = tf.where(self.syn_has_spiked,\n",
    "                                 tf.constant(0.0, shape=[self.n_syn]),\n",
    "                                 tf.constant(-1.0, shape=[self.n_syn]))\n",
    "        \n",
    "        # Replace older spikes by new ones\n",
    "        return tf.scatter_nd_update(old_spikes_op, coord_op, new_spikes_op)\n",
    "\n",
    "    # Override parent get_input_op method\n",
    "    def get_input_op(self):\n",
    "        \n",
    "        # Update our memory of spike times with the new spikes\n",
    "        t_spikes_op = self.update_spike_times()\n",
    "\n",
    "        # Evaluate synaptic input current for each spike on each synapse\n",
    "        i_syn_op = tf.where(t_spikes_op >=0,\n",
    "                            self.q/self.tau_syn * tf.exp(tf.negative(t_spikes_op/self.tau_syn)),\n",
    "                            t_spikes_op*0.0)\n",
    "\n",
    "        # Add each synaptic current to the input current\n",
    "        i_op =  tf.reduce_sum(self.w * i_syn_op)\n",
    "        \n",
    "        return tf.add(self.i_app, i_op)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation with synaptic input currents\n",
    "\n",
    "# Duration of the simulation in ms\n",
    "T = 200\n",
    "# Duration of each time step in ms\n",
    "dt = 1\n",
    "# Number of iterations = T/dt\n",
    "steps = int(T / dt)\n",
    "# Number of synapses\n",
    "n_syn = 25\n",
    "# Spiking frequency in Hz\n",
    "f = 20\n",
    "# We need to keep track of input spikes over time\n",
    "syn_has_spiked = np.full((steps,n_syn), False)\n",
    "# We define the synaptic efficacy as a random vector\n",
    "W = np.random.normal(1.0, 0.5, size=n_syn)\n",
    "# Output variables\n",
    "I = []\n",
    "U = []\n",
    "\n",
    "# Instantiate our synaptic LIF neuron, with a memory of 200 events\n",
    "# Note that in practice, a much shorter period is required as the\n",
    "# contribution of each synapse decreases very rapidly\n",
    "neuron = LIFSynapticNeuron(n_syn=n_syn, w=W, max_spikes=200)\n",
    "    \n",
    "with tf.Session(graph=neuron.graph) as sess:\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())    \n",
    "\n",
    "    for step in range(steps):\n",
    "        \n",
    "        t = step * dt\n",
    "        \n",
    "        if t > 10 and t < 180:\n",
    "            r = np.random.uniform(0,1, size=(n_syn))\n",
    "            syn_has_spiked[step,:] = r < f * dt * 1e-3\n",
    "\n",
    "        feed = { neuron.i_app: 0.0, neuron.syn_has_spiked: syn_has_spiked[step], neuron.dt: dt}\n",
    "        i, u = sess.run([neuron.input, neuron.potential], feed_dict=feed)\n",
    "\n",
    "        I.append(i)\n",
    "        U.append(u)\n",
    "plt.rcParams[\"figure.figsize\"] =(12,6)\n",
    "# Draw spikes\n",
    "spikes = np.argwhere(syn_has_spiked)\n",
    "t, s = spikes.T\n",
    "plt.figure()\n",
    "plt.axis([0, T, 0, n_syn])\n",
    "plt.title('Synaptic spikes')\n",
    "plt.ylabel('spikes')\n",
    "plt.xlabel('Time (msec)')\n",
    "plt.scatter(t, s)\n",
    "# Draw the input current and the membrane potential\n",
    "plt.figure()\n",
    "plt.plot([i for i in I])\n",
    "plt.title('Synaptic input')\n",
    "plt.ylabel('Input current (I)')\n",
    "plt.xlabel('Time (msec)')\n",
    "plt.figure()\n",
    "plt.plot([u for u in U])\n",
    "plt.axhline(y=1.0, color='r', linestyle='-')\n",
    "plt.title('LIF response')\n",
    "plt.ylabel('Membrane Potential (mV)')\n",
    "plt.xlabel('Time (msec)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next step is combining a recurrent input with input to the network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References and resources\n",
    "\n",
    "https://github.com/kaizouman/tensorsandbox/blob/master/snn/leaky_integrate_fire.ipynb\n",
    "\n",
    "https://lcn.epfl.ch/~gerstner/SPNM/node26.html#SECTION02311000000000000000\n",
    " \n",
    "Fiete, Ila R., Michale S. Fee, and H. Sebastian Seung. 2007. “Model of Birdsong Learning Based on Gradient Estimation by Dynamic Perturbation of Neural Conductances.” Journal of Neurophysiology 98 (4): 2038–57.\n",
    "\n",
    "Fiete, Ila R., and H. Sebastian Seung. 2006. “Gradient Learning in Spiking Neural Networks by Dynamic Perturbation of Conductances.” Physical Review Letters 97 (4): 048104."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(KeRNL)",
   "language": "python",
   "name": "kernl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
