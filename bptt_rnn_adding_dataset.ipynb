{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python libraries\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import collections\n",
    "import hashlib\n",
    "import numbers\n",
    "import matplotlib.cm as cm\n",
    "from sys import getsizeof\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "from IPython.display import HTML\n",
    "import re\n",
    "\n",
    "# tensorflow and its dependencies \n",
    "import tensorflow as tf\n",
    "from tensorflow.python.eager import context\n",
    "from tensorflow.python.framework import constant_op\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.framework import tensor_util\n",
    "from tensorflow.python.layers import base as base_layer\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import clip_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.ops import partitioned_variables\n",
    "from tensorflow.python.ops import random_ops\n",
    "from tensorflow.python.ops import tensor_array_ops\n",
    "from tensorflow.python.ops import variable_scope as vs\n",
    "from tensorflow.python.ops import variables as tf_variables\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "from tensorflow.python.util import nest\n",
    "from tensorflow.contrib.rnn.python.ops.core_rnn_cell import _Linear\n",
    "from tensorflow.contrib import slim\n",
    "\n",
    "## user defined modules \n",
    "# kernel rnn cell \n",
    "import kernl_rnn_cell\n",
    "import adding_problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/eghbal/MyData/KeRNL/logs/bptt_rnn_addition/dataset_add_eta_weight_1e-03_batch_2e+01_hum_hidd_1e+02_gc_1e+02_steps_5e+02_run_20190305_1027'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Training Parameters\n",
    "weight_learning_rate = 1e-3\n",
    "training_steps = 500\n",
    "batch_size = 20\n",
    "training_size=batch_size*training_steps\n",
    "epochs=100 \n",
    "test_size=10000\n",
    "display_step = 100\n",
    "grad_clip=100\n",
    "# Network Parameters\n",
    "num_input = 2 # adding problem data input (first input are the random digits , second input is the mask)\n",
    "time_steps = 200\n",
    "num_hidden = 100 # hidden layer num of features\n",
    "num_output = 1 # value of the addition estimation \n",
    "# \n",
    "buffer_size=500\n",
    "# Noise Parameters\n",
    "perturbation_std=1e-4\n",
    "log_dir = os.environ['HOME']+\"/MyData/KeRNL/logs/bptt_rnn_addition/dataset_add_eta_weight_%1.0e_batch_%1.0e_hum_hidd_%1.0e_gc_%1.0e_steps_%1.0e_run_%s\" %(weight_learning_rate,batch_size,num_hidden,grad_clip,training_steps, datetime.now().strftime(\"%Y%m%d_%H%M\"))\n",
    "log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a training and testing dataset\n",
    "training_x, training_y = adding_problem.get_batch(batch_size=training_size,time_steps=time_steps)\n",
    "# \n",
    "testing_x, testing_y = adding_problem.get_batch(batch_size=test_size,time_steps=time_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define KeRNL unit\n",
    "def _hinton_identity_initializer(shape,dtype=None,partition_info=None,verify_shape=None, max_val=1):\n",
    "    if dtype is None:\n",
    "        dtype=tf.float32\n",
    "    #extract second dimension \n",
    "    W_rec=tf.eye(shape[-1],dtype=dtype)\n",
    "    new_shape=[shape[0]-shape[-1],shape[-1]]\n",
    "    W_in=tf.random_uniform(new_shape,0,maxval=max_val)\n",
    "    return tf.concat([W_in,W_rec],axis=0) \n",
    "\n",
    "def bptt_rnn(x,rnn_weights,rnn_bias):\n",
    "    # Define a KeRNL cell, the initialization is done inside the cell with default initializers\n",
    "    with tf.variable_scope(\"bptt\",initializer=_hinton_identity_initializer) as scope:\n",
    "        rnn_cell = tf.contrib.rnn.BasicRNNCell(num_hidden,name='irnn')\n",
    "        rnn_outputs, rnn_states = tf.nn.dynamic_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "        rnn_output=tf.matmul(rnn_outputs[:,-1,:], rnn_weights) +rnn_biases\n",
    "     \n",
    "    return rnn_output, rnn_states  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-d7c01d15a417>:14: BasicRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.\n",
      "All parameters: 31203\n",
      "Trainable parameters: 10401\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "graph=tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.variable_scope('bptt_output',initializer=tf.initializers.random_normal()) as scope:\n",
    "        rnn_weights = tf.get_variable(shape=[num_hidden, num_output],name='output_weight')\n",
    "        rnn_biases = tf.get_variable(shape=[num_output],name='output_addition')\n",
    "    # define weights and inputs to the network\n",
    "    BATCH_SIZE=tf.placeholder(tf.int64)\n",
    "    X = tf.placeholder(\"float\", [None, time_steps, num_input])\n",
    "    Y = tf.placeholder(\"float\", [None, num_output])\n",
    "    # create a dataset \n",
    "    dataset=tf.data.Dataset.from_tensor_slices((X,Y)).batch(BATCH_SIZE).repeat()\n",
    "    dataset = dataset.shuffle(buffer_size=buffer_size)\n",
    "    iter = dataset.make_initializable_iterator()\n",
    "    inputs,labels =iter.get_next()\n",
    "    # define a function for extraction of variable names\n",
    "    rnn_output,rnn_states=bptt_rnn(inputs,rnn_weights,rnn_biases)\n",
    "    trainables=tf.trainable_variables()\n",
    "    variable_names=[v.name for v in tf.trainable_variables()]\n",
    "    #\n",
    "    find_joing_index = lambda x, name_1,name_2 : [a and b for a,b in zip([np.unicode_.find(k.name, name_1)>-1 for k in x] ,[np.unicode_.find(k.name, name_2)>-1 for k in x])].index(True)\n",
    "    # find trainable parameters for kernl\n",
    "    with tf.name_scope(\"bptt_Trainables\") as scope:\n",
    "            # find trainables parameters for bptt\n",
    "        bptt_output_weight_index= find_joing_index(trainables,'bptt','output_weight')\n",
    "        bptt_output_addition_index= find_joing_index(trainables,'bptt','output_addition')\n",
    "        bptt_kernel_index= find_joing_index(trainables,'bptt','kernel')\n",
    "        bptt_bias_index= find_joing_index(trainables,'bptt','bias')\n",
    "            #\n",
    "        bptt_weight_training_indices=np.asarray([bptt_kernel_index,bptt_bias_index,bptt_output_weight_index,bptt_output_addition_index],dtype=np.int)\n",
    "        bptt_weight_trainables= [trainables[k] for k in bptt_weight_training_indices]\n",
    "            \n",
    "            ##################\n",
    "            ## bptt train ####\n",
    "            ##################\n",
    "    with tf.name_scope(\"bptt_train\") as scope:\n",
    "                # BPTT\n",
    "        bptt_loss_output_prediction=tf.losses.mean_squared_error(labels,rnn_output)\n",
    "                # define optimizer\n",
    "        bptt_weight_optimizer = tf.train.RMSPropOptimizer(learning_rate=weight_learning_rate)\n",
    "        bptt_grads=tf.gradients(bptt_loss_output_prediction,bptt_weight_trainables)\n",
    "        bptt_weight_grads_and_vars=list(zip(bptt_grads,bptt_weight_trainables))\n",
    "                # Apply gradient Clipping to recurrent weights\n",
    "        bptt_cropped_weight_grads_and_vars=[(tf.clip_by_norm(grad, grad_clip),var) if  np.unicode_.find(var.name,'output')==-1 else (grad,var) for grad,var in bptt_weight_grads_and_vars]\n",
    "                # apply gradients\n",
    "        bptt_weight_train_op = bptt_weight_optimizer.apply_gradients(bptt_cropped_weight_grads_and_vars)\n",
    "    with tf.name_scope(\"bptt_evaluate\") as scope: \n",
    "        bptt_loss_cross_validiation=tf.losses.mean_squared_error(labels,rnn_output)\n",
    "        \n",
    "    with tf.name_scope('cross_validation_summary') as scope: \n",
    "        tf.summary.scalar('cross_validation_summary',bptt_loss_cross_validiation+1e-10)\n",
    "        bptt_evaluate_summary_op=tf.summary.merge_all(scope=\"cross_validation_summary\") \n",
    "        \n",
    "                ##################\n",
    "                # SUMMARIES ######\n",
    "                ##################\n",
    "                \n",
    "    with tf.name_scope(\"bptt_summaries\") as scope:\n",
    "                    # bptt kernel\n",
    "        tf.summary.histogram('bptt_kernel_grad',bptt_grads[0]+1e-10)\n",
    "        tf.summary.histogram('bptt_kernel', bptt_weight_trainables[0]+1e-10)\n",
    "                    # bptt output weight\n",
    "        tf.summary.histogram('bptt_output_weight_grad',bptt_grads[2]+1e-10)\n",
    "        tf.summary.histogram('bptt_output_weights', bptt_weight_trainables[2]+1e-10)\n",
    "                    # bptt output bias\n",
    "        tf.summary.histogram('bptt_output_addition_grad',bptt_grads[3]+1e-10)\n",
    "        tf.summary.histogram('bptt_output_addition', bptt_weight_trainables[3]+1e-10)\n",
    "                    # bptt loss and accuracy\n",
    "        tf.summary.scalar('bptt_loss_output_prediction',bptt_loss_output_prediction+1e-10)\n",
    "        \n",
    "        \n",
    "        bptt_merged_summary_op=tf.summary.merge_all(scope=\"bptt_summaries\")            \n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    print(\"All parameters:\", np.sum([np.product([xi.value for xi in x.get_shape()]) for x in tf.global_variables()]))\n",
    "    print(\"Trainable parameters:\", np.sum([np.product([xi.value for xi in x.get_shape()]) for x in tf.trainable_variables()]))\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['variable: ', 'bptt_output/output_weight:0']\n",
      "['variable: ', 5]\n",
      "['shape: ', (100, 1)]\n",
      "['variable: ', 'bptt_output/output_addition:0']\n",
      "['variable: ', 5]\n",
      "['shape: ', (1,)]\n",
      "['variable: ', 'bptt/rnn/irnn/kernel:0']\n",
      "['variable: ', -1]\n",
      "['shape: ', (102, 100)]\n",
      "['variable: ', 'bptt/rnn/irnn/bias:0']\n",
      "['variable: ', -1]\n",
      "['shape: ', (100,)]\n"
     ]
    }
   ],
   "source": [
    "# verify initialization\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "with tf.Session(graph=graph,) as sess : \n",
    "    sess.run(init)\n",
    "    values,trainable_vars = sess.run([variable_names,trainables])\n",
    "    for k, v in zip(variable_names,values):\n",
    "        print([\"variable: \" , k])\n",
    "        #print([\"value: \" , v])\n",
    "        print([\"variable: \" , np.unicode_.find(k,'output')]) \n",
    "        print([\"shape: \" , v.shape])\n",
    "        #print(v) training_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(log_dir).mkdir(exist_ok=True, parents=True)\n",
    "filelist = [ f for f in os.listdir(log_dir) if f.endswith(\".local\") ]\n",
    "for f in filelist:\n",
    "    os.remove(os.path.join(log_dir, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write graph into tensorboard \n",
    "tb_writer = tf.summary.FileWriter(log_dir,graph)\n",
    "# run a training session \n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(epochs):\n",
    "        sess.run(iter.initializer,feed_dict={X: training_x, Y: training_y , BATCH_SIZE: batch_size})\n",
    "        for step in range(training_steps): \n",
    "            bptt_train, bptt_loss,bptt_merged_summary=sess.run([bptt_weight_train_op,bptt_loss_output_prediction,bptt_merged_summary_op])\n",
    "            tb_writer.add_summary(bptt_merged_summary, global_step=step)\n",
    "\n",
    "            if step % display_step==0 or step==1 : \n",
    "                print('Epoch: {}, Batch: {},bptt train Loss: {:.3f}'.format(epoch+1,step + 1, bptt_loss))\n",
    "                \n",
    "        # run test at the end of each epoch \n",
    "        sess.run(iter.initializer, feed_dict={ X: testing_x, Y: testing_y, BATCH_SIZE: testing_x.shape[0]})    \n",
    "        bptt_test_loss, bptt_evaluate_summary=sess.run([bptt_loss_cross_validiation,bptt_evaluate_summary_op])        \n",
    "        tb_writer.add_summary(bptt_evaluate_summary, global_step=step)\n",
    "        print('Epoch: {}, cross validation loss :{:.3f}'.format(epoch+1,bptt_test_loss))\n",
    "            \n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    save_path = saver.save(sess, log_dir+\"/model.ckpt\", global_step=step,write_meta_graph=True)\n",
    "    print(\"Model saved in path: %s\" % save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
