{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for implementing MNIST learning on kernel_SNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background \n",
    "This code implements a spiking neural net with conductance in input. the following equations govern the dynamic of the network. \n",
    "### transmembrane voltage dynamics\n",
    "first we model the transmembrane voltage as \n",
    "$$\\tau_m \\frac{dV_i}{dt}= - V_i(t)+ R_m \\times I^{syn}_i(t) $$ \n",
    "$$ {\\tau_a}_i \\frac{dB_i(t)}{dt} = b_i^0 -B_i(t)$$ \n",
    "where, $R_m$ is membrane resistance, $\\tau_m$ is membrane time constant, and ${\\tau_a}_i$ is adaptation time constant  .\n",
    "the synaptic current relates to synaptic activations in the following way\n",
    "$$I^{syn}_i(t)= \\sum_j W^{in}_{ij} \\times X(t) + \\sum_j W^{rec}_{ij} \\times S_j(t) $$ \n",
    "\n",
    "### neuron firing dynamics \n",
    "The firing dynamics of the neuron is model as a simple reseting. More specifically, \n",
    "$$V_i \\rightarrow V_{reset} \\ \\ \\  if \\ \\ \\ V_i>=B_{i} $$\n",
    "\n",
    "$ V_{\\Theta}$ represent the threshold voltage and $V_{reset}$ is the reset voltage of the neuron.\n",
    "\n",
    "### Input dynamics \n",
    "Input synapes are the the site of learning in the spiking network. Below a conductance based formulation is presented. \n",
    "First, the time-dependent input conductance to membrane is calculated as follows \n",
    "$$ g_i(t) = \\sum_j W_{ij} S_{j}(t) $$\n",
    "\n",
    "in the current version $S_{j}(t)$ is equal to spike at timestep $t$ without any decay dynamics. \n",
    "-  TODO the term $j$ reperesent all the neurons that have a synapse onto the neuron $i$. the time dependence of conductance is due to $S(t)$ which represent the spiking activity for neurons connected to neuron $i$ . The spiking activity has the following governing equations \n",
    "$$ S_{j} \\rightarrow S_{j}+1 \\quad if \\ neuron\\ j\\ fires$$\n",
    "$$ \\frac{dS_{j}(t)}{dt} = \\frac{-S_{j}(t)}{\\tau_s}$$ \n",
    "\n",
    "### Spike Adaptation dynamics \n",
    "The threshold for spiking increases with every spike emited from a neuron with the following dynamics \n",
    "$$ B_{i}(t) \\rightarrow B_{i}(t)+\\frac{\\beta}{{\\tau_a}_i} \\quad if \\ neuron\\ i\\ fires$$\n",
    "\n",
    "### References \n",
    "-  Fiete, Ila R., Walter Senn, Claude Z. H. Wang, and Richard H. R. Hahnloser. 2010. “Spike-Time-Dependent Plasticity and Heterosynaptic Competition Organize Networks to Produce Long Scale-Free Sequences of Neural Activity.” Neuron 65 (4): 563–76. \n",
    "\n",
    "-  Bellec, Guillaume, Darjan Salaj, Anand Subramoney, Robert Legenstein, and Wolfgang Maass. 2018. “Long Short-Term Memory and Learning-to-Learn in Networks of Spiking Neurons.” arXiv [cs.NE]. arXiv. http://arxiv.org/abs/1803.09574.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python libraries\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import collections\n",
    "import hashlib\n",
    "import numbers\n",
    "import matplotlib.cm as cm\n",
    "from sys import getsizeof\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import os\n",
    "from IPython.display import HTML\n",
    "import re\n",
    "\n",
    "# tensorflow and its dependencies \n",
    "import tensorflow as tf\n",
    "from tensorflow.python.eager import context\n",
    "from tensorflow.python.framework import constant_op\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.framework import tensor_util\n",
    "from tensorflow.python.layers import base as base_layer\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import clip_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.ops import partitioned_variables\n",
    "from tensorflow.python.ops import random_ops\n",
    "from tensorflow.python.ops import tensor_array_ops\n",
    "from tensorflow.python.ops import variable_scope as vs\n",
    "from tensorflow.python.ops import variables as tf_variables\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "from tensorflow.python.util import nest\n",
    "from tensorflow.contrib.rnn.python.ops.core_rnn_cell import _Linear\n",
    "from tensorflow.contrib import slim\n",
    "\n",
    "## user defined modules \n",
    "# kernel rnn cell \n",
    "import kernl_spiking_cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# uplading mnist data \n",
    "old_v = tf.logging.get_verbosity()\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "train_data = mnist.train.images  # Returns np.array\n",
    "train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "eval_data = mnist.test.images  # Returns np.array\n",
    "eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)\n",
    "tf.logging.set_verbosity(old_v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initial parameters for the network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of batches: 2200\n"
     ]
    }
   ],
   "source": [
    "# Training Parameters\n",
    "weight_learning_rate = 1e-5\n",
    "tensor_learning_rate=1e-6 \n",
    "training_steps = 5000\n",
    "batch_size = 25\n",
    "display_step = 25\n",
    "test_len=128\n",
    "grad_clip=200\n",
    "# Network Parameters\n",
    "# 1-input layer \n",
    "num_input = 1 # MNIST data input (img shape: 28*28)\n",
    "num_context_input=1\n",
    "MNIST_timesteps = 28*28 # timesteps\n",
    "context_timesteps=54\n",
    "timesteps=MNIST_timesteps+context_timesteps\n",
    "num_unit_input_layer=80 # input layer neurons\n",
    "num_context_unit=1\n",
    "# 2-hidden layer \n",
    "num_hidden = 200 # hidden layer num of features\n",
    "# 3-output layer \n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# report batch number \n",
    "total_batch = int(mnist.train.num_examples / batch_size)\n",
    "print(\"Total number of batches:\", total_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernl_SNN_all_states(x,context):\n",
    "    with tf.variable_scope('context_layer') as scope:\n",
    "        context_input_layer_cell=kernl_spiking_cell.context_input_spike_cell(num_units=1,context_switch=MNIST_timesteps)\n",
    "        context_initial_state = context_input_layer_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "        output_context, states_context = tf.nn.dynamic_rnn(context_input_layer_cell, dtype=tf.float32, inputs=context,initial_state=context_initial_state)\n",
    "    with tf.variable_scope('input_layer') as scope: \n",
    "        input_layer_cell=kernl_spiking_cell.input_spike_cell(num_units=num_unit_input_layer)\n",
    "        input_initial_state = input_layer_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "        output_l1, states_l1 = tf.nn.dynamic_rnn(input_layer_cell, dtype=tf.float32, inputs=x,initial_state=input_initial_state)\n",
    "    with tf.variable_scope('hidden_layer') as scope: \n",
    "        hidden_layer_cell=kernl_spiking_cell.kernl_spike_Cell(num_units=num_hidden,num_inputs=num_unit_input_layer+num_context_unit,output_is_tuple=True,tau_refract=5.0)\n",
    "        hidden_initial_state = hidden_layer_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "        output_hidden, states_hidden = tf.nn.dynamic_rnn(hidden_layer_cell, dtype=tf.float32, inputs=tf.concat([output_l1,output_context],-1),initial_state=hidden_initial_state)\n",
    "    with tf.variable_scope('output_layer') as scope : \n",
    "        output_layer_cell=kernl_spiking_cell.output_spike_cell(num_units=num_classes)\n",
    "        output_voltage, voltage_states=tf.nn.dynamic_rnn(output_layer_cell,dtype=tf.float32,inputs=output_hidden.spike)\n",
    "\n",
    "    return output_voltage,output_hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient equation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first we derive the governing equations for calculating gradients, sensitivity tensor, and temporal filter. We first rewrite the equation for the cell for mathematical simplicitiy. We can rewrite the governing diffrential equation for the network in discrete time as follows <br>\n",
    "$$V_{t+1}=\\sigma_{threshold}(\\delta \\times V_{t} + R_m \\times W^{in} \\times X_{t}+ R_m \\times W^{rec} \\times S_{t})$$ <br>\n",
    "$$S_{t+1}= \\rho \\times S_t + \\sigma_{spike}(V_t)$$ <br>\n",
    "$$Y^{spike}_t=\\sigma_{spike}(V_t)$$  <br>\n",
    "<font color='red'> there is a problem with units in the equation above, look into it. </font> <br>\n",
    "considering a cost function that depends on the $Y^{spike}_t$ and $X_t$ such as $C=C(X(-),f(Y^{spike}(-)),Y^{target})$ where $f(Y^{spike}(-))$ is the output transformation from hidden layer to output layer, in this case a linear layer of neurons. we need to calculate $\\frac{dC}{dW}$ for $W^{rec}$ and $W^{in}$. considering that output is determined by $V_t$ we make the following assertion. \n",
    "$$\\frac{\\partial C}{\\partial S} = 0 $$ \n",
    "\n",
    "in addition following the chain rule for computing $\\frac{\\partial C}{\\partial W}$ we have  <br>\n",
    "$$\\frac{\\partial C}{\\partial W^{rec}} = \\frac{\\partial C}{\\partial Y^{spike}} \\times \\frac{\\partial Y^{spike}}{\\partial V} \\times \\frac{\\partial V}{\\partial W^{rec}}$$ <br> \n",
    "$$\\frac{\\partial C}{\\partial W^{in}} = \\frac{\\partial C}{\\partial Y^{spike}} \\times \\frac{\\partial Y^{spike}}{\\partial V} \\times \\frac{\\partial V}{\\partial W^{in}}$$ <br>\n",
    "\n",
    "we start by implementing the sensitivity lemma, and rewrite the equation for gradients \n",
    "$$g_i(t)=\\sum_jW_{ij}S_j(t)$$ <br>\n",
    "$$\\frac{dC}{dW_{ij}}=\\frac{1}{T} \\int_0^T dt \\frac{\\delta C}{\\delta W_{ij}(t)}=\\frac{1}{T} \\int_0^T dt \\frac{\\delta C}{\\delta g_{i}(t)}\\times S_j(t)$$  <br>\n",
    "$$ \\frac{\\delta C}{\\delta g_{i}(t)}= \\sum_k \\int_0^T \\frac{\\delta C}{\\delta y^{spike}_{k}(t')} \\times \\frac{\\delta y^{spike}_{k}(t') }{\\delta v_{k}(t')} \\times \\frac{\\delta v_k(t')}{\\delta g_i(t)}$$ \n",
    "<br>\n",
    "first we focus on the first two terms in the equation, we can caluculate $\\frac{\\delta C}{\\delta y^{spike}_{k}(t')}$ as error in output $k$ at time $t'$. In addition $\\frac{\\delta y^{spike}_{k}(t') }{\\delta v_{k}(t')}$ is equivalent to $ \\sigma'_{spike}$ . in order to avoid discontinuitiy in $\\sigma'_{spike}$ we use a pseudo derivative defined as \n",
    "$$ \\frac{\\delta y^{spike}_k(t)}{\\delta v_{k}(t)} := max\\{0,1-|v^{norm}{k}(t)|\\}$$\n",
    "$$ v^{norm}{k}(t) = \\frac{v_k(t)-B_{k}(t)}{B_{k}(t)} $$\n",
    "<br>\n",
    "in the equation above the term that depends on interactions between units is  $ \\frac{\\delta v_k(t')}{\\delta g_i(t)} $ and captures how activity of neuron $i$ at time $t$ affects the activity of neuron $k$ at time $t'$ . In order to estimate this interaction we make the following assumption <br>\n",
    "\n",
    "$$\\frac{\\delta v_k(t')}{\\delta g_i(t)} = m_{ki}(t,t') = M_{ki}\\times K(t-t') \\times h(s_i(t))$$  \n",
    "<font color='red'> how one can learn h and M and K emprically, or what other forms are good estimates of the following interaction. </font> <br>\n",
    "going back to the gradient definition we can incorporate the derivation of interaction and and calculate the gradient as follows\n",
    "<br>\n",
    "$$\\frac{dC}{dW_{ij}}=\\frac{1}{T} \\int_0^T dt \\frac{\\delta C}{\\delta W_{ij}(t)}=\\frac{1}{T} \\int_0^T dt \\frac{\\delta C}{\\delta g_{i}(t)}\\times S_j(t)$$ <br>\n",
    "$$\\frac{dC}{dW_{ij}}=\\frac{1}{T} \\int_0^T dt \\frac{\\delta C}{\\delta W_{ij}(t)}=\\frac{1}{T} \\int_0^T dt \\frac{\\delta C}{\\delta y^{spike}_{k}(t')} \\times  M_{ki}\\times K(t-t') \\times h\\big(S_i(t)\\big) \\times S_j(t)$$\n",
    "<br>\n",
    "first we focus on estimating the two parameters $M_{ki}$ and $K(t)$ . We first apply a small iid hidden perturbation $\\xi$ to $S(t)$ and track its effect on the output voltage.\n",
    "<br>\n",
    "$$\\tilde{V}_{t+1}=\\sigma_{threshold}(\\delta \\times \\tilde{V}_{t} + R_m \\times W^{in} \\times X_{t}+ R_m \\times W^{rec} \\times \\big(S_{t}+\\xi_t)\\big)$$ <br>\n",
    "we then minimize the following cost function <br>\n",
    "$$ C_{M,K}= \\Big(\\tilde{V_i}(t)-V_i(t)-\\sum_j M_{ij} \\sum_{\\tau}K(\\tau)\\times\\xi_j(t-\\tau)\\Big)^2$$\n",
    "<br>\n",
    "for a single exponential filter $K(\\tau)=exp(-\\gamma_j\\tau)$ we have  <br>\n",
    "\n",
    "$$ C_{M,K}= \\Big(\\tilde{V_i}(t)-V_i(t)-\\sum_j M_{ij} \\sum_{\\tau} exp(-\\gamma_j\\tau) \\times \\xi_j(t-\\tau)\\Big)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation graph for learning $M$ and $K(\\tau)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "graph=tf.Graph()\n",
    "with graph.as_default():\n",
    "    # check hardware \n",
    "    \n",
    "    # define weights and inputs to the network\n",
    "    X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "    Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "    Context=tf.placeholder('float',shape=[batch_size,timesteps,num_context_input])\n",
    "    # define a function for extraction of variable names\n",
    "    kernl_output,kernl_hidden_states=kernl_SNN_all_states(X,Context)\n",
    "    \n",
    "    trainables=tf.trainable_variables()\n",
    "    variable_names=[v.name for v in tf.trainable_variables()]\n",
    "    # \n",
    "    find_joing_index = lambda x, name_1,name_2 : [a and b for a,b in zip([np.unicode_.find(k.name, name_1)>-1 for k in x] ,[np.unicode_.find(k.name, name_2)>-1 for k in x])].index(True)\n",
    "    # find trainable parameters for kernl \n",
    "    with tf.name_scope('kernl_Trainables') as scope:\n",
    "        kernl_output_weight_index= find_joing_index(trainables,'output_layer','kernel')\n",
    "        kernl_temporal_filter_index= find_joing_index(trainables,'kernl','temporal_filter')\n",
    "        kernl_sensitivity_tensor_index= find_joing_index(trainables,'kernl','sensitivity_tensor')\n",
    "        kernl_kernel_index= find_joing_index(trainables,'hidden_layer','kernel')\n",
    "    # \n",
    "        kernl_tensor_training_indices=np.asarray([kernl_sensitivity_tensor_index,kernl_temporal_filter_index],dtype=np.int)\n",
    "        kernl_tensor_trainables= [trainables[k] for k in kernl_tensor_training_indices]\n",
    "    #\n",
    "        kernl_weight_training_indices=np.asarray([kernl_kernel_index,kernl_output_weight_index],dtype=np.int)\n",
    "        kernl_weight_trainables= [trainables[k] for k in kernl_weight_training_indices]\n",
    "    \n",
    " \n",
    "    ##################\n",
    "    # kernl train ####\n",
    "    ##################\n",
    "    with tf.name_scope(\"kernl_performance\") as scope:\n",
    "        # outputs \n",
    "        kernl_logit=tf.reduce_mean(kernl_output[:,-context_timesteps:,:],axis=1)\n",
    "        kernl_loss_output_prediction=tf.losses.softmax_cross_entropy(onehot_labels=Y,logits=kernl_logit)\n",
    "        kernl_prediction = tf.nn.softmax(kernl_logit)\n",
    "        kernl_correct_pred = tf.equal(tf.argmax(kernl_prediction, 1), tf.argmax(Y, 1))\n",
    "        kernl_accuracy = tf.reduce_mean(tf.cast(kernl_correct_pred, tf.float32))\n",
    "        # states\n",
    "        \n",
    "        kernl_loss_state_prediction=tf.losses.mean_squared_error(tf.subtract(kernl_hidden_states.v_mem_hat, kernl_hidden_states.v_mem),\n",
    "                                                                 tf.matmul(kernl_hidden_states.Theta,trainables[kernl_sensitivity_tensor_index]))\n",
    "       \n",
    "        \n",
    "\n",
    "        \n",
    "    with tf.name_scope('kernl_train_tensors') as scope: \n",
    "        kernl_weight_optimizer = tf.train.RMSPropOptimizer(learning_rate=tensor_learning_rate)\n",
    "        \n",
    "    with tf.name_scope('kernel_train_weights') as scope : \n",
    "        \n",
    "         \n",
    "            kernl_grad_cost_to_output=tf.gradients(kernl_loss_output_prediction,kernl_logit, name= 'kernl_grad_cost_to_y')\n",
    "            kernl_error_in_hidden_state=tf.matmul(kernl_grad_cost_to_output[-1],tf.transpose(trainables[kernl_output_weight_index]))\n",
    "            kernl_delta_weight=tf.matmul(kernl_error_in_hidden_state,trainables[kernl_sensitivity_tensor_index]) \n",
    "            kernl_eligibility_trace=tf.reduce_mean(kernl_hidden_states.eligibility_trace[:,-context_timesteps:,:],axis=1)\n",
    "            kernl_weight_update_test=tf.einsum(\"un,unv->unv\",kernl_delta_weight,kernl_eligibility_trace)\n",
    "            kernl_weight_update=tf.transpose(tf.reduce_mean(kernl_weight_update_test,axis=0))\n",
    "            # output layer \n",
    "            kernl_grad_cost_to_output_layer=tf.gradients(ys=kernl_loss_output_prediction,xs=trainables[kernl_output_weight_index])\n",
    "            # crop the gradients  \n",
    "            kernl_weight_grads_and_vars=list(zip([kernl_weight_update,kernl_grad_cost_to_output_layer[0]],kernl_weight_trainables))\n",
    "            kernl_cropped_weight_grads_and_vars=[(tf.clip_by_norm(grad, grad_clip),var) if  np.unicode_.find(var.name,'output')==-1 else (grad,var) for grad,var in kernl_weight_grads_and_vars]\n",
    "            # apply gradients \n",
    "            kernl_weight_train_op = kernl_weight_optimizer.apply_gradients(kernl_cropped_weight_grads_and_vars)\n",
    "\n",
    "    ##################\n",
    "    # SUMMARIES ######\n",
    "    ##################\n",
    "    \n",
    "    with tf.name_scope(\"kernl_tensor_summaries\") as scope: \n",
    "        # kernl sensitivity tensor \n",
    "        #tf.summary.histogram('kernl_sensitivity_tensor_grad',kernl_sensitivity_tensor_update+1e-10)\n",
    "        tf.summary.histogram('kernl_sensitivity_tensor',trainables[kernl_sensitivity_tensor_index]+1e-10)\n",
    "        # kernl temporal filter \n",
    "        #tf.summary.histogram('kernl_temporal_filter_grad',kernl_temporal_filter_update+1e-10)\n",
    "        tf.summary.histogram('kernl_temporal_filter',trainables[kernl_temporal_filter_index]+1e-10)\n",
    "        # kernl loss \n",
    "        #tf.summary.scalar('kernl_loss_state_prediction',kernl_loss_state_prediction+1e-10)\n",
    "        # kernl senstivity tensor and temporal filter \n",
    "        tf.summary.image('kernl_sensitivity_tensor',tf.expand_dims(tf.expand_dims(trainables[kernl_sensitivity_tensor_index],axis=0),axis=-1))\n",
    "        #tf.summary.image('kernl_sensitivity_tensor_grad',tf.expand_dims(tf.expand_dims(kernl_sensitivity_tensor_update,axis=0),axis=-1))\n",
    "        tf.summary.image('kernl_temporal_filter',tf.expand_dims(tf.expand_dims(tf.expand_dims(trainables[kernl_temporal_filter_index],axis=0),axis=-1),axis=-1))\n",
    "        #tf.summary.image('kernl_temporal_filter_grad',tf.expand_dims(tf.expand_dims(tf.expand_dims(kernl_temporal_filter_update,axis=0),axis=-1),axis=-1))\n",
    "        kernl_tensor_merged_summary_op=tf.summary.merge_all(scope=\"kernl_tensor_summaries\")\n",
    "        \n",
    "    with tf.name_scope(\"kernl_weight_summaries\") as scope: \n",
    "        # kernl kernel\n",
    "        tf.summary.histogram('kernl_kernel_grad',kernl_weight_update+1e-10)\n",
    "        tf.summary.histogram('kernl_kernel',trainables[kernl_kernel_index]+1e-10)\n",
    "        # kernl output weight\n",
    "        tf.summary.histogram('kernl_output_weight_grad',kernl_grad_cost_to_output_layer[0]+1e-10)\n",
    "        tf.summary.histogram('kernl_output_weights', trainables[kernl_output_weight_index]+1e-10)\n",
    "\n",
    "        # kernl loss \n",
    "        tf.summary.scalar('kernl_loss_output_prediction',kernl_loss_output_prediction+1e-10)\n",
    "        tf.summary.scalar('kernl_accuracy',kernl_accuracy)\n",
    "        # kernl kernel and output weight \n",
    "        tf.summary.image('kernl_kernel',tf.expand_dims(tf.expand_dims(trainables[kernl_kernel_index],axis=0),axis=-1))\n",
    "        tf.summary.image('kernl_kernel_grad',tf.expand_dims(tf.expand_dims(kernl_weight_update,axis=0),axis=-1))\n",
    "        tf.summary.image('kernl_output_weight',tf.expand_dims(tf.expand_dims(trainables[kernl_output_weight_index],axis=0),axis=-1))\n",
    "        tf.summary.image('kernl_output_weight_grad',tf.expand_dims(tf.expand_dims(kernl_grad_cost_to_output_layer[0],axis=0),axis=-1))\n",
    "        kernl_weight_merged_summary_op=tf.summary.merge_all(scope=\"kernl_weight_summaries\")\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "graph=tf.Graph()\n",
    "with graph.as_default():\n",
    "    # check hardware \n",
    "    \n",
    "    # define weights and inputs to the network\n",
    "    X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "    Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "    Context=tf.placeholder('float',shape=[batch_size,timesteps,num_context_input])\n",
    "    # define a function for extraction of variable names\n",
    "    kernl_output,kernl_hidden_states=kernl_SNN_all_states(X,Context)\n",
    "    \n",
    "    trainables=tf.trainable_variables()\n",
    "    variable_names=[v.name for v in tf.trainable_variables()]\n",
    "    # \n",
    "    find_joing_index = lambda x, name_1,name_2 : [a and b for a,b in zip([np.unicode_.find(k.name, name_1)>-1 for k in x] ,[np.unicode_.find(k.name, name_2)>-1 for k in x])].index(True)\n",
    "    # find trainable parameters for kernl \n",
    "    with tf.name_scope('kernl_Trainables') as scope:\n",
    "        kernl_output_weight_index= find_joing_index(trainables,'output_layer','kernel')\n",
    "        kernl_temporal_filter_index= find_joing_index(trainables,'kernl','temporal_filter')\n",
    "        kernl_sensitivity_tensor_index= find_joing_index(trainables,'kernl','sensitivity_tensor')\n",
    "        kernl_kernel_index= find_joing_index(trainables,'hidden_layer','kernel')\n",
    "    # \n",
    "        kernl_tensor_training_indices=np.asarray([kernl_sensitivity_tensor_index,kernl_temporal_filter_index],dtype=np.int)\n",
    "        kernl_tensor_trainables= [trainables[k] for k in kernl_tensor_training_indices]\n",
    "    #\n",
    "        kernl_weight_training_indices=np.asarray([kernl_kernel_index,kernl_output_weight_index],dtype=np.int)\n",
    "        kernl_weight_trainables= [trainables[k] for k in kernl_weight_training_indices]\n",
    "    \n",
    " \n",
    "    ##################\n",
    "    # kernl train ####\n",
    "    ##################\n",
    "    with tf.name_scope(\"kernl_train\") as scope:\n",
    "        # outputs \n",
    "        kernl_logit=tf.reduce_mean(kernl_output[:,-context_timesteps:,:],axis=1)\n",
    "        kernl_loss_output_prediction=tf.losses.softmax_cross_entropy(onehot_labels=Y,logits=kernl_logit)\n",
    "        kernl_prediction = tf.nn.softmax(kernl_logit)\n",
    "        kernl_correct_pred = tf.equal(tf.argmax(kernl_prediction, 1), tf.argmax(Y, 1))\n",
    "        kernl_accuracy = tf.reduce_mean(tf.cast(kernl_correct_pred, tf.float32))\n",
    "        # states \n",
    "        # define optimizers \n",
    "        kernl_weight_optimizer = tf.train.RMSPropOptimizer(learning_rate=weight_learning_rate)\n",
    "        \n",
    "\n",
    "        \n",
    "        with tf.name_scope('kernl_train_weights') as scope: \n",
    "            kernl_grad_cost_to_output=tf.gradients(kernl_loss_output_prediction,kernl_logit, name= 'kernl_grad_cost_to_y')\n",
    "            kernl_error_in_hidden_state=tf.matmul(kernl_grad_cost_to_output[-1],tf.transpose(trainables[kernl_output_weight_index]))\n",
    "            kernl_delta_weight=tf.matmul(kernl_error_in_hidden_state,trainables[kernl_sensitivity_tensor_index]) \n",
    "            kernl_eligibility_trace=tf.reduce_mean(kernl_hidden_states.eligibility_trace[:,-context_timesteps:,:],axis=1)\n",
    "            kernl_weight_update_test=tf.einsum(\"un,unv->unv\",kernl_delta_weight,kernl_eligibility_trace)\n",
    "            kernl_weight_update=tf.transpose(tf.reduce_mean(kernl_weight_update_test,axis=0))\n",
    "            # output layer \n",
    "            kernl_grad_cost_to_output_layer=tf.gradients(ys=kernl_loss_output_prediction,xs=trainables[kernl_output_weight_index])\n",
    "            # crop the gradients  \n",
    "            kernl_weight_grads_and_vars=list(zip([kernl_weight_update,kernl_grad_cost_to_output_layer[0]],kernl_weight_trainables))\n",
    "            kernl_cropped_weight_grads_and_vars=[(tf.clip_by_norm(grad, grad_clip),var) if  np.unicode_.find(var.name,'output')==-1 else (grad,var) for grad,var in kernl_weight_grads_and_vars]\n",
    "            # apply gradients \n",
    "            kernl_weight_train_op = kernl_weight_optimizer.apply_gradients(kernl_cropped_weight_grads_and_vars)\n",
    "\n",
    "    ##################\n",
    "    # SUMMARIES ######\n",
    "    ##################\n",
    "    \n",
    "    with tf.name_scope(\"kernl_tensor_summaries\") as scope: \n",
    "        # kernl sensitivity tensor \n",
    "        #tf.summary.histogram('kernl_sensitivity_tensor_grad',kernl_sensitivity_tensor_update+1e-10)\n",
    "        tf.summary.histogram('kernl_sensitivity_tensor',trainables[kernl_sensitivity_tensor_index]+1e-10)\n",
    "        # kernl temporal filter \n",
    "        #tf.summary.histogram('kernl_temporal_filter_grad',kernl_temporal_filter_update+1e-10)\n",
    "        tf.summary.histogram('kernl_temporal_filter',trainables[kernl_temporal_filter_index]+1e-10)\n",
    "        # kernl loss \n",
    "        #tf.summary.scalar('kernl_loss_state_prediction',kernl_loss_state_prediction+1e-10)\n",
    "        # kernl senstivity tensor and temporal filter \n",
    "        tf.summary.image('kernl_sensitivity_tensor',tf.expand_dims(tf.expand_dims(trainables[kernl_sensitivity_tensor_index],axis=0),axis=-1))\n",
    "        #tf.summary.image('kernl_sensitivity_tensor_grad',tf.expand_dims(tf.expand_dims(kernl_sensitivity_tensor_update,axis=0),axis=-1))\n",
    "        tf.summary.image('kernl_temporal_filter',tf.expand_dims(tf.expand_dims(tf.expand_dims(trainables[kernl_temporal_filter_index],axis=0),axis=-1),axis=-1))\n",
    "        #tf.summary.image('kernl_temporal_filter_grad',tf.expand_dims(tf.expand_dims(tf.expand_dims(kernl_temporal_filter_update,axis=0),axis=-1),axis=-1))\n",
    "        kernl_tensor_merged_summary_op=tf.summary.merge_all(scope=\"kernl_tensor_summaries\")\n",
    "        \n",
    "    with tf.name_scope(\"kernl_weight_summaries\") as scope: \n",
    "        # kernl kernel\n",
    "        tf.summary.histogram('kernl_kernel_grad',kernl_weight_update+1e-10)\n",
    "        tf.summary.histogram('kernl_kernel',trainables[kernl_kernel_index]+1e-10)\n",
    "        # kernl output weight\n",
    "        tf.summary.histogram('kernl_output_weight_grad',kernl_grad_cost_to_output_layer[0]+1e-10)\n",
    "        tf.summary.histogram('kernl_output_weights', trainables[kernl_output_weight_index]+1e-10)\n",
    "\n",
    "        # kernl loss \n",
    "        tf.summary.scalar('kernl_loss_output_prediction',kernl_loss_output_prediction+1e-10)\n",
    "        tf.summary.scalar('kernl_accuracy',kernl_accuracy)\n",
    "        # kernl kernel and output weight \n",
    "        tf.summary.image('kernl_kernel',tf.expand_dims(tf.expand_dims(trainables[kernl_kernel_index],axis=0),axis=-1))\n",
    "        tf.summary.image('kernl_kernel_grad',tf.expand_dims(tf.expand_dims(kernl_weight_update,axis=0),axis=-1))\n",
    "        tf.summary.image('kernl_output_weight',tf.expand_dims(tf.expand_dims(trainables[kernl_output_weight_index],axis=0),axis=-1))\n",
    "        tf.summary.image('kernl_output_weight_grad',tf.expand_dims(tf.expand_dims(kernl_grad_cost_to_output_layer[0],axis=0),axis=-1))\n",
    "        kernl_weight_merged_summary_op=tf.summary.merge_all(scope=\"kernl_weight_summaries\")\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify initialization\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "\n",
    "with tf.Session(graph=graph,) as sess : \n",
    "    sess.run(init)\n",
    "    values,trainable_vars = sess.run([variable_names,trainables])\n",
    "    for k, v in zip(variable_names,values):\n",
    "        print([\"variable: \" , k])\n",
    "        #print([\"value: \" , v])\n",
    "        print([\"variable: \" , np.unicode_.find(k,'output')]) \n",
    "        print([\"shape: \" , v.shape])\n",
    "        #print(v) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
